{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfraAd/CSC413-Homeworks/blob/main/Fall25_Hw12_DistributedTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526dea0b",
      "metadata": {
        "id": "526dea0b"
      },
      "source": [
        "# Homework 12\n",
        "\n",
        "In this homework, we will implement a (simulated) distributed training framework, focusing on how backend communication operates under different parallelism strategies. We will implement two widely used approaches in distributed training: data parallelism and tensor parallelism.\n",
        "\n",
        "In Section 1, we build a `DistributedContext` class, which coordinates communication across multiple processes (e.g., training steps running on different GPUs). This simulates the role of a real distributed backend such as torch.distributed. However, to keep the assignment lightweight, our setting uses Python threads on a single CPU as though they were running on separate devices.\n",
        "\n",
        "Using `DistributedContext`, the data parallel and tensor parallel implementations in Sections 2 and 3 will communicate the necessary activations and gradients during the forward and backward passes. For this assignment, we return to the simple MLP model we worked with earlier in the semester so that the focus remains on understanding the communication patterns rather than model complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841d8662",
      "metadata": {
        "id": "841d8662"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c23a4cb",
      "metadata": {
        "id": "7c23a4cb"
      },
      "source": [
        "# 0. Setup\n",
        "\n",
        "Here, we define the SimpleMLP class and the reference_step() to set up a non-distributed baseline for training. reference_step() performs a simple forward and backward pass on a single worker with no communication involved.\n",
        "\n",
        "We will compare the outputs of the distributed training steps against the outputs of reference_step() to make sure our distributed training implementations produce numerically consistent updates with the non-distributed version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691da286",
      "metadata": {
        "id": "691da286"
      },
      "outputs": [],
      "source": [
        "################## DO NOT CHANGE ##################\n",
        "class SimpleMLP(nn.Module):\n",
        "    \"\"\"Small MLP used as reference model.\"\"\"\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_in, d_hidden, bias=False)\n",
        "        self.act = nn.GELU()\n",
        "        self.lin2 = nn.Linear(d_hidden, d_out, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.act(self.lin1(x)))\n",
        "\n",
        "def reference_step(model, optimizer, x, y, loss_fn=nn.MSELoss()):\n",
        "    \"\"\"Single-device forward+backward+step. Returns loss scalar and output.\"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf17fedf",
      "metadata": {
        "id": "cf17fedf"
      },
      "source": [
        "# Part 1: Communication during parallel training (1 point)\n",
        "\n",
        "In distributed training setup, we need a context manager to coordinate the sending and receiving of tensors throughout training. Its key role is to collect the corresponding tensors from each worker, aggregate them, and return the aggregated results back to all processes. This class simulates torch.distributed, while differing in important ways. For instance, it uses Python threads and shared memory instead of actual multi-process, multi-GPU communication backends like NCCL, meaning that no real network transfers, device synchronization, or CUDA semantics are involved.\n",
        "\n",
        "Here, we will implement a simple DistributedContext by completing the all_gather() and all_reduce() functions:\n",
        "\n",
        "* **all_gather()**: gather all tensors from the workers and return a list of tensors (sorted in the order of the corresponding ranks).\n",
        "* **all_reduce()**: gather all tensors from the workers, which are then aggregated with a reduction operation. You can only implement the *sum* and *mean* operations in our assignment.\n",
        "\n",
        "For high-level introduction to the collective operations (all reduce, all gather, etc.):\n",
        "* https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439e829f",
      "metadata": {
        "id": "439e829f"
      },
      "outputs": [],
      "source": [
        "class DistributedContext:\n",
        "    \"\"\"Lightweight simulation of torch.distributed for threaded workers.\"\"\"\n",
        "\n",
        "    def __init__(self, world_size: int):\n",
        "        self.world_size = world_size\n",
        "        self.barrier = threading.Barrier(world_size)\n",
        "        self.barrier_lock = threading.Lock()\n",
        "        self.all_gather_buffer = {}\n",
        "        self.all_reduce_buffer = {}\n",
        "\n",
        "    def all_gather(self, tensor, rank: int, group_id: str) -> List:\n",
        "        \"\"\"Gather tensors from workers into a single list\"\"\"\n",
        "        with self.barrier_lock:\n",
        "            ################## YOUR CODE ##################\n",
        "            # TODO: Collect the tensors in the self.all_gather_buffer\n",
        "            # Hint: Use group_id to store the gathered tensors from the rank r;\n",
        "            #       e.g. self.all_gather_buffer[group_id][r]\n",
        "            pass\n",
        "            ################################################\n",
        "\n",
        "        self.barrier.wait() # wait to collect results from all workers\n",
        "\n",
        "        ################## YOUR CODE ##################\n",
        "        # TODO: compute the final all-gather tensor to return\n",
        "        result = []\n",
        "        ################################################\n",
        "\n",
        "        self.barrier.wait() # wait until result is calculated across all workers\n",
        "\n",
        "        ## empty the buffer\n",
        "        if rank == 0:\n",
        "            with self.barrier_lock:\n",
        "                del self.all_gather_buffer[group_id]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def all_reduce(self, tensor: torch.Tensor, rank: int, group_id: str, op: str) -> torch.Tensor:\n",
        "        \"\"\"Sum-reduce tensors across workers.\"\"\"\n",
        "        with self.barrier_lock:\n",
        "            ################## YOUR CODE ##################\n",
        "            # TODO: collect the tensors in the self.all_reduce_buffers\n",
        "            #       e.g. self.all_reduce_buffer[group_id][r]\n",
        "            pass\n",
        "            ################################################\n",
        "\n",
        "        self.barrier.wait() # wait to collect results from all workers\n",
        "\n",
        "        if op == 'sum':\n",
        "        ################## YOUR CODE ##################\n",
        "        # TODO: compute the final all-reduce tensors to return (summed)\n",
        "            result = None\n",
        "        ################################################\n",
        "\n",
        "        elif op == 'mean':\n",
        "        ################## YOUR CODE ##################\n",
        "        # TODO: compute the final all-reduce tensors to return (averaged)\n",
        "            result = None\n",
        "        ################################################\n",
        "\n",
        "        self.barrier.wait() # wait until gathered is done being calculated\n",
        "\n",
        "        ## empty the buffer\n",
        "        if rank == 0:\n",
        "            with self.barrier_lock:\n",
        "                del self.all_reduce_buffer[group_id]\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0007a629",
      "metadata": {
        "id": "0007a629"
      },
      "source": [
        "Check whether the all_gather and all_reduce functions return the same results across all processes:\n",
        "\n",
        "For instance, to test the correctness of your all_gather function:\n",
        "\n",
        "```\n",
        "num_process = 4\n",
        "context = DistributedContext(num_process)\n",
        "x_in = torch.randn(20,16) # some random input\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_process) as executor:\n",
        "    futures = [executor.submit(context.all_gather, x_in, rank, 'test_allgather') for rank in range(num_process)]\n",
        "    fin = [f.result() for f in as_completed(futures)]\n",
        "\n",
        "# Then check the results in the fin\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb21fb5",
      "metadata": {
        "id": "5cb21fb5"
      },
      "source": [
        "# Part 2: Data Parallelism (2 points)\n",
        "\n",
        "In this section, we will build a simple implementation of data parallel (DP) training.\n",
        "\n",
        "In data parallel training, the goal is to train with larger effective batch sizes by distributing the batch across multiple workers. Suppose we have a model $M$, dataset $D$, and $n$ GPUs. A batch $B$ is split into $n$ micro-batches $b_1, ..., b_n$, and each micro-batch is processed by one replica of the model ($M_1, M_2, ..., M_n$). Each replica computes its own gradients $G_1, ..., G_n$.\n",
        "\n",
        "At the end of the backward pass, these gradients are aggregated into a single gradient $G$ and is communicated back to all replicas. After one training step, the effect is (almost!) equivalent to training the full batch $B$ on a single model $M$.\n",
        "\n",
        "In real-world DP (e.g., PyTorch DistributedDataParallel), each replica receives a different micro-batch, computes gradients locally, and then an all-reduce operation synchronizes the gradients before the optimizer step. In this homework, we simulate $n$-way data parallelism inside a single process by splitting the batch into $n$ chunks and making $n$ asychronous calls to run the model forward and backward steps.\n",
        "\n",
        "**TODO**\n",
        "* Implement `split_batch()`: Given a data tensor `x`, the worker `rank`, and the `world_size`, return the correct slice of the batch. You should also handle the case where the batch size is not perfectly divisible by the number of workers.\n",
        "* Implement `dp_step()`: This simulates the training step for rank-i process. Get a slice of the batch, run forward and backward on the local model copy, call `all_reduce()` to synchronize gradients across processes, apply the optimizer update, and return the loss.\n",
        "    * In practice, torch's implementation of distributed data parallel uses autograd hooks registered at construction time to trigger gradients synchronizations during the backward pass. However, to simplify the assignment, we simply synchronize all gradients once at the end of the loss computation (TODO 2 and 3 of dp_step).\n",
        "\n",
        "\n",
        "**NOTE**\n",
        "* You can read more about torch's distributed data parallelism here: https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d77638",
      "metadata": {
        "id": "30d77638"
      },
      "outputs": [],
      "source": [
        "def split_batch(x, rank, world_size):\n",
        "\n",
        "    ######################## YOUR CODE ########################\n",
        "    # TODO: Given a tensor of shape (batch_size, input_dimension),\n",
        "    #       return a microbatch sized (batch_size // world_size, input_dimension)\n",
        "    #                              or (batch_size // world_size + 1, input_dimension)\n",
        "    # NOTE: You should also handle the case where data_size % world_size != 0\n",
        "    #       No examples should be dropped.\n",
        "\n",
        "    return x\n",
        "    ###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b879f8bc",
      "metadata": {
        "id": "b879f8bc"
      },
      "outputs": [],
      "source": [
        "def dp_step(rank, world_size, model, optimizer, fullbatch_x, fullbatch_y, dist_context, epoch_num, loss_fn=nn.MSELoss()):\n",
        "\n",
        "    ######################### YOUR CODE #########################\n",
        "    # TODO 1: split the batch into a list of minibatches\n",
        "    minibatch_x = None\n",
        "    minibatch_y = None\n",
        "    #############################################################\n",
        "\n",
        "    out = model(minibatch_x)\n",
        "    loss = loss_fn(out, minibatch_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    ######################### YOUR CODE #########################\n",
        "    # TODO 2: Gather gradients inot a single flat vector, and call all_reduce\n",
        "    #         using dist_context, which is an instance of DistributedContext(world_size)\n",
        "    #         Define the avg_grad = summed gradient from all processes / total # workers\n",
        "\n",
        "    ############################################################\n",
        "\n",
        "    ######################### YOUR CODE #########################\n",
        "    # TODO 3: Apply the gradient (avg_grad) from the previous step\n",
        "    #         You can use model.parameters() to iterate through the parameters,\n",
        "    #         and set parameter gradient = slice of the aggregated gradient\n",
        "\n",
        "    ############################################################\n",
        "\n",
        "    # Once the gradients are propagated, all optimizer.step() updates the params\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.detach().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a28b9b2",
      "metadata": {
        "id": "4a28b9b2"
      },
      "source": [
        "We use `run_dp_process` to launch simulated distributed training. Each process holds its own model replica and optimizer, while communicating through the shared `dist_context`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba68de5",
      "metadata": {
        "id": "1ba68de5"
      },
      "outputs": [],
      "source": [
        "########################## DO NOT MODIFY ##########################\n",
        "def run_dp_process(world_size, x, y, epoch, base_model, step, dist_context):\n",
        "\n",
        "    models = [copy.deepcopy(base_model) for _ in range(world_size)]\n",
        "    optimizers = [torch.optim.SGD(m.parameters(), lr=0.05) for m in models]\n",
        "\n",
        "    dist_context = dist_context(world_size=world_size)\n",
        "\n",
        "    train_losses = []\n",
        "    for i in range(epoch):\n",
        "        with ThreadPoolExecutor(max_workers=world_size) as executor:\n",
        "            futures = [executor.submit(step,\n",
        "                                    rank,\n",
        "                                    world_size,\n",
        "                                    models[rank],\n",
        "                                    optimizers[rank],\n",
        "                                    x, y,\n",
        "                                    dist_context,\n",
        "                                    i) for rank in range(world_size)]\n",
        "            losses = [f.result() for f in as_completed(futures)]\n",
        "\n",
        "            avg_loss = sum(losses) / len(losses)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0b1a3b",
      "metadata": {
        "id": "5b0b1a3b"
      },
      "source": [
        "Check that your DP training outcome and non-DP training outcome are the same:\n",
        "\n",
        "```\n",
        "epoch = N\n",
        "world_size=4\n",
        "x = torch.randn(20,16)\n",
        "y = torch.rand(20,2)\n",
        "model = SimpleMLP(16,32,2)\n",
        "\n",
        "dp_losses = run_dp_process(world_size, x, y, epoch, model, dp_step, DistributedContext)\n",
        "\n",
        "nondp_losses = []\n",
        "for i in range(epoch):\n",
        "    ref_loss = reference_step(model, torch.optim.SGD(model.parameters(), 0.05), x, y)\n",
        "    nondp_losses.append(ref_loss)\n",
        "\n",
        "np.allclose(dp_losses[-1], nondp_losses[-1], atol=1e-6)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422815b4",
      "metadata": {
        "id": "422815b4"
      },
      "outputs": [],
      "source": [
        "# You can also check that your DP training returns the same loss as the non-Dp training!\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "    x = torch.tensor(x).to(torch.float)\n",
        "    y = torch.tensor(y).to(torch.float).reshape(-1,1)\n",
        "\n",
        "    world_size = 10\n",
        "    num_epoch = 10\n",
        "    base_model = SimpleMLP(2,16,1)\n",
        "    dp_model = copy.deepcopy(base_model)\n",
        "    nondp_model = copy.deepcopy(base_model)\n",
        "\n",
        "    # DP run\n",
        "    train_losses = run_dp_process(world_size=world_size,\n",
        "                                           base_model=dp_model,\n",
        "                                           x=x, y=y,\n",
        "                                           epoch=num_epoch,\n",
        "                                           step=dp_step,\n",
        "                                           dist_context=DistributedContext)\n",
        "\n",
        "    # non-DP run\n",
        "    train_losses_ref = []\n",
        "    for i in range(num_epoch):\n",
        "        ref_loss = reference_step(nondp_model, torch.optim.SGD(nondp_model.parameters(), 0.05), x, y)\n",
        "        train_losses_ref.append(ref_loss)\n",
        "\n",
        "    plt.plot(train_losses, label='dp run')\n",
        "    plt.plot(train_losses_ref, label='non-dp run')\n",
        "    plt.title(\"DP train loss vs. non-DP train loss\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c2f24d",
      "metadata": {
        "id": "51c2f24d"
      },
      "source": [
        "# Part 3 TensorParallel Layers (3.5 points)\n",
        "\n",
        "For tensor parallelism, you need tensor-parallel–specific layers to perform both column-parallel and row-parallel tensor sharding. The advantage of this approach is improved memory efficiency: each process only stores a slice of the full weight matrix instead of the entire layer. As a result, each process also stores proportionally smaller input cache and gradients. However, this comes at the cost of additional communication, since processes must exchange partial results during both the forward and backward passes.\n",
        "\n",
        "Both layers inherit from the shared TensorParallelLinear base class.\n",
        "\n",
        "* **ColumnParallelLinear**: A child class of TensorParallelLinear. It splits the full linear layer’s weight matrix along its column dimension, and each process holds a subset of the output features. During the forward pass, each process computes its partial output, and then performs a collective communication step (all-gather) so that the full output tensor becomes globally available.\n",
        "\n",
        "* **RowParallelLinear**:  A child class of TensorParallelLinear. It splits the weight matrix along its row dimension, and each process holds only the rows relevant to its shard of the input features. Each process computes a partial output from its local rows, and these partial outputs are then combined across processes using an all-reduce so that the full output is reconstructed.\n",
        "\n",
        "Because the input and output partitions differ across these two sharding schemes, they each require their own specialized *forward()* method. In addition, they need customized *backward()* logic: while gradients w.r.t the sharded weights can be computed locally without communication, gradients w.r.t. the input require an additional communication (e.g., all-reduce for column-parallel layers or all-gather for row-parallel layers). Again, we avoid using autograd and do it the manual way in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965f86e8",
      "metadata": {
        "id": "965f86e8"
      },
      "outputs": [],
      "source": [
        "########################## DO NOT CHANGE ##########################\n",
        "class TensorParallelLinear(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 output_size: int,\n",
        "                 rank: int, world_size: int,\n",
        "                 dist_context: DistributedContext,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.rank = rank\n",
        "        self.world_size = world_size\n",
        "        self.dist_context = dist_context\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError(\"Child class must implement backward()\")\n",
        "\n",
        "    def backward(self):\n",
        "        raise NotImplementedError(\"Child class must implement backward()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805fb96d",
      "metadata": {
        "id": "805fb96d"
      },
      "source": [
        "## Part 3.1: Column-parallel layer (1.5 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae4950e",
      "metadata": {
        "id": "8ae4950e"
      },
      "outputs": [],
      "source": [
        "class ColumnParallelLinear(TensorParallelLinear):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        assert self.output_size % self.world_size == 0\n",
        "        self.output_size_per_partition = self.output_size // self.world_size\n",
        "        self.weight = nn.Parameter(torch.randn(self.input_size, self.output_size_per_partition)) # nn.Linear(self.input_size, self.output_size_per_partition)\n",
        "        self.input_cache = None\n",
        "        self.forward_counter = 0\n",
        "        self.backward_counter = 0\n",
        "\n",
        "        print(f\"  [Rank {self.rank}] initialized column parallel layer, weight shape: {self.weight.shape}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        ############################ YOUR CODE ############################\n",
        "        # TODO 1: Compute local output using the sharded weight (self.weight)\n",
        "        #         Store the input cache for backprop later\n",
        "\n",
        "        # TODO 2: Perform all-gather to collect outputs from all workers;\n",
        "        #         The final output should be equivalent to the output computed without parallelism.\n",
        "        if self.world_size > 1:\n",
        "            pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "        ###################################################################\n",
        "\n",
        "    def backward(self, grad_upstream):\n",
        "\n",
        "        ############################ YOUR CODE ############################\n",
        "        # TODO 1: Given the full upstream gradient, only take the gradient corresponding to this shard\n",
        "        #         The upstream gradient grad_upstream is w.r.t. the full output,\n",
        "        #         but each rank only needs the parts corresponding to its columns\n",
        "        #         Hint: use self.rank, self.output_size_per_partition to select the corresponding *columns* of the grad_upstream\n",
        "\n",
        "        # TODO 2: Compute gradient w.r.t. weights and store the gradient inside self.weight.grad\n",
        "\n",
        "        # TODO 3: output the gradient w.r.t. the input using all-reduce\n",
        "\n",
        "        ###################################################################\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4507dd6b",
      "metadata": {
        "id": "4507dd6b"
      },
      "source": [
        "You can check the correctness of your implementation by launching a threadpool executor:\n",
        "\n",
        "```\n",
        "world_size=N\n",
        "in_features=A\n",
        "out_features=B\n",
        "context = DistributedContext(world_size)\n",
        "x_in = torch.randn(50,in_features)\n",
        "grad_upstream = torch.ones(50,out_features)\n",
        "\n",
        "layers = [ColumnParallelLinear(input_size=in_features,output_size=out_features,rank=i,world_size=world_size,dist_context=context) for i in range(world_size)]\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=world_size) as executor:\n",
        "    futures = [executor.submit(layers[rank].forward, x_in) for rank in range(world_size)]\n",
        "    fin_forward = [f.result() for f in as_completed(futures)]\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(layers[rank].backward, grad_upstream) for rank in range(world_size)]\n",
        "    fin_grad = [f.result() for f in as_completed(futures)]\n",
        "\n",
        "# check the forward() result and backward() result\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9866a526",
      "metadata": {
        "id": "9866a526"
      },
      "source": [
        "## 3.2 Row-parallel layer (1.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959b6b1d",
      "metadata": {
        "id": "959b6b1d"
      },
      "outputs": [],
      "source": [
        "class RowParallelLinear(TensorParallelLinear):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        assert self.input_size % self.world_size == 0\n",
        "        self.input_size_per_partition = self.input_size // self.world_size\n",
        "        self.weight = nn.Parameter(torch.randn(self.input_size_per_partition, self.output_size))\n",
        "        self.input_cache = None\n",
        "        self.forward_counter = 0\n",
        "        self.backward_counter = 0\n",
        "        print(f\"  [Rank {self.rank}] Initialized row parallel layer, weight shape: {self.weight.shape}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        ############################ YOUR CODE ############################\n",
        "        # TODO 1: Compute local output using the sharded weight\n",
        "        #         For row-parallelism, you need to splice the input x\n",
        "        #         corresponding to the weights\n",
        "        #         Cache the sliced input used to compute the output\n",
        "\n",
        "        # TODO 2: Perform all-reduce to collect outputs from all workers;\n",
        "        #         Again, the final output should be equivalent to the output computed without parallelism.\n",
        "        if self.world_size > 1:\n",
        "            pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def backward(self, grad_upstream):\n",
        "\n",
        "        ############################ YOUR CODE ############################\n",
        "        # TODO 1: Compute gradient w.r.t. weights and store the gradient inside self.weight.grad\n",
        "        #         Unlike column-parallel case, row-parallel needs the full grad_upstream\n",
        "        #         to compute the gradient w.r.t. the weights\n",
        "\n",
        "        # TODO 2: output the gradient w.r.t. the input using all-gather\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e1de65",
      "metadata": {
        "id": "87e1de65"
      },
      "source": [
        "You can test the correctness of your implementation using a similar code snippet as above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b308c302",
      "metadata": {
        "id": "b308c302"
      },
      "source": [
        "## Part 3.3 Tensor-parallel training (0.5 point)\n",
        "\n",
        "Just like in the previous section, we will use the tensor-parallel layers from the previous part to train the model. We will also use threading and `tp_step` to instantiates each process, assigns the appropriate weight shards, runs the global forward pass, and then executes the global backward pass by coordinating the partial computations across all workers.\n",
        "\n",
        "Again, in practice, actual tensor-parallel training does not use threads—each worker runs in its own process, typically one process per GPU.\n",
        "\n",
        "In this section, you will implement the following functions for you to run the tensor-parallel training:\n",
        "\n",
        "* replace_to_tp_simple_mlp(): manually maps original model layers to tensor-parallel layers (if it's nn.Linear model, either with ColumnParallelLinear or RowParallelLinear).\n",
        "\n",
        "We provide the following code to simulate the distributed training:\n",
        "* tp_step(): a single forward and backward pass using the tensor-parallel model\n",
        "* run_tp_process(): simulated distributed training environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51ca3d2",
      "metadata": {
        "id": "c51ca3d2"
      },
      "outputs": [],
      "source": [
        "########################## DO NOT MODIFY ##########################\n",
        "def replace_to_tp_simple_mlp(base_model: SimpleMLP,\n",
        "                             rank: int,\n",
        "                             world_size: int,\n",
        "                             dist_context: DistributedContext,\n",
        "                             tp_map: dict\n",
        "                             ):\n",
        "\n",
        "    model = copy.deepcopy(base_model)\n",
        "    base_modules = dict(base_model.named_modules())\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if name not in tp_map:\n",
        "            continue\n",
        "\n",
        "        d_in, d_out = module.in_features, module.out_features\n",
        "        if tp_map[name] == 'column':\n",
        "            setattr(model, name, ColumnParallelLinear(d_in, d_out, rank, world_size, dist_context))\n",
        "            output_size_tp = d_out // world_size\n",
        "            slice_tp = slice(rank * output_size_tp, (rank + 1) * output_size_tp)\n",
        "            weight_slice = base_modules[name].weight.T[:, slice_tp]\n",
        "\n",
        "        elif tp_map[name] == 'row':\n",
        "            setattr(model, name, RowParallelLinear(d_in, d_out, rank, world_size, dist_context))\n",
        "            output_size_tp = d_in // world_size\n",
        "            slice_tp = slice(rank * output_size_tp, (rank + 1) * output_size_tp)\n",
        "            weight_slice = base_modules[name].weight.T[slice_tp, :]\n",
        "\n",
        "        new_modules = dict(model.named_modules())\n",
        "        with torch.no_grad():\n",
        "            new_modules[name].weight.copy_(weight_slice)\n",
        "\n",
        "    model.layers = [model.lin1, model.act, model.lin2]\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8a6c22",
      "metadata": {
        "id": "9e8a6c22"
      },
      "outputs": [],
      "source": [
        "########################## DO NOT MODIFY ##########################\n",
        "def run_tp_process(world_size, x, y, epoch, base_model, step, dist_context, tp_dict):\n",
        "\n",
        "    dist_context = dist_context(world_size=world_size)\n",
        "    models = [replace_to_tp_simple_mlp(base_model, rank, world_size, dist_context, tp_dict)\n",
        "              for rank in range(world_size)\n",
        "            ]\n",
        "    optimizers = [torch.optim.SGD(model.parameters(), 0.05) for model in models]\n",
        "\n",
        "    train_losses = []\n",
        "    for i in range(epoch):\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=world_size) as executor:\n",
        "            futures = [executor.submit(step,\n",
        "                                       rank,\n",
        "                                       world_size,\n",
        "                                       models[rank],\n",
        "                                       optimizers[rank],\n",
        "                                       x,y,\n",
        "                                       dist_context,\n",
        "                                       i) for rank in range(world_size)]\n",
        "            losses = [f.result() for f in as_completed(futures)]\n",
        "\n",
        "            avg_loss = sum(losses) / len(losses)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227ca51c",
      "metadata": {
        "id": "227ca51c"
      },
      "outputs": [],
      "source": [
        "############################## DO NOT MODIFY ##############################\n",
        "def tp_step(rank: int,\n",
        "            world_size: int,\n",
        "            model,\n",
        "            optimizer,\n",
        "            input_data: torch.Tensor,\n",
        "            target: torch.Tensor,\n",
        "            dist_context: DistributedContext,\n",
        "            loss_fn=nn.MSELoss()\n",
        "            ):\n",
        "\n",
        "    # Step 1: forward pass\n",
        "    input_data = input_data.clone().detach()\n",
        "    target = target.clone().detach()\n",
        "    output = model.forward(input_data)\n",
        "    loss = nn.functional.mse_loss(output, target)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    dist_context.barrier.wait()\n",
        "\n",
        "    # Step 2: Instead of loss.backward(), perform (manual) gradient back-propagation\n",
        "    # NOTE:   This is a simple workaround defining a custom autograd function for tensor-parallel operations\n",
        "    grad = 2 * (output - target) / output.numel()\n",
        "    for layer in model.layers[::-1]:\n",
        "        if hasattr(layer, 'backward'):\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "    dist_context.barrier.wait()\n",
        "\n",
        "    # Step 3: update the model gradient step\n",
        "    optimizer.step()\n",
        "\n",
        "    dist_context.barrier.wait()\n",
        "\n",
        "    return loss.detach().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cba057a",
      "metadata": {
        "id": "9cba057a"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    world_size = 2\n",
        "    num_epoch = 5\n",
        "    x, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "    x = torch.tensor(x).to(torch.float)\n",
        "    y = torch.tensor(y).to(torch.float).reshape(-1,1)\n",
        "    base_model = SimpleMLP(2,32,1)\n",
        "    tp_model = copy.deepcopy(base_model)\n",
        "    nontp_model = copy.deepcopy(base_model)\n",
        "    tp_dict = {'lin1': 'column', 'lin2': 'row'}\n",
        "\n",
        "    train_losses = run_tp_process(world_size=world_size,\n",
        "                                  base_model=tp_model,\n",
        "                                  x=x, y=y,\n",
        "                                  epoch=num_epoch,\n",
        "                                  step=tp_step,\n",
        "                                  dist_context=DistributedContext,\n",
        "                                  tp_dict=tp_dict\n",
        "                                  )\n",
        "\n",
        "    # non-DP run\n",
        "    train_losses_ref = []\n",
        "    for i in range(num_epoch):\n",
        "        ref_loss = reference_step(nontp_model, torch.optim.SGD(nontp_model.parameters(), 0.05), x, y)\n",
        "        train_losses_ref.append(ref_loss)\n",
        "\n",
        "    plt.plot(train_losses, label='tp_loss')\n",
        "    plt.plot(train_losses_ref, label='org_loss')\n",
        "    plt.title(\"TP train loss vs. non-TP train loss\")\n",
        "    plt.legend()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "dataid_env2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}