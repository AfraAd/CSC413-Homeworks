{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfraAd/CSC413-Homeworks/blob/main/Fall25_Hw7_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218e3cb4",
      "metadata": {
        "id": "218e3cb4"
      },
      "source": [
        "# Homework 7 - Attention\n",
        "CSC413/2516: Neural Networks and Deep Learning\n",
        "\n",
        "As with previous homeworks, replace \"## Your Code\" lines with your implementation.\n",
        "\n",
        "\n",
        "In this homework, you will implement various attention mechanisms to solve\n",
        "sequence problems. You'll start with basic attention pooling and progressively\n",
        "build up to multi-head attention with different operations.\n",
        "\n",
        "Grading:\n",
        "- Data collation: 0.5\n",
        "- Attention: 3.8\n",
        "- Trainer and training: 0.9\n",
        "- Multi-head attention: 1.3\n",
        "\n",
        "You don't need to use GPU for this homework.\n",
        "\n",
        "Version: 1.0.2\n",
        "\n",
        "Changelog:\n",
        "- 1.0.1 (Oct 17, 12:01): Changed the default `variable_len=False` to True in the `SequenceData` creation.\n",
        "- 1.0.2 (Oct 17, 13:57): Added `set_seed` statement before the model creation in the single head attention training loops."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0055d4c4",
      "metadata": {
        "id": "0055d4c4"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "70572605",
      "metadata": {
        "id": "70572605"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Literal, Dict, Optional\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def set_seed():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5471aad8",
      "metadata": {
        "id": "5471aad8"
      },
      "source": [
        "# Attention Pooling\n",
        "\n",
        "\n",
        "First, you will implement a form of attention called *attention pooling* to solve the \"addition problem\". The addition problem was introduced in [the LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf) as a way to test whether an RNN could propagate information across many time steps. In the addition problem, the model is given a sequence of 2D vectors in the format:\n",
        "\n",
        "| Pos 0 | Pos 1 | Pos 2 | Pos 3 | Pos 4 | ... | ... | ... | ... |  ... |  ... |\n",
        "|-----|------|-----|-----|------|-----|------|-----|-----|-----|-----|\n",
        "| 0.5 | -0.7 | 0.3 | 0.1 | -0.2 | ... | -0.5 | 0.9 | ... | 0.8 | 0.2 |\n",
        "| 0   |   0  |  1  |  0  |   0  |     |   0  |  1  |     |  0  |  0  |\n",
        "\n",
        "The first dimension of each vector in the sequence is a random number between 0 and 1. The second dimension is 0 for all entries of the sequence except for 2 of the entries, where it is 1. The goal of the addition problem is to output the sum of the values in the first dimension at the two indices where the second dimension is 1. In the example above, the target would be 0.9 + 0.3 = 1.2. Below is a code snippet that generates a sequence and its target for the addition problem.\n",
        "\n",
        "The dataset may contain sequences of different lenghts, i.e. one example could have 50 items while another could have only 10. Tensors should have coherent shapes, i.e. each sample should have the same number of items. Your first task is to implement the `collate_fn` where you will append `PAD_ID` to the shorter sequences so that each sequence will appear the same length. Later when you are implementing the model, you will ignore these indices so that the function is computed only on the actual elements of the sequence.\n",
        "\n",
        "You can read more about how PyTorch's dataloader processes samples [here](https://docs.pytorch.org/docs/stable/data.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "7bfff84f",
      "metadata": {
        "id": "7bfff84f"
      },
      "outputs": [],
      "source": [
        "## don't change the following code\n",
        "\n",
        "PADDING_ID = 100\n",
        "NUM_SAMPLES = 1000\n",
        "MAX_SEQ_LEN = 50\n",
        "\n",
        "\n",
        "def generate_sequence_problem(\n",
        "    sequence_length: int = 50,\n",
        "    k: int = 2,\n",
        "    problem_type: Literal[\"add\", \"multiply\", \"mask\"] = \"add\",\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Generate a k-way sequence problem.\n",
        "\n",
        "    Args:\n",
        "        sequence_length: Length of the sequence\n",
        "        k: Number of positions to mark (default 2)\n",
        "        problem_type: 'add', 'multiply', or 'average'\n",
        "\n",
        "    Returns:\n",
        "        sequence: tensor of shape (sequence_length, 2)\n",
        "        target: scalar tensor with the target value\n",
        "\n",
        "    The sequence has:\n",
        "    - First dimension: random values in [-1, 1]\n",
        "    - Second dimension: 0 everywhere except k positions marked with 1\n",
        "\n",
        "    Targets:\n",
        "    - 'add': sum of marked values\n",
        "    - 'multiply': product of marked values\n",
        "    - 'average': mean of marked values\n",
        "    \"\"\"\n",
        "    sequence = np.zeros((sequence_length, 2))\n",
        "    ## Randomly sample the numbers in the input sequence\n",
        "    sequence[:, 0] = np.random.uniform(-1, 1, sequence_length)\n",
        "\n",
        "    random_indices = np.random.choice(sequence_length, size=k, replace=False)\n",
        "    sequence[random_indices, 1] = 1\n",
        "\n",
        "    marked_values = sequence[random_indices, 0]\n",
        "\n",
        "    if problem_type == \"add\":\n",
        "        target = marked_values.sum()\n",
        "    elif problem_type == \"multiply\":\n",
        "        target = marked_values.prod()\n",
        "    elif problem_type == \"average\":\n",
        "        target = marked_values.mean()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown problem_type: {problem_type}\")\n",
        "\n",
        "    return torch.tensor(sequence, dtype=torch.float32), torch.tensor(\n",
        "        [target], dtype=torch.float32\n",
        "    )\n",
        "\n",
        "\n",
        "class SequenceData(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_samples: int = NUM_SAMPLES,\n",
        "        max_seq_len: int = MAX_SEQ_LEN,\n",
        "        padding_id: int = PADDING_ID,\n",
        "        k: int = 2,\n",
        "        problem_type: Literal[\"add\", \"multiply\", \"average\"] = \"add\",\n",
        "        variable_len: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            k: Number of positions to mark\n",
        "            variable_len: If True, generate sequences of random length between k and max_seq_len\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.padding_id = padding_id\n",
        "        self.k = k\n",
        "        self.max_seq_len = max_seq_len\n",
        "        data = [\n",
        "            generate_sequence_problem(\n",
        "                sequence_length=random.randint(k, MAX_SEQ_LEN)\n",
        "                if variable_len\n",
        "                else max_seq_len,\n",
        "                k=k,\n",
        "                problem_type=problem_type,\n",
        "            )\n",
        "            for _ in range(num_samples)\n",
        "        ]\n",
        "        self.X = [d[0] for d in data]\n",
        "        self.y = [d[1] for d in data]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = {\n",
        "            \"inputs\": self.X[index],\n",
        "            \"targets\": self.y[index],\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "cb3f1cf5",
      "metadata": {
        "id": "cb3f1cf5"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch: List[Dict[str, torch.Tensor]]):\n",
        "    targets = None\n",
        "    inputs = None\n",
        "    attention_mask = None\n",
        "    ##########################################\n",
        "    ## 0.5 points\n",
        "    ## TODO: 1. Extract targets from batch\n",
        "\n",
        "    ## TODO: Pad the input so that we can batch process it\n",
        "    ## Hint: You might want to use `nn.utils.rnn.pad_sequence` with batch_first\n",
        "\n",
        "    ## TODO: We don't want padding indices to be considered for attention\n",
        "    ## attention_mask should have 1 for PADDING positions and 0 for VALID positions\n",
        "    ## (This follows the convention used in most attention implementations)\n",
        "\n",
        "    ##########################################\n",
        "    targets = [item[\"targets\"] for item in batch]\n",
        "    inputs = [item[\"inputs\"] for item in batch]\n",
        "    inputs_padded = nn.utils.rnn.pad_sequence(\n",
        "        inputs,\n",
        "        batch_first=True,\n",
        "        padding_value=PADDING_ID\n",
        "    )\n",
        "    attention_mask = (inputs_padded == PADDING_ID)\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=PADDING_ID)\n",
        "    targets = torch.stack(targets)\n",
        "    return {\n",
        "        \"inputs\": inputs,\n",
        "        \"targets\": targets,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5117d87",
      "metadata": {
        "id": "f5117d87"
      },
      "source": [
        "\n",
        "Attention pooling allows a model to reduce a variable-length sequence $\\{h_1, h_2, \\ldots, h_T\\}$ to a fixed-length representation through a learned weighted average:\n",
        "\n",
        "\\begin{align}\n",
        "e_t &= \\mathrm{a}(q, h_t) \\quad \\text{(compute attention scores)} \\\\\n",
        "\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_k \\exp(e_k)} \\quad \\text{(normalize to probabilities)} \\\\\n",
        "c &= \\sum_{t = 1}^T \\alpha_t h_t \\quad \\text{(weighted sum)}\n",
        "\\end{align}\n",
        "\n",
        "The key component is the **attention energy function** $\\mathrm{a}(q, h_t)$, which computes how much the model should attend to each position. The query $q$ is a learnable parameter, and $h_t \\in \\mathbb{R}^d$ is the input at position $t$.\n",
        "\n",
        "### Attention Mechanisms\n",
        "\n",
        "You will implement three different attention energy functions:\n",
        "\n",
        "**1. Bahdanau (Additive) Attention:**\n",
        "$$\\mathrm{a}(q, h_t) = q^\\top \\tanh(W_a h_t + b_a)$$\n",
        "where $q \\in \\mathbb{R}^q$, $W_a \\in \\mathbb{R}^{q \\times d}$, and $b_a \\in \\mathbb{R}^q$ are learnable parameters.\n",
        "\n",
        "**2. Multiplicative (Luong) Attention:**\n",
        "$$\\mathrm{a}(q, h_t) = q^\\top W_a h_t$$\n",
        "Similar to Bahdanau but without the $\\tanh$ nonlinearity.\n",
        "\n",
        "**3. Scaled Dot-Product Attention:**\n",
        "$$\\mathrm{a}(q, h_t) = \\frac{q^\\top h_t}{\\sqrt{d}}$$\n",
        "No weight matrix—just a scaled dot product. The scaling by $\\sqrt{d}$ prevents the dot products from becoming too large.\n",
        "\n",
        "\n",
        "### Network Architecture\n",
        "\n",
        "The attention pooling layer converts variable-length sequences into fixed-length vectors, allowing you to use it within a standard feed-forward network. A typical architecture is:\n",
        "\n",
        "```\n",
        "Input (batch_size, seq_len, input_dim)\n",
        "↓\n",
        "Dense Layer (position-wise)\n",
        "↓\n",
        "Attention Pooling → (batch_size, hidden_dim)\n",
        "↓\n",
        "Dense Layer\n",
        "↓\n",
        "Output Layer → (batch_size, output_dim)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f8392d",
      "metadata": {
        "id": "b2f8392d"
      },
      "source": [
        "# Implement Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "f1aed850",
      "metadata": {
        "id": "f1aed850"
      },
      "outputs": [],
      "source": [
        "## You are not allowed to use nn.Linear her\n",
        "## Inside your attention implementations use nn.Parameter only\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau (Additive) Attention Pooling.\n",
        "    Computes: e_t = q^T tanh(W_a h_t + b_a)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, query_dim: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_dim: Dimension of input features\n",
        "            query_dim: Dimension of attention space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        ##########################################\n",
        "        ## TODO: Initialize parameters for Bahdanau attention, 0.2 points\n",
        "        self.w = nn.Parameter(torch.zeros((query_dim, hidden_dim)))\n",
        "        self.b = nn.Parameter(torch.empty(query_dim))\n",
        "        self.query = nn.Parameter(torch.empty(query_dim))\n",
        "        nn.init.kaiming_uniform_(self.w)\n",
        "        nn.init.zeros_(self.b)\n",
        "        nn.init.uniform_(self.query, a=-0.1, b=0.1)\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                values: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (keys) of shape (batch_size, seq_len, hidden_dim)\n",
        "            attention_mask: Optional mask of shape (batch_size, seq_len)\n",
        "                          where True indicates positions to IGNORE (padding)\n",
        "            values: Optional separate values tensor of shape (batch_size, seq_len, value_dim)\n",
        "                   If None, use x as values (self-attention)\n",
        "        \"\"\"\n",
        "        # Use x as values if not provided (self-attention)\n",
        "        if values is None:\n",
        "            values = x\n",
        "        ##########################################\n",
        "        res = None\n",
        "        attention_weights = None\n",
        "        ## TODO: Implement Bahdanau attention, 0.8 points\n",
        "        # 1. Apply weight transformation to keys\n",
        "        weighted_keys = torch.einsum('btd,qd->btq', x, self.w)\n",
        "        # 2. Apply tanh\n",
        "        tanh_out = torch.tanh(weighted_keys + self.b)\n",
        "        # 3. Compute energies\n",
        "        energies = torch.einsum('btq,q->bt', tanh_out, self.query)\n",
        "        # 4. Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.dim() > 2:\n",
        "                mask_condition = (attention_mask == 0).all(dim=-1)\n",
        "            else:\n",
        "                mask_condition = attention_mask == 0\n",
        "            energies = energies.masked_fill(mask_condition, -float('inf'))\n",
        "        # 5. Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(energies, dim=1)\n",
        "        # 6. Apply attention weights to values\n",
        "        res = torch.einsum('bt,btd->bd', attention_weights, values)\n",
        "        ##########################################\n",
        "        return res, attention_weights\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention Pooling.\n",
        "    Computes: e_t = (q^T h_t) / sqrt(d)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.scale = np.sqrt(hidden_dim)\n",
        "\n",
        "        ##########################################\n",
        "        ## TODO: Initialize query parameter, 0.1 points\n",
        "        self.query = nn.Parameter(torch.empty(hidden_dim))\n",
        "        nn.init.uniform_(self.query, a=-0.1, b=0.1)\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                values: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (keys) of shape (batch_size, seq_len, hidden_dim)\n",
        "            attention_mask: Optional mask of shape (batch_size, seq_len)\n",
        "                          where True indicates positions to IGNORE\n",
        "            values: Optional separate values tensor of shape (batch_size, seq_len, value_dim)\n",
        "                   If None, use x as values (self-attention)\n",
        "        \"\"\"\n",
        "        # Use x as values if not provided (self-attention)\n",
        "        if values is None:\n",
        "            values = x\n",
        "        ##########################################\n",
        "        res = None\n",
        "        attention_weights = None\n",
        "        ## TODO: Implement Scaled Dot-Product attention, 0.65 points\n",
        "        # 1. Compute dot product and scale\n",
        "        energies = torch.einsum('btd,d->bt', x, self.query) / self.scale\n",
        "        # 2. Apply mask if provided\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.dim() > 2:\n",
        "                attention_mask = (attention_mask != 0).all(dim=-1).long()\n",
        "            mask_expanded = attention_mask.unsqueeze(1)\n",
        "            energies = energies.masked_fill(mask_expanded == 0, -1e9)\n",
        "        # 3. Apply softmax\n",
        "        attention_weights = F.softmax(energies, dim=-1)\n",
        "        # 4. Apply attention weights to values\n",
        "        res = torch.einsum('bt,btd->bd', attention_weights, values)\n",
        "        ##########################################\n",
        "        return res, attention_weights\n",
        "\n",
        "\n",
        "class MultiplicativeAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiplicative (Luong) Attention Pooling.\n",
        "    Computes: e_t = q^T W_a h_t (no tanh)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, query_dim: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_dim: Dimension of input features\n",
        "            query_dim: Dimension of attention space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        ##########################################\n",
        "        ## TODO: Initialize parameters for Multiplicative attention, 0.1 points\n",
        "        ## No bias\n",
        "        self.w = nn.Parameter(torch.zeros((query_dim, hidden_dim)))\n",
        "        nn.init.kaiming_uniform_(self.w)\n",
        "        self.query = nn.Parameter(torch.zeros((query_dim)))\n",
        "        nn.init.uniform_(self.query, a=-0.1, b=0.1)\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                values: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (keys) of shape (batch_size, seq_len, hidden_dim)\n",
        "            attention_mask: Optional mask of shape (batch_size, seq_len)\n",
        "                          where True indicates positions to IGNORE (padding)\n",
        "            values: Optional separate values tensor of shape (batch_size, seq_len, value_dim)\n",
        "                   If None, use x as values (self-attention)\n",
        "        \"\"\"\n",
        "        # Use x as values if not provided (self-attention)\n",
        "        if values is None:\n",
        "            values = x\n",
        "        ##########################################\n",
        "        res = None\n",
        "        attention_weights = None\n",
        "        ## TODO: Implement Multiplicative attention, 0.75 points\n",
        "        # 1. Apply weight transformation to keys\n",
        "        weighted_keys = torch.einsum('btd,qd->btq', x, self.w)\n",
        "        # 2. Compute energies\n",
        "        energies = torch.einsum('btq,q->bt', weighted_keys, self.query)\n",
        "        # 3. Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            energies = energies.masked_fill(attention_mask == 0, -float('inf'))\n",
        "        # 4. Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(energies, dim=-1)\n",
        "        # 5. Apply attention weights to values\n",
        "        res = torch.einsum('bt,btd->bd', attention_weights, values)\n",
        "        ##########################################\n",
        "\n",
        "        return res, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "106547bb",
      "metadata": {
        "id": "106547bb"
      },
      "outputs": [],
      "source": [
        "class AttentionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete network with attention pooling.\n",
        "    Architecture: Dense -> Attention -> Dense -> Output\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        query_dim: int,\n",
        "        output_dim: int,\n",
        "        attention_type: Literal[\"bahdanau\", \"multiplicative\", \"scaled_dot\"] = \"bahdanau\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.query_dim = query_dim\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        ##########################################\n",
        "        ## TODO: Create the network architecture, 0.3 points\n",
        "        ## For these layers, you use nn.Linear\n",
        "        ## Inside your attention implementations use nn.Parameter only\n",
        "        # Layer 1: Transforms input to hidden dimension\n",
        "        self.hidden1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Layer 2: Attention mechanism\n",
        "        self.attention_module = nn.ModuleDict({\n",
        "            \"bahdanau\": BahdanauAttention(hidden_dim, query_dim),\n",
        "            \"multiplicative\": MultiplicativeAttention(hidden_dim, query_dim),\n",
        "            \"scaled_dot\": ScaledDotProductAttention(hidden_dim)\n",
        "        })\n",
        "        # Layer 3: Second dense layer (after pooling)\n",
        "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # Layer 4: Output layer\n",
        "        self.prediction_head = nn.Linear(hidden_dim, output_dim)\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                return_attention: bool = False):\n",
        "        \"\"\"\n",
        "        Forward pass with optional attention masking.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
        "            attention_mask: Optional mask of shape (batch_size, seq_len, input_dim)\n",
        "                          where 1 indicates positions to IGNORE (padding)\n",
        "            return_attention: If True, also return attention weights\n",
        "\n",
        "        Returns:\n",
        "            output: Predictions of shape (batch_size, output_dim)\n",
        "            attention_weights (optional): Attention distribution if return_attention=True\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_dim = x.shape\n",
        "        ##########################################\n",
        "        ## TODO: Implement forward pass, 0.8 points\n",
        "        # 1. First hidden layer (position-wise feedforward)\n",
        "        ## Hint 1: We first need to map inputs to the embedding space,\n",
        "        ## this operation should be applied to every item in the input\n",
        "        ## Hint 2: x is a 3D tensor of shape (batch_size, seq_len, input_dim)\n",
        "        ## You could reshape the tensor to apply the linear layer to **every** input item\n",
        "        ## Hint 3: Don't forget to reshape it back before attention\n",
        "        h = self.hidden1(x)\n",
        "        h = F.relu(h)\n",
        "        # 2. Attention pooling\n",
        "        # Apply attention (x serves as both keys and values here)\n",
        "        attention_module = self.attention_module[self.attention_type]\n",
        "        context_vector, attention_weights = attention_module(\n",
        "            x=h,\n",
        "            attention_mask=attention_mask,\n",
        "            values=h\n",
        "        )\n",
        "        # 3. Second hidden layer\n",
        "        h_pooled = self.hidden2(context_vector)\n",
        "        h_pooled = F.relu(h_pooled)\n",
        "        # 4. Output layer\n",
        "        output = self.prediction_head(h_pooled)\n",
        "        ##########################################\n",
        "        if return_attention:\n",
        "            return output, attention_weights\n",
        "        return output\n",
        "\n",
        "    def loss_fn(self, y_hat: torch.Tensor, y: torch.Tensor):\n",
        "        ##########################################\n",
        "        ## TODO: Return the loss value, 0.1 points\n",
        "        ## Hint: This is a regression problem. Your model should predict a continuous scalar value.\n",
        "        loss = F.mse_loss(y_hat.squeeze(-1), y)\n",
        "        ##########################################\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "daefc0d6",
      "metadata": {
        "id": "daefc0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "840d56be-450f-4c75-9b35-f2914114da2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (1) for operand 1 and no ellipsis was given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-226740582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Test 2: Scaled Dot-Product Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mscaled_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"res\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4022633064.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask, values)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m## TODO: Implement Scaled Dot-Product attention, 0.65 points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# 1. Compute dot product and scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bqd,bkd->bqk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 2. Apply mask if provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (1) for operand 1 and no ellipsis was given"
          ]
        }
      ],
      "source": [
        "###################### SANITY CHECK ######################\n",
        "batch_size, seq_len, input_dim = 2, 50, 2\n",
        "hidden_dim, query_dim, output_dim = 32, 16, 1\n",
        "\n",
        "def check_shape(a, shape, name=\"tensor\"):\n",
        "    \"\"\"Check the shape of a tensor.\"\"\"\n",
        "    assert a.shape == shape, \\\n",
        "            f'{name}\\'s shape {a.shape} != expected shape {shape}'\n",
        "\n",
        "def check_sum(a, expected, dim, name=\"tensor\", tol=1e-5):\n",
        "    \"\"\"Check if tensor sums to expected value along dimension.\"\"\"\n",
        "    actual = a.sum(dim=dim)\n",
        "    assert torch.allclose(actual, torch.full_like(actual, expected), atol=tol), \\\n",
        "            f'{name} sum along dim {dim} != {expected}'\n",
        "\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    # Test 1: Bahdanau Attention\n",
        "    bahdanau_attn = BahdanauAttention(hidden_dim, query_dim)\n",
        "    X = torch.randn(batch_size, seq_len, hidden_dim)\n",
        "    res, attn_weights = bahdanau_attn(X)\n",
        "\n",
        "    check_shape(res, (batch_size, hidden_dim), \"res\")\n",
        "    check_shape(attn_weights, (batch_size, seq_len), \"attention weights\")\n",
        "    check_sum(attn_weights, 1.0, dim=1, name=\"attention weights\")\n",
        "\n",
        "    # Test 2: Scaled Dot-Product Attention\n",
        "    scaled_attn = ScaledDotProductAttention(hidden_dim)\n",
        "    res, attn_weights = scaled_attn(X)\n",
        "\n",
        "    check_shape(res, (batch_size, hidden_dim), \"res\")\n",
        "    check_shape(attn_weights, (batch_size, seq_len), \"attention weights\")\n",
        "    check_sum(attn_weights, 1.0, dim=1, name=\"attention weights\")\n",
        "\n",
        "    # Test 3: Multiplicative Attention\n",
        "    mult_attn = MultiplicativeAttention(hidden_dim, query_dim)\n",
        "    res, attn_weights = mult_attn(X)\n",
        "\n",
        "    check_shape(res, (batch_size, hidden_dim), \"res\")\n",
        "    check_shape(attn_weights, (batch_size, seq_len), \"attention weights\")\n",
        "    check_sum(attn_weights, 1.0, dim=1, name=\"attention weights\")\n",
        "    ###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae983d1",
      "metadata": {
        "id": "cae983d1"
      },
      "source": [
        "## Trainer Class\n",
        "- Reuse your `Trainer` class implementation from the RNN homework with a small change (Hint in the forward pass)\n",
        "- Implement `predict` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c06a2e",
      "metadata": {
        "id": "80c06a2e"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, max_epochs, batch_size, gradient_clip_val=1, device=\"cpu\", print_every: int=5):\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.gradient_clip_val = gradient_clip_val\n",
        "        self.device = device\n",
        "        self.train_loss = []  # record the avg. batch loss every epoch\n",
        "        self.valid_loss = []  # record the avg. batch loss every epoch\n",
        "        self.print_every = print_every\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_gradients(model, max_norm):\n",
        "        if max_norm:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "    def get_dataloader(self, data):\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(SEED)\n",
        "        train_size = int(0.8 * len(data))\n",
        "        train_data, val_data = random_split(data, [train_size, len(data) - train_size])\n",
        "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn, generator=g)\n",
        "        valid_loader = DataLoader(val_data, batch_size=self.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    def fit(self, model, data, optimizer=None):\n",
        "        model.to(self.device)\n",
        "        if optimizer is None:\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=model.lr)\n",
        "        train_loader, valid_loader = self.get_dataloader(data)\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            valid_loss = 0\n",
        "            ########################### YOUR CODE ###################################\n",
        "            # TODO: Train the model for max_epochs many steps, 0.1 points\n",
        "            # Complete a single forward and backward pass on a given training batch\n",
        "            # Record the training loss\n",
        "            total_train_loss = 0.0\n",
        "            for batch in train_loader:\n",
        "                # Move data to device\n",
        "                inputs = batch[\"inputs\"].to(self.device)\n",
        "                targets = batch[\"targets\"].to(self.device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                predictions = model(inputs, attention_mask=attention_mask)\n",
        "                loss = model.loss_fn(predictions, targets)\n",
        "\n",
        "                # Accumulate loss weighted by batch size\n",
        "                total_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                loss.backward()\n",
        "                # Apply gradient clipping\n",
        "                self.clip_gradients(model, self.gradient_clip_val)\n",
        "                optimizer.step()\n",
        "            ########################################################################\n",
        "            self.train_loss.append(train_loss / len(train_loader))\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                ########################### YOUR CODE ###################################\n",
        "                # TODO: at the end of each epoch, evaluate the model on the validation set.\n",
        "                # Complete a single forward pass on a given validation batch\n",
        "                # Record the validation loss\n",
        "                total_valid_loss = 0.0\n",
        "                for batch in valid_loader:\n",
        "                    inputs = batch[\"inputs\"].to(self.device)\n",
        "                    targets = batch[\"targets\"].to(self.device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "\n",
        "                    predictions = model(inputs, attention_mask=attention_mask)\n",
        "                    loss = model.loss_fn(predictions, targets)\n",
        "\n",
        "                    # Accumulate loss weighted by batch size\n",
        "                    total_valid_loss += loss.item() * inputs.size(0)\n",
        "                ########################################################################\n",
        "            self.valid_loss.append(valid_loss / len(valid_loader))\n",
        "            if (epoch + 1) %self.print_every == 0:\n",
        "                print(\n",
        "                f\"Epoch {epoch + 1} train loss: {self.train_loss[-1]}, validation loss {self.valid_loss[-1]}\"\n",
        "            )\n",
        "\n",
        "    def predict(self, model, dataloader):\n",
        "        \"\"\"\n",
        "        Generate predictions on a dataset.\n",
        "        Returns:\n",
        "            predictions, targets, avg_loss\n",
        "        \"\"\"\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ########################### YOUR CODE ###################################\n",
        "            # TODO: Generate predictions for all batches in the dataloader, 0.4 points\n",
        "            # 1. Extract inputs, targets, and attention_mask from batch\n",
        "            # 2. Move tensors to device\n",
        "            # 3. Get model predictions\n",
        "            # 4. Store predictions and targets\n",
        "            # 5. Compute loss\n",
        "            for batch in dataloader:\n",
        "                # 1. & 2. Extract and move to device\n",
        "                inputs = batch[\"inputs\"].to(self.device)\n",
        "                targets = batch[\"targets\"].to(self.device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "\n",
        "                # 3. Get model predictions\n",
        "                predictions = model(inputs, attention_mask=attention_mask)\n",
        "\n",
        "                # 5. Compute loss\n",
        "                loss = model.loss_fn(predictions, targets)\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # 4. Store predictions and targets\n",
        "                all_predictions.append(predictions.cpu())\n",
        "                all_targets.append(targets.cpu())\n",
        "            ########################################################################\n",
        "        # Concatenate all predictions and targets\n",
        "        predictions = torch.cat(all_predictions, dim=0)\n",
        "        targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "        avg_loss = total_loss / (num_batches + 1e-9)\n",
        "        return predictions, targets, avg_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca49f468",
      "metadata": {
        "id": "ca49f468"
      },
      "source": [
        "Train the model for 15 epochs to achieve val_loss less than\n",
        "- Bahdanu: <= 0.00015\n",
        "- Scaled dot product: <= 0.0017\n",
        "- Multiplicative: <=0.0003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a9be82",
      "metadata": {
        "id": "22a9be82"
      },
      "outputs": [],
      "source": [
        "default_args = {\n",
        "    \"sequence_length\": 50,\n",
        "    \"num_samples\": NUM_SAMPLES,\n",
        "    \"k\": 2,\n",
        "    \"problem_type\": \"add\",\n",
        "    \"print_every\": 5,\n",
        "}\n",
        "args = {\n",
        "    \"batch_size\": 2,\n",
        "    \"num_epochs\": 15,\n",
        "    \"lr\": 10,\n",
        "    \"hidden_dim\": 1,\n",
        "    \"query_dim\": 1,\n",
        "    \"input_dim\": 2,\n",
        "    \"output_dim\": 1,\n",
        "    \"gradient_clip_val\": None,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279ef44a",
      "metadata": {
        "id": "279ef44a"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    set_seed()\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configuration\n",
        "    attention_type = \"bahdanau\"\n",
        "    problem_type = \"add\"\n",
        "\n",
        "    # Create dataset\n",
        "    data = SequenceData(\n",
        "        num_samples=default_args[\"num_samples\"],\n",
        "        max_seq_len=default_args[\"sequence_length\"],\n",
        "        k=default_args[\"k\"],\n",
        "        problem_type=problem_type,\n",
        "        variable_len=True\n",
        "    )\n",
        "    for attention_type in [\"bahdanau\", \"scaled_dot\", \"multiplicative\"]:\n",
        "        print(\"=\"*30 + attention_type + \"=\"*30)\n",
        "        # Create model with specified attention type\n",
        "        set_seed()\n",
        "        model = AttentionNetwork(\n",
        "            input_dim=args[\"input_dim\"],\n",
        "            hidden_dim=args[\"hidden_dim\"],\n",
        "            query_dim=args[\"query_dim\"],\n",
        "            output_dim=args[\"output_dim\"],\n",
        "            attention_type=attention_type\n",
        "        )\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = Trainer(\n",
        "            batch_size=args[\"batch_size\"],\n",
        "            max_epochs=args[\"num_epochs\"],\n",
        "            gradient_clip_val=args[\"gradient_clip_val\"],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        trainer.fit(model, data, optimizer)\n",
        "\n",
        "        tr, val = trainer.get_dataloader(data)\n",
        "        preds, targets, avg_loss = trainer.predict(model, val)\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "        fig.suptitle(attention_type)\n",
        "        axs[0].scatter(preds, targets)\n",
        "        axs[0].set_xlabel(\"Predictions\")\n",
        "        axs[0].set_ylabel(\"Targets\")\n",
        "        axs[1].plot(trainer.train_loss)\n",
        "        axs[1].set_ylabel(\"Loss\")\n",
        "        axs[1].set_xlabel(\"Epoch\")\n",
        "        axs[1].set_title(\"Training Loss\")\n",
        "        fig.tight_layout()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e377e3",
      "metadata": {
        "id": "d2e377e3"
      },
      "source": [
        "## Multi-head Self Attention\n",
        "\n",
        "Now we will extend this problem to test whether a multi-head attention mechanism can learn to perform **different operations** on the **same sequence** but with **different marked positions**.\n",
        "\n",
        "In the multi-dimensional problem, the model is given a sequence of 4D vectors:\n",
        "\n",
        "|            | Pos 0 | Pos 1 | Pos 2 | Pos 3 | Pos 4 | ... | Pos 49 |\n",
        "|------------|-------|-------|-------|-------|-------|-----|--------|\n",
        "| **Values**     | 0.5   | -0.7  | 0.3   | 0.1   | -0.2  | ... | 0.2    |\n",
        "| **Add markers**    | 0     | 0     | 1     | 0     | 0     | ... | 1      |\n",
        "| **Mult markers**   | 1     | 0     | 0     | 0     | 1     | ... | 0      |\n",
        "| **Avg markers**    | 0     | 1     | 0     | 1     | 0     | ... | 0      |\n",
        "\n",
        "Where:\n",
        "- **Dimension 0 (Values)**: Random numbers in [-1, 1] - **shared by all operations**\n",
        "- **Dimension 1 (Add markers)**: Marks 2 positions for the addition operation\n",
        "- **Dimension 2 (Mult markers)**: Marks 2 different positions for the multiplication operation  \n",
        "- **Dimension 3 (Avg markers)**: Marks 2 different positions for the averaging operation\n",
        "\n",
        "The model must compute three separate targets:\n",
        "1. **Addition**: Sum of values at positions marked in dimension 1\n",
        "   - Example: Values at positions 2 and 49 → `0.3 + 0.2 = 0.5`\n",
        "2. **Multiplication**: Product of values at positions marked in dimension 2\n",
        "   - Example: Values at positions 0 and 4 → `0.5 × (-0.2) = -0.1`\n",
        "3. **Averaging**: Mean of values at positions marked in dimension 3\n",
        "   - Example: Values at positions 1 and 3 → `(-0.7 + 0.1) / 2 = -0.3`\n",
        "\n",
        "**Final target**: Average of the three operation results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "f522cd51",
      "metadata": {
        "id": "f522cd51"
      },
      "outputs": [],
      "source": [
        "class MultiHeadScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Multi-Head Scaled Dot-Product Attention.\n",
        "\n",
        "    Each head has its own learnable query parameter.\n",
        "    All heads share the same keys and values (the input).\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int,  num_heads: int = 3):\n",
        "        \"\"\"\n",
        "        num_heads: Number of attention heads (number of different queries)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = np.sqrt(hidden_dim)\n",
        "\n",
        "        ##########################################\n",
        "        ## TODO: Initialize query parameters - one per head!, 0.2 points\n",
        "        # Hint: Shape: (num_heads, hidden_dim, 1)\n",
        "        self.queries = nn.Parameter(torch.empty(num_heads, hidden_dim))\n",
        "        nn.init.uniform_(self.queries, a=-0.1, b=0.1)\n",
        "        # Aggregate all heads back to hidden_dim:\n",
        "        # should be a parameter\n",
        "        self.head_aggregator = nn.Linear(num_heads * hidden_dim, hidden_dim)\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
        "                values: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (keys) of shape (batch_size, seq_len, hidden_dim)\n",
        "            attention_mask: Optional mask of shape (batch_size, seq_len)\n",
        "                          where True indicates positions to IGNORE\n",
        "            values: Optional separate values tensor of shape (batch_size, seq_len, value_dim)\n",
        "                   If None, use x as values (self-attention)\n",
        "        \"\"\"\n",
        "        bs, seq_len, hidden_dim = x.shape\n",
        "        # Use x as values if not provided (self-attention)\n",
        "        if values is None:\n",
        "            values = x\n",
        "        ##########################################\n",
        "        res = None\n",
        "        attention_weights = torch.zeros((bs, self.num_heads, seq_len))\n",
        "        ## TODO: Implement multi-head attention, 0.8 points\n",
        "        # Compute energies for all heads at once\n",
        "        # Hint: x: (bs, seq_len, hidden_dim)\n",
        "        # queries: (num_heads, hidden_dim)\n",
        "        # Hint: You might find torch.einsum useful\n",
        "        # https://docs.pytorch.org/docs/stable/generated/torch.einsum.html\n",
        "        energies = torch.einsum('btd,hd->bht', x, self.queries) / self.scale\n",
        "        # Apply mask if provided\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.dim() > 2:\n",
        "                attention_mask = (attention_mask != 0).all(dim=-1).long() # -> (B, T)\n",
        "\n",
        "            mask_expanded = attention_mask.unsqueeze(1)\n",
        "            energies = energies.masked_fill(mask_expanded == 0, -1e9)\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(energies, dim=-1)\n",
        "        # Apply attention weights to values for all heads\n",
        "        res_heads = torch.einsum('bht,btd->bhd', attention_weights, values)\n",
        "        # Aggregate from multi-heads back to hidden_dim\n",
        "        res_combined = res_heads.reshape(bs, self.num_heads * hidden_dim)\n",
        "        res = self.head_aggregator(res_combined)\n",
        "        ##########################################\n",
        "        assert attention_weights.shape == (bs, self.num_heads, seq_len)\n",
        "        return res, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "0f00ab2d",
      "metadata": {
        "id": "0f00ab2d"
      },
      "outputs": [],
      "source": [
        "## don't change the following code\n",
        "class MultiHeadAttentionNetwork(AttentionNetwork):\n",
        "    \"\"\"\n",
        "    Complete network with attention pooling.\n",
        "    Architecture: Dense -> Attention -> Dense -> Output\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        query_dim: int,\n",
        "        output_dim: int,\n",
        "        num_heads: int,\n",
        "        attention_type: Literal[\"scaled_dot\"] = \"scaled_dot\"\n",
        "    ):\n",
        "        super().__init__(input_dim, hidden_dim, query_dim, output_dim, attention_type)\n",
        "        # Over-write the attention module\n",
        "        self.attention_module = MultiHeadScaledDotProductAttention(hidden_dim, num_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "236719a4",
      "metadata": {
        "id": "236719a4"
      },
      "outputs": [],
      "source": [
        "## don't change the following code\n",
        "\n",
        "PADDING_ID = 100\n",
        "NUM_SAMPLES = 1000\n",
        "MAX_SEQ_LEN = 50\n",
        "\n",
        "class MultiDimSequenceData(SequenceData):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_samples: int = NUM_SAMPLES,\n",
        "        max_seq_len: int = MAX_SEQ_LEN,\n",
        "        padding_id: int = PADDING_ID,\n",
        "        k: int = 2,\n",
        "        variable_len: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k: Number of positions to mark\n",
        "            variable_len: If True, generate sequences of random length between k and max_seq_len\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.padding_id = padding_id\n",
        "        self.k = k\n",
        "        self.max_seq_len = max_seq_len\n",
        "        data = []\n",
        "        for _ in range(num_samples):\n",
        "            sequence_length=random.randint(k, MAX_SEQ_LEN)\n",
        "            sequence = np.zeros((sequence_length, 4))\n",
        "            # input values\n",
        "            sequence[:, 0] = np.random.uniform(-1, 1, sequence_length)\n",
        "            targets = []\n",
        "            for ind, problem_type in enumerate([\"add\", \"multiply\", \"average\"], 1):\n",
        "                random_indices = np.random.choice(sequence_length, size=k, replace=False)\n",
        "                sequence[random_indices, ind] = 1\n",
        "                marked_values = sequence[random_indices, 0]\n",
        "                if problem_type == \"add\":\n",
        "                    target = marked_values.sum()\n",
        "                elif problem_type == \"multiply\":\n",
        "                    target = marked_values.prod()\n",
        "                elif problem_type == \"average\":\n",
        "                    target = marked_values.mean()\n",
        "                targets.append(target)\n",
        "            x = torch.tensor(sequence, dtype=torch.float32)\n",
        "            y = torch.tensor([np.mean(targets)], dtype=torch.float32)\n",
        "            data.append((x, y))\n",
        "        self.X = [d[0] for d in data]\n",
        "        self.y = [d[1] for d in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "c42c2f2c",
      "metadata": {
        "id": "c42c2f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "c1af3fd7-4044-4221-dd34-535f0bc42cbd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'MultiHeadScaledDotProductAttention' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3837733604.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3764093387.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, data, optimizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2460241471.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask, return_attention)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# 2. Attention pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Apply attention (x serves as both keys and values here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mattention_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         context_vector, attention_weights = attention_module(\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'MultiHeadScaledDotProductAttention' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "multi_default_args = {\n",
        "    \"sequence_length\": 50,\n",
        "    \"num_samples\": 1000,\n",
        "    \"k\": 16,\n",
        "    \"print_every\": 10\n",
        "}\n",
        "multi_args = {\n",
        "    \"batch_size\": 2,\n",
        "    \"num_epochs\": 15,\n",
        "    \"lr\": 10,\n",
        "    \"hidden_dim\": 1,\n",
        "    \"query_dim\": 1,\n",
        "    \"input_dim\": 4,\n",
        "    \"output_dim\": 1,\n",
        "    \"gradient_clip_val\": None,\n",
        "    \"num_heads\": 3\n",
        "}\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed()\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configuration\n",
        "    attention_type = \"scaled_dot\"\n",
        "\n",
        "    # Create dataset\n",
        "    data = MultiDimSequenceData(\n",
        "        num_samples=multi_default_args[\"num_samples\"],\n",
        "        max_seq_len=multi_default_args[\"sequence_length\"],\n",
        "        k=multi_default_args[\"k\"],\n",
        "        variable_len=True\n",
        "    )\n",
        "\n",
        "    # Create model with specified attention type\n",
        "    model = MultiHeadAttentionNetwork(\n",
        "        input_dim=multi_args[\"input_dim\"],\n",
        "        hidden_dim=multi_args[\"hidden_dim\"],\n",
        "        query_dim=multi_args[\"query_dim\"],\n",
        "        output_dim=multi_args[\"output_dim\"],\n",
        "        num_heads=multi_args[\"num_heads\"],\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=multi_args[\"lr\"])\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        batch_size=multi_args[\"batch_size\"],\n",
        "        max_epochs=multi_args[\"num_epochs\"],\n",
        "        gradient_clip_val=multi_args[\"gradient_clip_val\"],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, data, optimizer)\n",
        "\n",
        "    tr, val = trainer.get_dataloader(data)\n",
        "    preds, targets, avg_loss = trainer.predict(model, val)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    fig.suptitle(attention_type)\n",
        "    axs[0].scatter(preds, targets)\n",
        "    axs[0].set_xlabel(\"Predictions\")\n",
        "    axs[0].set_ylabel(\"Targets\")\n",
        "    axs[1].plot(trainer.train_loss)\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_title(\"Training Loss\")\n",
        "    fig.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013c7c6e",
      "metadata": {
        "id": "013c7c6e"
      },
      "source": [
        "# Collaboration / External Help\n",
        "Disclose any help you used (LLM usage, blogs, search, Github links, etc) and collaborations with your classmates. If you  completed the homework on your own, you can leave this part empty.\n",
        "\n",
        "> TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61e8d11",
      "metadata": {
        "id": "e61e8d11"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prep-ta-fall-2025 (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}