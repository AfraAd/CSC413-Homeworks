{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfraAd/CSC413-Homeworks/blob/main/Fall25_Hw11_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0a6511",
      "metadata": {
        "id": "6d0a6511"
      },
      "source": [
        "# Homework 11 - Variational AutoEncoders\n",
        "CSC413/2516: Neural Networks and Deep Learning\n",
        "\n",
        "As with previous homeworks, replace \"#### Your Code ####\" lines with your implementation.\n",
        "\n",
        "In this homework you will implement and train a Beta-Variational Autoencoder ($\\beta$-VAE) on a synthetic dataset we created\n",
        "1. Implement and train a SimpleVAE (that has Linear layers only)\n",
        "2. Implement Convolutional $\\beta$-VAE (ConvVAE).\n",
        "\n",
        "\n",
        "Similar to Hw10, you have more freedom in this homework. We will not run your training loop, or test your `Trainer` class, they are there as an initial guidance. We will instead evaluate your predictions, run your model on a test set to make sure. Make use of a gpu on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31186515",
      "metadata": {
        "id": "31186515"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856e0bcf",
      "metadata": {
        "id": "856e0bcf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Literal, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def set_seed():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d086f8dd",
      "metadata": {
        "id": "d086f8dd"
      },
      "outputs": [],
      "source": [
        "################## HELPER CODE FOR SAVING RELEVANT FILES ##################\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    def in_colab():\n",
        "        try:\n",
        "            import google.colab\n",
        "\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "\n",
        "    if in_colab():\n",
        "        from google.colab import drive\n",
        "\n",
        "        drive.mount(\"/content/drive\")\n",
        "        SAVE_PATH = \"/content/drive/MyDrive\"\n",
        "    else:\n",
        "        SAVE_PATH = \".\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c567e9",
      "metadata": {
        "id": "74c567e9"
      },
      "source": [
        "## Explore the Dataset\n",
        "don't change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5506bc",
      "metadata": {
        "id": "3b5506bc"
      },
      "outputs": [],
      "source": [
        "class ParametricShapesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, dataset_path, split: Literal[\"train\", \"test\", \"validation\"] = \"train\"\n",
        "    ):\n",
        "        self.dataset = load_dataset(dataset_path)[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        return self.dataset[0][\"image\"].size[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        # 2. Permute to PyTorch standard (Channels, Height, Width)\n",
        "        image_np = np.array(item[\"image\"])\n",
        "        image = torch.tensor(image_np).permute(2, 0, 1).float() / 255.0\n",
        "        return {\n",
        "            \"inputs\": image,\n",
        "            \"targets\": item[\"shape_type\"],\n",
        "            \"shape_name\": item[\"shape_name\"],\n",
        "            \"pos_x\": item[\"pos_x\"],\n",
        "            \"pos_y\": item[\"pos_y\"],\n",
        "            \"size\": item[\"size\"],\n",
        "            \"rotation\": item[\"rotation\"],\n",
        "            \"id\": item[\"id\"],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba3be4c",
      "metadata": {
        "id": "8ba3be4c"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(\n",
        "    dataloader, num_samples: int = 16, figsize: Tuple[int, int] = (12, 12)\n",
        "):\n",
        "    \"\"\"Visualize random samples from the dataset.\"\"\"\n",
        "    rows = int(np.sqrt(num_samples))\n",
        "    cols = (num_samples + rows - 1) // rows\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
        "\n",
        "    # Collect enough samples from the dataloader\n",
        "    collected_samples = []\n",
        "    data_iter = iter(dataloader)\n",
        "\n",
        "    while len(collected_samples) < num_samples:\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "            batch_size = batch[\"inputs\"].shape[0]\n",
        "\n",
        "            # Unpack the batch into individual items\n",
        "            for i in range(batch_size):\n",
        "                if len(collected_samples) >= num_samples:\n",
        "                    break\n",
        "\n",
        "                sample = {\n",
        "                    \"img\": batch[\"inputs\"][i],\n",
        "                    \"shape_name\": batch[\"shape_name\"][i],\n",
        "                    \"pos_x\": batch[\"pos_x\"][i],\n",
        "                    \"pos_y\": batch[\"pos_y\"][i],\n",
        "                    \"size\": batch[\"size\"][i],\n",
        "                    \"rotation\": batch[\"rotation\"][i],\n",
        "                }\n",
        "                collected_samples.append(sample)\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "    # Plotting Loop\n",
        "    for i, sample in enumerate(collected_samples):\n",
        "        img = sample[\"img\"]\n",
        "\n",
        "        # Convert tensor (C, H, W) to (H, W, C)\n",
        "        img_display = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "        s_name = sample[\"shape_name\"]\n",
        "\n",
        "        title = f\"{s_name}\\n\"\n",
        "        title += f\"Pos:({sample['pos_x']:.2f}, {sample['pos_y']:.2f})\\n\"\n",
        "        title += f\"Size:{sample['size']:.2f}, Rot:{sample['rotation']:.2f}\"\n",
        "        axes[i].set_title(title, fontsize=8)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(len(collected_samples), len(axes)):\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dl = DataLoader(\n",
        "        ParametricShapesDataset(\"r-three/parametric-shapes\", split=\"train\"),\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    visualize_samples(dl, num_samples=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69af7ae6",
      "metadata": {
        "id": "69af7ae6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def visualize_reconstructions(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    num_images: int = 10,\n",
        "    figsize: Tuple[int, int] = (15, 4),\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes a batch of data, runs it through the VAE, and visualizes the\n",
        "    original inputs next to their reconstructions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Get a single batch of data\n",
        "    data = next(iter(dataloader))[\"inputs\"]\n",
        "    # Ensure we don't try to plot more images than are in the batch\n",
        "    num_to_plot = min(num_images, data.size(0))\n",
        "    data = data.to(device)[:num_to_plot]\n",
        "\n",
        "    # 2. Forward Pass: Get Reconstruction\n",
        "    recon, _, _ = model(data)\n",
        "    #  (N, H, W, C) for plotting\n",
        "    original_images = data.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    reconstructed_images = recon.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "    # 3. Setup Plot Grid (2 rows: Original\\\\ Reconstruction)\n",
        "    fig, axes = plt.subplots(2, num_to_plot, figsize=figsize)\n",
        "    fig.suptitle(\n",
        "        f\"Originals vs. Reconstructions (VAE Latent Dim: {model.fc_mu.out_features})\",\n",
        "        fontsize=14,\n",
        "    )\n",
        "\n",
        "    for i in range(num_to_plot):\n",
        "        # Plot Originals\n",
        "        ax_orig = axes[0, i]\n",
        "        ax_orig.imshow(original_images[i], cmap=\"gray\")\n",
        "        ax_orig.set_title(f\"Original {i + 1}\", fontsize=9)\n",
        "        ax_orig.axis(\"off\")\n",
        "\n",
        "        # Plot Reconstructions\n",
        "        ax_recon = axes[1, i]\n",
        "        ax_recon.imshow(reconstructed_images[i], cmap=\"gray\")\n",
        "        ax_recon.set_title(f\"Recon {i + 1}\", fontsize=9)\n",
        "        ax_recon.axis(\"off\")\n",
        "\n",
        "    # plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5032db1",
      "metadata": {
        "id": "e5032db1"
      },
      "source": [
        "# Implement the VAE (2 points)\n",
        "Here implement the reparameterization trick and the VAE loss function. If you wish to play around with the loss function for training, please do so in the subclasses, we will test the `BaseVAE.loss_fn`.\n",
        "\n",
        "- Reconstruction loss: $$\\frac 1 N \\sum_i^N \\lVert X_i - \\hat X_i\\rVert_F^2$$\n",
        "- Unscaled KL divergence penalty: $$KL(q || p)$$, again average per instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0420c3",
      "metadata": {
        "id": "ec0420c3"
      },
      "outputs": [],
      "source": [
        "class BaseVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseVAE, self).__init__()\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Reparameterization trick to sample from N(mu, var) from\n",
        "        N(0,1).\"\"\"\n",
        "        z = torch.zeros_like(mu)\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Implement the reparameterization trick\n",
        "\n",
        "        #########################################################################\n",
        "        return z\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_fn(\n",
        "        x: torch.Tensor,\n",
        "        x_recon: torch.Tensor,\n",
        "        mu: torch.Tensor,\n",
        "        logvar: torch.Tensor,\n",
        "        beta: float = 1.0,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Computes the VAE loss function.\n",
        "        The Evidence Lower Bound (ELBO) Loss for VAE.\n",
        "        Loss = Reconstruction Loss + KL Divergence Loss\n",
        "        Should return total_loss, recon_loss, kl_loss\"\"\"\n",
        "        total_loss = torch.tensor(0.0)\n",
        "        recon_loss = torch.tensor(0.0)\n",
        "        kl_loss = torch.tensor(0.0) # this is prior to scaling\n",
        "        ########################### YOUR CODE ###################################\n",
        "        # TODO: 1. Compute reconstruction Loss (MSE), should be averaged over batch dimension (no pixel-level averaging)\n",
        "        # TODO: 2. KL Divergence Loss (Unscaled), Hint: make sure your KL term includes the negative here\n",
        "        # HINT: Use the analytical formula, averaged over batch dimension\n",
        "\n",
        "        #########################################################################\n",
        "        return total_loss, recon_loss, kl_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd47295",
      "metadata": {
        "id": "bbd47295"
      },
      "outputs": [],
      "source": [
        "class SimpleVAE(BaseVAE):\n",
        "    def __init__(self, input_dim: int, hidden_dim, latent_dim):\n",
        "        super(SimpleVAE, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.input_dim = input_dim\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ### TODO: Encoder\n",
        "        ## 1. Define the encoder layers (these should all be Linear layers or LeakyReLU act)\n",
        "        self.encoder = None\n",
        "        self.fc_mu = None\n",
        "        self.fc_logvar = None\n",
        "        self.act1 = None\n",
        "        ### Decoder\n",
        "        self.decoder_1 = None\n",
        "        self.decoder_2 = None\n",
        "        self.act2 = None\n",
        "        #########################################################################\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes (mean and log variance of q(z|X)).\"\"\"\n",
        "        mu = torch.zeros(self.latent_dim)\n",
        "        logvar = torch.zeros(self.latent_dim)\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Implement the encode function\n",
        "        #########################################################################\n",
        "        return mu, logvar\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Maps the given latent codes onto the image space (still flattened).\"\"\"\n",
        "        x_recon = torch.zeros(z.size(0), self.input_dim)\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Implement the decode function\n",
        "        #########################################################################\n",
        "        return x_recon\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through the network. Returns the reconstructed\n",
        "        image along with the latent codes.\"\"\"\n",
        "        # No need to change this code\n",
        "        orig_shape = x.shape\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x_recon = x.clone()\n",
        "        mu = torch.zeros(orig_shape[0], self.latent_dim)\n",
        "        logvar = torch.zeros(orig_shape[0], self.latent_dim)\n",
        "        ## 1. Encode to hidden state (Hint: use self.encode)\n",
        "        ## 2. Reparameterize to get latent code (Hint: use self.reparameterize)\n",
        "        ## 3. Decode to get reconstructed image (Hint: use self.decode)\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        x_recon = x_recon.view(orig_shape)\n",
        "        return x_recon, mu, logvar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece9302e",
      "metadata": {
        "id": "ece9302e"
      },
      "source": [
        "## Implement the Trainer Class (Ungraded)\n",
        "Implement the trainer.fit function, this should be quite similar to Hw 6, 7, and 9 trainer with a small tweak for loss calculation. You are free to change inner functionality, loss computation, etc. as we won't call the Trainer class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857d1fc3",
      "metadata": {
        "id": "857d1fc3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_epochs,\n",
        "        batch_size,\n",
        "        gradient_clip_val=1,\n",
        "        device=\"cpu\",\n",
        "        print_every: int = 1,\n",
        "    ):\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.gradient_clip_val = gradient_clip_val\n",
        "        self.device = device\n",
        "        self.train_metrics = {\"total_loss\": [], \"recon_loss\": [], \"kl_loss\": []}\n",
        "        self.valid_metrics = {\"total_loss\": [], \"recon_loss\": [], \"kl_loss\": []}\n",
        "        self.print_every = print_every\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_gradients(model, max_norm):\n",
        "        if not max_norm:\n",
        "            return\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "    def get_dataloader(self, data, batch_size=None, shuffle=True):\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(SEED)\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "        loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, generator=g)\n",
        "        return loader\n",
        "\n",
        "    def fit(self, model, train_data, val_data, optimizer=None, beta=1.0):\n",
        "        print(\"Starting training...\")\n",
        "        print(\n",
        "            f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f} million trainable parameters.\"\n",
        "        )\n",
        "        model.to(self.device)\n",
        "        if optimizer is None:\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=model.lr)\n",
        "        train_loader, valid_loader = (\n",
        "            self.get_dataloader(train_data),\n",
        "            self.get_dataloader(val_data),\n",
        "        )\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            train_mse_loss, train_kl_loss = 0, 0\n",
        "            valid_loss = 0\n",
        "            val_mse_loss, val_kl_loss = 0, 0\n",
        "            ########################### YOUR CODE ###################################\n",
        "            # TODO: Train the model for max_epochs\n",
        "            # Complete a single forward and backward pass on a given training batch\n",
        "            # Record the training losses\n",
        "            for batch in train_loader:\n",
        "                pass\n",
        "            ########################################################################\n",
        "            self.train_metrics[\"total_loss\"].append(train_loss / len(train_loader))\n",
        "            self.train_metrics[\"recon_loss\"].append(train_mse_loss / len(train_loader))\n",
        "            self.train_metrics[\"kl_loss\"].append(train_kl_loss / len(train_loader))\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                ########################### YOUR CODE ###################################\n",
        "                # TODO: at the end of each epoch, evaluate the model on the validation set.\n",
        "                # Complete a single forward pass on a given validation batch\n",
        "                # Record the validation loss\n",
        "                for batch in valid_loader:\n",
        "                    pass\n",
        "                ########################################################################\n",
        "            self.valid_metrics[\"total_loss\"].append(valid_loss / len(valid_loader))\n",
        "            self.valid_metrics[\"recon_loss\"].append(val_mse_loss / len(valid_loader))\n",
        "            self.valid_metrics[\"kl_loss\"].append(val_kl_loss / len(valid_loader))\n",
        "\n",
        "            if (epoch + 1) % self.print_every == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1} train loss: {self.train_metrics['total_loss'][-1]:.5f},\\t train recon loss: {self.train_metrics['recon_loss'][-1]:.5f}, train kl loss: {self.train_metrics['kl_loss'][-1]:.5f}\\n\\tvalidation loss {self.valid_metrics['total_loss'][-1]:.5f}, val recon loss: {self.valid_metrics['recon_loss'][-1]:.5f}, val kl loss: {self.valid_metrics['kl_loss'][-1]:.5f} \"\n",
        "                )\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"Plot training and validation metrics.\"\"\"\n",
        "        epochs = range(1, len(self.train_metrics[\"total_loss\"])+1)\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Total Loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs, self.train_metrics[\"total_loss\"], label=\"Train Total Loss\")\n",
        "        plt.plot(epochs, self.valid_metrics[\"total_loss\"], label=\"Valid Total Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Total Loss\")\n",
        "        plt.title(\"Total Loss over Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        # Reconstruction Loss\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs, self.train_metrics[\"recon_loss\"], label=\"Train Recon Loss\")\n",
        "        plt.plot(epochs, self.valid_metrics[\"recon_loss\"], label=\"Valid Recon Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Reconstruction Loss\")\n",
        "        plt.title(\"Reconstruction Loss over Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        # KL Divergence Loss\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs, self.train_metrics[\"kl_loss\"], label=\"Train KL Loss\")\n",
        "        plt.plot(epochs, self.valid_metrics[\"kl_loss\"], label=\"Valid KL Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"KL Divergence Loss\")\n",
        "        plt.title(\"KL Divergence Loss over Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_and_dump(self, model, dataloader, output_path: str):\n",
        "        \"\"\"\n",
        "        Generate predictions on a dataset and dump them to a file.\n",
        "        Don't change this function.\n",
        "        \"\"\"\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        records = dict()\n",
        "        for batch in dataloader:\n",
        "            # Extract batch data\n",
        "            X = batch[\"inputs\"].to(self.device)\n",
        "            idx = batch[\"id\"]\n",
        "\n",
        "            # Get predictions\n",
        "            x_reconstructed, mu, logvar = model(X)\n",
        "            for i in range(X.size(0)):\n",
        "                record = {\n",
        "                    \"id\": idx[i].item(),\n",
        "                    \"x_reconstructed\": x_reconstructed[i].cpu(),\n",
        "                    \"mu\": mu[i].cpu(),\n",
        "                    \"logvar\": logvar[i].cpu(),\n",
        "                }\n",
        "                records[idx[i].item()] = record\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            torch.save(records, f)\n",
        "        return records"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c0575d",
      "metadata": {
        "id": "50c0575d"
      },
      "source": [
        "# Train SimpleVAE (2 points)\n",
        "Your goal is to achieve <26 MSE reconstruction error on the validation set. You are free to change any args, use another optimizer, add additional tricks, etc.\n",
        "\n",
        "- You will receive 1.25 points if it is between $26<x<28$\n",
        "- You will receive 0.75 point if it is between $28<x<30$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd099da",
      "metadata": {
        "id": "2dd099da"
      },
      "outputs": [],
      "source": [
        "## TODO: Optimize args\n",
        "args = {\n",
        "    \"hidden_dim\": 1,\n",
        "    \"latent_dim\": 1,\n",
        "    \"lr\": 1,\n",
        "    \"batch_size\": 1,\n",
        "    \"num_epochs\": 1, # it is possible to get to the target loss in ~20 epochs\n",
        "    \"gradient_clip_val\": None,\n",
        "    \"beta\": 1.0,\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configuration\n",
        "    train_data = ParametricShapesDataset(\"r-three/parametric-shapes\", split=\"train\")\n",
        "    val_data = ParametricShapesDataset(\"r-three/parametric-shapes\", split=\"validation\")\n",
        "\n",
        "    set_seed()\n",
        "    model = SimpleVAE(\n",
        "        input_dim=3 * train_data.input_dim * train_data.input_dim,\n",
        "        hidden_dim=args[\"hidden_dim\"],\n",
        "        latent_dim=args[\"latent_dim\"],\n",
        "    )\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
        "\n",
        "    trainer = Trainer(\n",
        "        batch_size=args[\"batch_size\"],\n",
        "        max_epochs=args[\"num_epochs\"],\n",
        "        gradient_clip_val=args[\"gradient_clip_val\"],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_data, val_data, optimizer)\n",
        "    trainer.plot_metrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d9622f",
      "metadata": {
        "id": "f5d9622f"
      },
      "outputs": [],
      "source": [
        "# if you are happy with the trained model, save it\n",
        "if __name__ == \"__main__\":\n",
        "    torch.save(model.state_dict(), f\"{SAVE_PATH}/simple_vae.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7c7c35",
      "metadata": {
        "id": "be7c7c35"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val_loader = trainer.get_dataloader(val_data, batch_size=16, shuffle=False)\n",
        "    visualize_reconstructions(model, val_loader, device, num_images=10)\n",
        "    set_seed()\n",
        "    trainer.predict_and_dump(model, val_loader, f\"{SAVE_PATH}/simple-val_predictions.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98968a59",
      "metadata": {
        "id": "98968a59"
      },
      "source": [
        "*ungraded*\n",
        "\n",
        "- Does your VAE learn to reconstruct input images?\n",
        "- Why do we need the reparameterization trick?\n",
        "- Are there specific classes or input features that the model is struggling with?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8caf16",
      "metadata": {
        "id": "cb8caf16"
      },
      "source": [
        "## Conv-VAE (2.5 points)\n",
        "In this part of the homework you will implement a U-NET-like VAE architecture without skip connections, the center of the network is a **stochastic bottleneck** defined by the latent distribution. This architecture is composed of a convolutional encoder and a deconvolutional decoder.\n",
        "\n",
        "We don't enforce a certain architectural scaffolding, hence you are free to use any architecture (no. of conv layers, non-linearity, etc.), but we will enforce that the architecture contains and calls `nn.Conv2d` and `nn.ConvTranspose2d` layers.\n",
        "\n",
        "Your goal is to come up with an architecture and optimize hyper-parameters to reach <26 reconstruction loss on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d959cf",
      "metadata": {
        "id": "42d959cf"
      },
      "outputs": [],
      "source": [
        "class ConvVAE(BaseVAE):\n",
        "    def __init__(self, hidden_dim, latent_dim):\n",
        "        super(ConvVAE, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        ########################### YOUR CODE ###################################\n",
        "        self.output_dim = 0 # Define output dimension (width) after encoder conv layers\n",
        "        ### TODO: Encoder (you are free to do anything as long as it has at least one conv layer)\n",
        "        self.encoder = nn.Sequential(\n",
        "        )\n",
        "        output_feature_map_size = 4  # After 3 conv layers with stride 2 on 32x32 input\n",
        "\n",
        "        # Linear layers for mu and logvar\n",
        "        self.fc_mu = None\n",
        "        self.fc_logvar = None\n",
        "\n",
        "        # ------------------- DECODER p(x|z) -------------------\n",
        "        self.fc_decode = None\n",
        "        self.act_decode = None\n",
        "        ## TODO: decoder (you are free to do anything as long as it has at least one ConvTranspose2d layer)\n",
        "        self.decoder = nn.Sequential(\n",
        "        )\n",
        "        #########################################################################\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Encodes the input image to latent codes (mu and log variance).\"\"\"\n",
        "        mu = torch.zeros(x.shape[0], self.latent_dim)\n",
        "        logvar = torch.zeros(x.shape[0], self.latent_dim)\n",
        "        ## TODO\n",
        "        return mu, logvar\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Maps the latent code z back to the image space.\"\"\"\n",
        "        x_recon = torch.zeros(z.shape[0], 3, self.output_dim, self.output_dim)\n",
        "        ## TODO\n",
        "\n",
        "        return x_recon\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3aa7b4",
      "metadata": {
        "id": "0c3aa7b4"
      },
      "outputs": [],
      "source": [
        "## TODO: Optimize conv args\n",
        "args = {\n",
        "    \"hidden_dim\": 1,\n",
        "    \"latent_dim\": 1,\n",
        "    \"lr\": 1,\n",
        "    \"batch_size\": 1,\n",
        "    \"num_epochs\": 1, # it is possible to get to the target loss in ~20 epochs\n",
        "    \"gradient_clip_val\": None,\n",
        "    \"beta\": 1.0,\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Configuration\n",
        "    set_seed()\n",
        "    train_data = ParametricShapesDataset(\"r-three/parametric-shapes\", split=\"train\")\n",
        "    val_data = ParametricShapesDataset(\"r-three/parametric-shapes\", split=\"validation\")\n",
        "    set_seed()\n",
        "    model = ConvVAE(\n",
        "        hidden_dim=conv_args[\"hidden_dim\"],\n",
        "        latent_dim=conv_args[\"latent_dim\"],\n",
        "    )\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=conv_args[\"lr\"])\n",
        "\n",
        "    trainer = Trainer(\n",
        "        batch_size=conv_args[\"batch_size\"],\n",
        "        max_epochs=conv_args[\"num_epochs\"],\n",
        "        gradient_clip_val=conv_args[\"gradient_clip_val\"],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_data, val_data, optimizer, beta=conv_args[\"beta\"])\n",
        "    trainer.plot_metrics()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f4ccab",
      "metadata": {
        "id": "51f4ccab"
      },
      "outputs": [],
      "source": [
        "# if you are happy with the trained model, save it\n",
        "if __name__ == \"__main__\":\n",
        "    torch.save(model.state_dict(), f\"{SAVE_PATH}/conv_vae.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d04c091",
      "metadata": {
        "id": "8d04c091"
      },
      "outputs": [],
      "source": [
        "# don't change\n",
        "if __name__ == \"__main__\":\n",
        "    val_loader = trainer.get_dataloader(val_data, batch_size=10, shuffle=False)\n",
        "    visualize_reconstructions(model, val_loader, device, num_images=10)\n",
        "    set_seed()\n",
        "    trainer.predict_and_dump(model, val_loader, f\"{SAVE_PATH}/conv-val_predictions.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e85fa2",
      "metadata": {
        "id": "e9e85fa2"
      },
      "source": [
        "In the following part, you can look more closely into the latent space of your model. The function below interpolates between the embeddings of two data points in the latent space. Think about the implications of this. Would you have expected it in a vanilla autoencoder setting as well?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a8b138",
      "metadata": {
        "id": "50a8b138"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def visualize_interpolation(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    idx_start: int = 0,\n",
        "    idx_end: int = 1,\n",
        "    num_steps: int = 8,\n",
        "    figsize: Tuple[int, int] = (15, 2),\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualizes the interpolation between the latent codes of two input images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    num_to_plot = num_steps + 2  # Start image, num_steps interpolations, End image\n",
        "\n",
        "    # 1. Get a batch and select the two endpoints\n",
        "    data = next(iter(dataloader))[\"inputs\"]\n",
        "\n",
        "    # Select the two images for interpolation\n",
        "    img_start = data[idx_start].unsqueeze(0).to(device)\n",
        "    img_end = data[idx_end].unsqueeze(0).to(device)\n",
        "\n",
        "    # 2. Encode to get latent means (mu)\n",
        "    mu_start, _ = model.encode(img_start)\n",
        "    mu_end, _ = model.encode(img_end)\n",
        "\n",
        "    # 3. Create interpolation path in latent space\n",
        "    alphas = torch.linspace(0, 1, num_to_plot).to(device).unsqueeze(1)\n",
        "    # Linear interpolation: z_interp = (1 - alpha) * mu_start + alpha * mu_end\n",
        "    latent_interpolations = (1 - alphas) * mu_start + alphas * mu_end\n",
        "\n",
        "    # 4. Decode all interpolated latent vectors\n",
        "    decoded_images = model.decode(latent_interpolations)\n",
        "    images_to_plot = decoded_images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    # 5. Plotting\n",
        "    fig, axes = plt.subplots(1, num_to_plot, figsize=figsize)\n",
        "    fig.suptitle(\n",
        "        f\"Latent Interpolation: $\\mathbf{{\\mu}}_{{{idx_start}}}$ to $\\mathbf{{\\mu}}_{{{idx_end}}}$ (Steps={num_steps})\",\n",
        "        fontsize=14,\n",
        "    )\n",
        "\n",
        "    for i in range(num_to_plot):\n",
        "        ax = axes[i]\n",
        "\n",
        "        ax.imshow(images_to_plot[i].squeeze())\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        if i == 0:\n",
        "            ax.set_title(\"Start\", fontsize=10)\n",
        "        elif i == num_to_plot - 1:\n",
        "            ax.set_title(\"End\", fontsize=10)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3304f086",
      "metadata": {
        "id": "3304f086"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    visualize_interpolation(\n",
        "        model, val_loader, device, idx_start=0, idx_end=5, num_steps=10, figsize=(18, 3)\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prep-ta-fall-2025 (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}