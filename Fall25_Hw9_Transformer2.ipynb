{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfraAd/CSC413-Homeworks/blob/main/Fall25_Hw9_Transformer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a7fe99",
      "metadata": {
        "id": "45a7fe99"
      },
      "source": [
        "# Homework 9 - Transformers Part II\n",
        "CSC413/2516: Neural Networks and Deep Learning\n",
        "\n",
        "As with previous homeworks, replace \"#### Your Code ####\" lines with your implementation.\n",
        "\n",
        "\n",
        "Transformer architecture has been evolving since the seminal [``Attention Is All You Need''](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. 2017.\n",
        "In this homework, you will implement some of these changes, namely\n",
        "1. Position Encodings (2.2 points)\n",
        "2. Architecture Changes (2.3 points)\n",
        "3. Architecture Detective (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0659b916",
      "metadata": {
        "id": "0659b916"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4920dc76",
      "metadata": {
        "id": "4920dc76"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Dict, Literal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def set_seed():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8817fe8d",
      "metadata": {
        "id": "8817fe8d"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will work on a simple sequential dataset that shift the elements of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "686077bb",
      "metadata": {
        "id": "686077bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54abdeb5-f9e2-4e86-f9dd-66b4dc09e049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example 1:\n",
            "Source:   16 |   14 |   12 |   15 |    3 |   15 |   16 |    7 |    6 |   14 |    3 |   16 |    4 |    4 |   18 |    8 |   11 |    9 |   18 |   18 |   14 |    5 |   17 |   11 |   15 |    9 |   11 |   10 |    7 |    7 |    9 |   17\n",
            "Target:    0 |    0 |    0 |    0 |   16 |   14 |   12 |   15 |    3 |   15 |   16 |    7 |    6 |   14 |    3 |   16 |    4 |    4 |   18 |    8 |   11 |    9 |   18 |   18 |   14 |    5 |   17 |   11 |   15 |    9 |   11 |   10\n",
            "\n",
            "Example 2:\n",
            "Source:   11 |    4 |    4 |   10 |    5 |   19 |    5 |    1 |   11 |    1 |   12 |   16 |   11 |    6 |   12 |    8 |   10 |    5 |   11 |   11 |   11 |    1 |    9 |   16 |    9 |    3 |   15 |   19 |   13 |    9 |   14 |    7\n",
            "Target:    0 |    0 |    0 |    0 |   11 |    4 |    4 |   10 |    5 |   19 |    5 |    1 |   11 |    1 |   12 |   16 |   11 |    6 |   12 |    8 |   10 |    5 |   11 |   11 |   11 |    1 |    9 |   16 |    9 |    3 |   15 |   19\n",
            "\n",
            "Example 3:\n",
            "Source:    3 |   17 |    2 |    5 |   12 |   15 |   14 |    2 |    5 |    1 |   19 |    8 |    3 |   14 |   10 |   13 |    8 |   11 |    8 |   12 |   19 |    7 |   14 |   15 |    6 |    7 |   12 |    2 |   17 |    3 |   11 |    8\n",
            "Target:    0 |    0 |    0 |    0 |    3 |   17 |    2 |    5 |   12 |   15 |   14 |    2 |    5 |    1 |   19 |    8 |    3 |   14 |   10 |   13 |    8 |   11 |    8 |   12 |   19 |    7 |   14 |   15 |    6 |    7 |   12 |    2\n",
            "\n",
            "Example 4:\n",
            "Source:   17 |   15 |    1 |    3 |   18 |   13 |   18 |    6 |    8 |   11 |    3 |    7 |   19 |   15 |    6 |   13 |    2 |    2 |   15 |   19 |   10 |    2 |    8 |   14 |   14 |    8 |   13 |    4 |    9 |   15 |   19 |   14\n",
            "Target:    0 |    0 |    0 |    0 |   17 |   15 |    1 |    3 |   18 |   13 |   18 |    6 |    8 |   11 |    3 |    7 |   19 |   15 |    6 |   13 |    2 |    2 |   15 |   19 |   10 |    2 |    8 |   14 |   14 |    8 |   13 |    4\n"
          ]
        }
      ],
      "source": [
        "# don't change\n",
        "data_args = {\n",
        "    \"num_samples\": 1000,\n",
        "    \"seq_len\": 32,\n",
        "    \"vocab_size\": 20,\n",
        "    \"distance\": 4,\n",
        "}\n",
        "\n",
        "\n",
        "class ShiftDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Shifts the elements of a sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, seq_len=32, vocab_size=20, distance=4):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.distance = distance\n",
        "\n",
        "        self.src = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
        "        self.tgt = torch.zeros_like(self.src)\n",
        "        self.tgt[:, distance:] = self.src[:, :-distance]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"inputs\": self.src[idx], \"targets\": self.tgt[idx]}\n",
        "\n",
        "\n",
        "def visualize_dataset(dataset, num_examples: int = 4):\n",
        "    for i in range(num_examples):\n",
        "        batch = dataset[i]\n",
        "\n",
        "        print(f\"\\nExample {i + 1}:\")\n",
        "        print(f\"Source: {' | '.join([f'{i:4d}' for i in batch['inputs'].tolist()])}\")\n",
        "        print(f\"Target: {' | '.join([f'{i:4d}' for i in batch['targets'].tolist()])}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed()\n",
        "    data = ShiftDataset(**data_args)\n",
        "    visualize_dataset(data, 4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79335274",
      "metadata": {
        "id": "79335274"
      },
      "source": [
        "# Part 1 - Positional Embeddings (2.5 Points)\n",
        "\n",
        "In class, we discussed various forms of position embeddings for self-attention in Transformers. In this part you will implement four positional embeddings, each worth 0.5 points:\n",
        "\n",
        "1. Sinusodial encoding (Vaswani et al. 2017) --- The original Transformer's absolute position encoding. The original Transformer used fixed sinusoidal functions to encode absolute positions:\n",
        "$$\\text{PE}(pos, 2i) = \\sin(pos / 10000^{2i/d_{model}})$$\n",
        "$$\\text{PE}(pos, 2i+1) = \\cos(pos / 10000^{2i/d_{model}})$$\n",
        "where pos is the position and i is the dimension. These encodings are added to the input embeddings before the first transformer layer.\n",
        "\n",
        "2. Relative bias encoding in [T5](https://arxiv.org/abs/1910.10683) --- Learned relative position biases. T5 introduced **relative position biases** that are added directly to attention scores rather than to embeddings. The key innovation is **bucketing**: relative distances are grouped into buckets, with exact positions for small distances and logarithmic spacing for larger distances.\n",
        "There is a single set of attention \"bias\" scalars ($e_\\Delta$) shared across all layers. Each attention head has its own set of bias values. The offsets in the bias values are logarithmically spaced, and there are 32 total bias values.\n",
        "- **Bucketing scheme**:\n",
        "  - First half of buckets: exact positions (0, 1, 2, ..., 15)\n",
        "  - Second half: logarithmically spaced (16-32, 32-64, 64-128, ...)\n",
        "The bias is added to attention logits: $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}} + \\text{bias})$\n",
        "\n",
        "We provide more explanation on T5 buckets later in the homework.\n",
        "\n",
        "3. [RoPE (Rotary Position Embedding)](https://arxiv.org/abs/2104.09864) (Su et al. 2021) --- Rotation-based relative encoding\n",
        "RoPE applies position-dependent rotations to query and key vectors in 2D subspaces. For a position $m$, pairs of dimensions are rotated by angle $m\\theta_i$, where $\\theta_i = 10000^{-2i/d}$. The rotation can be expressed as:\n",
        "$$\\begin{bmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{bmatrix} \\begin{bmatrix} q_{2i} \\\\ q_{2i+1} \\end{bmatrix}$$\n",
        "\n",
        "4. [ALIBI](https://arxiv.org/abs/2108.12409) (Press et al. 2021) --- Linear position relatigve position bias. Like the T5 relative position bias, ALiBi adds a simple linear bias based on distance: $-m \\cdot (i - j)$, where $m$ is a learned head-specific scalar slope parameter and $(i-j)$ is the distance between positions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9c6b4b9d",
      "metadata": {
        "id": "9c6b4b9d"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_position_encoding(seq_len: int, d_model: int) -> torch.Tensor:\n",
        "    res = torch.zeros((seq_len, d_model))\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO: Compute sinusoidal position encodings\n",
        "    ## PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "    ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "    ## Hint: Returns tensor of shape (seq_len, d_model) containing position encodings\n",
        "    #########################################################################\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
        "        -(math.log(10000.0) / d_model)\n",
        "    )\n",
        "\n",
        "    res[:, 0::2] = torch.sin(position * div_term)\n",
        "    res[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T5 Buckets Explanation**\n",
        "\n",
        "- 1: Split buckets for bidirectional distances\n",
        "  - `bidirectional=True`, so half of the buckets are for negative relative positions and half for positive relative positions.  \n",
        "  - `num_buckets = 8` $\\to$ 4 buckets for negative, 4 buckets for positive.  \n",
        "\n",
        "- 2: Map negative positions\n",
        "  - Negative relative positions (`query < key`) are mapped to the lower half of the buckets: `[0, 1, 2,3]`.  \n",
        "  - Zero distance (`query == key`) is treated as the smallest positive distance (usually mapped to the first (0) positive bucket).  \n",
        "\n",
        "- 3: Map positive positions\n",
        "  - Positive relative positions (`query > key`) are mapped to the upper half of the buckets: `[4, 5, 6, 7]`.  \n",
        "\n",
        "- 4: Exact vs logarithmic buckets\n",
        "  - Small distances (i.e., distances < num_buckets//2) are assigned to exact buckets.  \n",
        "  - Larger distances (beyond this range) are mapped logarithmically to the remaining buckets, which allows covering larger relative positions efficiently without needing a bucket for every possible distance.\n",
        "\n",
        "  We provide you with a sanity check in the second cell and you can find more test cases on Markus.\n"
      ],
      "metadata": {
        "id": "SE8tvzHAa5KK"
      },
      "id": "SE8tvzHAa5KK"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5a5890db",
      "metadata": {
        "id": "5a5890db"
      },
      "outputs": [],
      "source": [
        "def t5_relative_position_bucket(\n",
        "    relative_position: torch.Tensor,\n",
        "    bidirectional: bool = True,\n",
        "    num_buckets: int = 32,\n",
        "    max_distance: int = 128,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Helper function\n",
        "    Translate relative positions to bucket indices.\n",
        "\n",
        "    Buckets are divided into two parts:\n",
        "    1. Exact buckets: for small relative distances  (0, 1, 2, ..., num_buckets//2 - 1)\n",
        "    2. Logarithmic buckets: for larger relative distances up to max_distance\n",
        "\n",
        "    If bidirectional=True, positive and negative positions get separate sets of buckets.\n",
        "\n",
        "    Args:\n",
        "        relative_position: Tensor of relative positions (query_pos - key_pos)\n",
        "        bidirectional: If True, use separate buckets for positive/negative distances\n",
        "        num_buckets: Total number of buckets,\n",
        "            You can assume num_buckets is always even and larger than 2\n",
        "        max_distance: Maximum distance to consider\n",
        "    \"\"\"\n",
        "    relative_buckets = torch.zeros((relative_position.shape), dtype=torch.int32)\n",
        "\n",
        "    # 1: Handle bidirectional positions\n",
        "    if bidirectional:\n",
        "        # Split buckets: half for negative, half for positive\n",
        "        num_buckets //= 2\n",
        "        # Add offset for positive positions (they go in upper half)\n",
        "        relative_buckets += (relative_position > 0).to(torch.int32) * num_buckets\n",
        "        # Work with absolute distances from now on\n",
        "        relative_position = torch.abs(relative_position)\n",
        "    else:\n",
        "        # For unidirectional (causal), only keep non-positive distances\n",
        "        # Clip positive values to 0 (future positions not allowed in decoder)\n",
        "        relative_position = torch.clamp(-relative_position, min=0)\n",
        "\n",
        "    # 2: Assign small distances to exact buckets\n",
        "    # Half of num_buckets are for exact small distances\n",
        "    max_exact = num_buckets // 2\n",
        "    is_small = relative_position < max_exact\n",
        "\n",
        "    # 3: Logarithmic buckets for large distances\n",
        "    # For distances >= max_exact, use logarithmic spacing\n",
        "    # Clamp the relative position to max_exact (lower bound) but don't clamp upper\n",
        "    relative_position_if_large = torch.clamp(\n",
        "        relative_position.float(),\n",
        "        min=max_exact\n",
        "    )\n",
        "\n",
        "    # Logarithmic bucketing formula\n",
        "    # Maps [max_exact, max_distance] to [max_exact, num_buckets-1]\n",
        "    val_if_large = max_exact + (\n",
        "        torch.log(relative_position_if_large / max_exact) /\n",
        "        math.log(max_distance / max_exact) *\n",
        "        (num_buckets - max_exact)\n",
        "    ).to(torch.int32)\n",
        "\n",
        "    # Clip to maximum bucket index\n",
        "    val_if_large = torch.clamp(val_if_large, max=num_buckets - 1)\n",
        "\n",
        "    # Combine: use exact buckets for small, logarithmic for large\n",
        "    relative_buckets += torch.where(is_small, relative_position, val_if_large)\n",
        "\n",
        "    return relative_buckets\n",
        "\n",
        "\n",
        "def t5_relative_position_bias_with_values(\n",
        "    query_len: int,\n",
        "    key_len: int,\n",
        "    bias_values: torch.Tensor,  # (num_heads, num_buckets) - learned parameters\n",
        "    num_buckets: int = 32,\n",
        "    max_distance: int = 128,\n",
        "    bidirectional: bool = True,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute T5-style relative position biases using PROVIDED bias values.\n",
        "    \"\"\"\n",
        "    # Create position matrices\n",
        "    query_positions = torch.arange(query_len).unsqueeze(1)  # (query_len, 1)\n",
        "    key_positions = torch.arange(key_len).unsqueeze(0)      # (1, key_len)\n",
        "\n",
        "    # Compute relative positions: query - key\n",
        "    # Broadcasting: (query_len, 1) - (1, key_len) -> (query_len, key_len)\n",
        "    relative_position = query_positions - key_positions\n",
        "\n",
        "    # Convert to buckets\n",
        "    buckets = t5_relative_position_bucket(\n",
        "        relative_position,\n",
        "        bidirectional=bidirectional,\n",
        "        num_buckets=num_buckets,\n",
        "        max_distance=max_distance,\n",
        "    )\n",
        "\n",
        "    # Use the provided learned bias values\n",
        "    res = bias_values[:, buckets]  # (num_heads, query_len, key_len)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## sanity check\n",
        "if __name__ == \"__main__\":\n",
        "    seq_len_ = 5\n",
        "    num_buckets_ = 8\n",
        "    relative_position = torch.tensor([\n",
        "        [ 0, -1, -2, -3, -4],\n",
        "        [ 1,  0, -1, -2, -3],\n",
        "        [ 2,  1,  0, -1, -2],\n",
        "        [ 3,  2,  1,  0, -1],\n",
        "        [ 4,  3,  2,  1,  0],\n",
        "    ])\n",
        "\n",
        "\n",
        "    buckets = t5_relative_position_bucket(relative_position, bidirectional=True,\n",
        "                                          num_buckets=num_buckets_, max_distance=3)\n",
        "    expected = torch.tensor([[0, 1, 2, 3, 3],\n",
        "        [5, 0, 1, 2, 3],\n",
        "        [6, 5, 0, 1, 2],\n",
        "        [7, 6, 5, 0, 1],\n",
        "        [7, 7, 6, 5, 0]], dtype=torch.int32)\n",
        "    assert torch.allclose(buckets, expected)"
      ],
      "metadata": {
        "id": "uDjK0NDkatO3"
      },
      "id": "uDjK0NDkatO3",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5e2e2ef9",
      "metadata": {
        "id": "5e2e2ef9"
      },
      "outputs": [],
      "source": [
        "def alibi_bias(query_len: int, key_len: int, num_heads: int = 8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute ALiBi position biases.\n",
        "    Returns:\n",
        "        Tensor of shape (num_heads, query_len, key_len) containing linear biases\n",
        "    \"\"\"\n",
        "    res = torch.zeros((num_heads, query_len, key_len))\n",
        "\n",
        "    # 1. Compute slopes for each head: 2^(-8*(h+1)/num_heads)\n",
        "    # Different heads get different slopes (geometric sequence)\n",
        "    head_indices = torch.arange(1, num_heads + 1, dtype=torch.float32)  # [1, 2, 3, ..., num_heads]\n",
        "    slopes = torch.pow(2, -8 * head_indices / num_heads)  # 2^(-8*h/num_heads)\n",
        "    slopes = slopes.unsqueeze(1).unsqueeze(2)  # Shape: (num_heads, 1, 1) for broadcasting\n",
        "\n",
        "    # 2. Create distance matrix: relative_positions_{ij} = (i - j)\n",
        "    query_positions = torch.arange(query_len, dtype=torch.float32).unsqueeze(1)  # (query_len, 1)\n",
        "    key_positions = torch.arange(key_len, dtype=torch.float32).unsqueeze(0)  # (1, key_len)\n",
        "\n",
        "    # Broadcasting: (query_len, 1) - (1, key_len) -> (query_len, key_len)\n",
        "    relative_positions = query_positions - key_positions  # (query_len, key_len)\n",
        "    relative_positions = relative_positions.unsqueeze(0)  # (1, query_len, key_len)\n",
        "\n",
        "    # 3. Apply slopes: -slope * distance for each head\n",
        "    res = -slopes * relative_positions  # (num_heads, query_len, key_len)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6145060e",
      "metadata": {
        "id": "6145060e"
      },
      "outputs": [],
      "source": [
        "def apply_rotary_position_embedding(\n",
        "    x: torch.Tensor,\n",
        "    positions: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply rotary position embedding to input tensor.\n",
        "    Args:\n",
        "        x: Input tensor of shape (..., seq_len, dim)\n",
        "        positions: Position indices of shape (seq_len,)\n",
        "\n",
        "    Returns:\n",
        "        Tensor of same shape as x with rotary embeddings applied\n",
        "    \"\"\"\n",
        "    *batch_dims, seq_len, dim = x.shape\n",
        "\n",
        "    # 1. Compute rotation frequencies: theta_i = 10000^(-2i/dim)\n",
        "    # i goes from 0, 1, 2, ..., dim//2 - 1 (for each pair)\n",
        "    i = torch.arange(0, dim, 2, dtype=torch.float32, device=x.device)\n",
        "    theta = torch.pow(10000.0, -i / dim)  # Shape: (dim//2,)\n",
        "\n",
        "    # 2. Compute angles: m * theta_i for each position m\n",
        "    # positions: (seq_len,), theta: (dim//2,)\n",
        "    # We want: (seq_len, dim//2)\n",
        "    positions = positions.float().unsqueeze(1)  # (seq_len, 1)\n",
        "    theta = theta.unsqueeze(0)  # (1, dim//2)\n",
        "    angles = positions * theta  # Broadcasting: (seq_len, dim//2)\n",
        "\n",
        "    # Compute cos and sin for the rotation\n",
        "    cos_angles = torch.cos(angles)  # (seq_len, dim//2)\n",
        "    sin_angles = torch.sin(angles)  # (seq_len, dim//2)\n",
        "\n",
        "    # Add batch dimensions for broadcasting\n",
        "    for _ in batch_dims:\n",
        "        cos_angles = cos_angles.unsqueeze(0)\n",
        "        sin_angles = sin_angles.unsqueeze(0)\n",
        "\n",
        "    # 3. Reshape x into pairs: (..., seq_len, dim//2, 2)\n",
        "    x_reshaped = x.view(*batch_dims, seq_len, dim // 2, 2)\n",
        "\n",
        "    # Extract the two elements of each pair\n",
        "    x1 = x_reshaped[..., 0]  # (..., seq_len, dim//2) - even indices\n",
        "    x2 = x_reshaped[..., 1]  # (..., seq_len, dim//2) - odd indices\n",
        "\n",
        "    # 4. Apply rotation matrix to each pair\n",
        "    # Rotation matrix: [cos  -sin] [x1]\n",
        "    #                  [sin   cos] [x2]\n",
        "    # Result: [x1*cos - x2*sin]\n",
        "    #         [x1*sin + x2*cos]\n",
        "\n",
        "    rotated_x1 = x1 * cos_angles - x2 * sin_angles\n",
        "    rotated_x2 = x1 * sin_angles + x2 * cos_angles\n",
        "\n",
        "    # 5. Reshape back to original shape\n",
        "    # Stack the rotated pairs back together\n",
        "    res = torch.stack([rotated_x1, rotated_x2], dim=-1)  # (..., seq_len, dim//2, 2)\n",
        "    res = res.view(*batch_dims, seq_len, dim)  # (..., seq_len, dim)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572245fc",
      "metadata": {
        "id": "572245fc"
      },
      "source": [
        "## Part 2 - Implement the transformer\n",
        "\n",
        "- Pre vs. post layernorm (0.5 points)\n",
        "- Norm type: layernorm and rmsnorm (0.5 points each)\n",
        "- Correct transformer implementation (0.8 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c7cadf",
      "metadata": {
        "id": "a5c7cadf"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization (used in LLaMA)\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = x\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: RMS = sqrt(mean(x^2) + eps^2)\n",
        "        ## Hint: RMSNorm operates on the last dimension\n",
        "        #########################################################################\n",
        "        return rms\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization (Ba et al., 2016)\n",
        "\n",
        "    Normalizes across the feature dimension:\n",
        "    y = gamma * (x - mean) / sqrt(var + eps) + beta\n",
        "\n",
        "    Used in original Transformer (Vaswani et al., 2017)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))  # gamma\n",
        "        self.bias = nn.Parameter(torch.zeros(d_model))  # beta\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        ########################### YOUR CODE ###################################\n",
        "        # TODO: Implement Layer Normalization\n",
        "        # 1. Compute mean, Hint: across feature dimension (dim=-1)\n",
        "        # 2. Compute variance across feature dimension\n",
        "        # 3. Normalize: (x - mean) / sqrt(var + eps)\n",
        "        # 4. Apply affine transformation: weight * normalized + bias\n",
        "        #########################################################################\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331e9a57",
      "metadata": {
        "id": "331e9a57"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tiny Transformer\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=64,\n",
        "        nhead=4,\n",
        "        num_layers=2,\n",
        "        max_len=128,\n",
        "        pos_encoding=\"rope\",\n",
        "        num_t5_buckets: int = 32,\n",
        "        norm_type: Literal[\"rmsnorm\", \"layernorm\"] = \"layernorm\",\n",
        "        pre_norm: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding_type = pos_encoding\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerLayer(d_model, nhead, pos_encoding, norm_type, pre_norm)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        ########################### YOUR CODE ###################################\n",
        "        # TODO: Create t5_bias if pos_encoding is t5\n",
        "        # Hint: T5 biases should be learnable parameters of shape (nhead, num_t5_buckets)\n",
        "        self.t5_bias = None\n",
        "        if pos_encoding == \"t5\":\n",
        "            pass\n",
        "        #########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        ########################### YOUR CODE ###################################\n",
        "        # TODO: Add sinusoidal position encodings if pos_encoding_type is \"sinusoidal\"\n",
        "        if self.pos_encoding_type == \"sinusoidal\":\n",
        "            ## Hint: Make sure to cast the pos_enc to shape (1, seq_len, d_model)\n",
        "            pass\n",
        "        #########################################################################\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, t5_biases=self.t5_bias)\n",
        "\n",
        "        return self.output(x)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_fn(y_hat: torch.Tensor, y: torch.Tensor):\n",
        "        res = 0.0\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Implement cross-entropy loss\n",
        "        ## Hint: y has class labels (e.g. 1, 2, 3, 4, ...), it is not one-hot encoded\n",
        "        #########################################################################\n",
        "        return res\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        nhead,\n",
        "        pos_encoding=\"rope\",\n",
        "        norm_type: Literal[\"layernorm\", \"rmsnorm\"] = \"layernorm\",\n",
        "        pre_norm: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.head_dim = d_model // nhead\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        ##########################################\n",
        "        ## Initialize normalization layers based on norm_type\n",
        "        if norm_type == \"layernorm\":\n",
        "            self.norm1 = nn.LayerNorm(d_model)\n",
        "            self.norm2 = nn.LayerNorm(d_model)\n",
        "        elif norm_type == \"rmsnorm\":\n",
        "            self.norm1 = RMSNorm(d_model)\n",
        "            self.norm2 = RMSNorm(d_model)\n",
        "        else:\n",
        "            raise ValueError(f\"Only norms available are RMSNorm and LayerNorm\")\n",
        "        ##########################################\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Create feed forward layers\n",
        "        ## Linear -> GeLU -> Linear, FFN inner_dimension should be 4 d_model\n",
        "        ## Hint: Make sure that your ffn is wrapped in nn.Sequential\n",
        "        self.ffn = nn.Sequential()\n",
        "        #########################################################################\n",
        "\n",
        "    def forward(self, x, t5_biases):\n",
        "        ########################### YOUR CODE ###################################\n",
        "        # TODO: Implement pre-norm or post-norm architecture\n",
        "        # Pre-norm (modern): x = x + sublayer(norm(x))\n",
        "        # Post-norm (original): x = norm(x + sublayer(x))\n",
        "        # Hint: Check self.pre_norm to decide which to use\n",
        "        ##########################################\n",
        "        if self.pre_norm:\n",
        "            ## TODO: Pre-norm: normalize before sublayer\n",
        "            pass\n",
        "        else:\n",
        "            ## TODO: Post-norm: normalize after residual\n",
        "            pass\n",
        "        #########################################################################\n",
        "        return x\n",
        "\n",
        "    def self_attention(self, x, t5_biases):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if self.pos_encoding == \"rope\":\n",
        "          positions = torch.arange(seq_len, device=x.device)\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Apply RoPE if pos_encoding is \"rope\"\n",
        "            pass\n",
        "        #########################################################################\n",
        "\n",
        "        # Compute attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim**0.5)\n",
        "\n",
        "        ########################### YOUR CODE ###################################\n",
        "        ## TODO: Add position biases based on pos_encoding type\n",
        "        ## ALiBi: Use alibi_bias(seq_len, seq_len, self.nhead)\n",
        "        ## T5: Use t5_relative_position_bias_with_values(seq_len, seq_len, t5_biases)\n",
        "        ## Hint: Add the bias to scores before softmax\n",
        "        ## Hint: Move bias to same device as scores\n",
        "        ##########################################\n",
        "        if self.pos_encoding == \"alibi\":\n",
        "            pass\n",
        "        elif self.pos_encoding == \"t5\":\n",
        "            pass\n",
        "        #########################################################################\n",
        "\n",
        "        # Apply causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
        "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, float(\"-inf\"))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.out_proj(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23cc2244",
      "metadata": {
        "id": "23cc2244"
      },
      "source": [
        "## Trainer (0.2 points)\n",
        "Implement the trainer, this will be quite similar to Hw 6 and 7 trainer with small tweaks:\n",
        "- We won't use attention_mask\n",
        "- We will keep track of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9350854f",
      "metadata": {
        "id": "9350854f"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_epochs,\n",
        "        batch_size,\n",
        "        gradient_clip_val=1,\n",
        "        device=\"cpu\",\n",
        "        print_every: int = 5,\n",
        "    ):\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.gradient_clip_val = gradient_clip_val\n",
        "        self.device = device\n",
        "        self.train_loss = []  # record the avg. batch loss every epoch\n",
        "        self.valid_loss = []  # record the avg. batch loss every epoch\n",
        "        self.train_acc = []  # record the avg. batch accuracy every epoch\n",
        "        self.val_acc = []  # record the avg. batch accuracy every epoch\n",
        "        self.print_every = print_every\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_gradients(model, max_norm):\n",
        "        if not max_norm:\n",
        "            return\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "    def get_dataloader(self, data):\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(SEED)\n",
        "        train_size = int(0.8 * len(data))\n",
        "        train_data, val_data = random_split(data, [train_size, len(data) - train_size], generator=g)\n",
        "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True, generator=g)\n",
        "        valid_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=False, generator=g)\n",
        "\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    def fit(self, model, data, optimizer=None):\n",
        "        model.to(self.device)\n",
        "        if optimizer is None:\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=model.lr)\n",
        "        train_loader, valid_loader = self.get_dataloader(data)\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            valid_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            ########################### YOUR CODE ###################################\n",
        "            # TODO: Train the model for max_epochs many steps\n",
        "            # Complete a single forward and backward pass on a given training batch\n",
        "            # Record the training loss and training accuracy\n",
        "            for batch in train_loader:\n",
        "                # Extract from dictionary\n",
        "                X = batch[\"inputs\"].to(self.device)\n",
        "                Y = batch[\"targets\"].to(self.device)\n",
        "                ## TODO: Training logic\n",
        "                ## TODO: compute accuracy\n",
        "            ########################################################################\n",
        "            assert 0.0 <= correct / total <= 1.0, \"Accuracy should be between 0 and 1\"\n",
        "            self.train_loss.append(train_loss / len(train_loader))\n",
        "            self.train_acc.append(correct / total)\n",
        "\n",
        "            model.eval()\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            with torch.no_grad():\n",
        "                ########################### YOUR CODE ###################################\n",
        "                # TODO: at the end of each epoch, evaluate the model on the validation set.\n",
        "                # Complete a single forward pass on a given validation batch\n",
        "                # Record the validation loss and accuracy\n",
        "                for batch in valid_loader:\n",
        "                    X = batch[\"inputs\"].to(self.device)\n",
        "                    Y = batch[\"targets\"].to(self.device)\n",
        "\n",
        "                ########################################################################\n",
        "            self.valid_loss.append(valid_loss / len(valid_loader))\n",
        "            self.val_acc.append(val_correct / val_total)\n",
        "\n",
        "            if (epoch + 1) % self.print_every == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1} train loss: {self.train_loss[-1]:.5f},\\t train acc: {self.train_acc[-1]:.5f}\\n\\tvalidation loss {self.valid_loss[-1]:.5f}, val acc: {self.val_acc[-1]:.5f} \"\n",
        "                )\n",
        "\n",
        "    def predict(self, model, dataloader):\n",
        "        \"\"\"\n",
        "        Generate predictions on a dataset.\n",
        "        Returns:\n",
        "            predictions, targets, avg_loss\n",
        "        \"\"\"\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ########################### YOUR CODE ###################################\n",
        "            # TODO: Generate predictions for all batches in the dataloader\n",
        "            # 1. Extract inputs and targets from batch\n",
        "            # 2. Move tensors to device\n",
        "            # 3. Get model predictions\n",
        "            # 4. Store predictions and targets, make sure to store predictions as the index of the predicted class\n",
        "            # 5. Compute loss\n",
        "            # 6. Compute accuracy\n",
        "\n",
        "            for batch in dataloader:\n",
        "                # Extract batch data\n",
        "                X = batch[\"inputs\"].to(self.device)\n",
        "                Y = batch[\"targets\"].to(self.device)\n",
        "\n",
        "            ########################################################################\n",
        "\n",
        "        # Concatenate all predictions and targets\n",
        "        predictions = torch.cat(all_predictions, dim=0)\n",
        "        targets = torch.cat(all_targets, dim=0)\n",
        "        avg_acc = (predictions == targets).float().mean().item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        return predictions, targets, avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65cb28e",
      "metadata": {
        "id": "e65cb28e"
      },
      "outputs": [],
      "source": [
        "# don't change\n",
        "def visualize_training(\n",
        "    trainer, data, positional_encoding_type, model=None, vocab_size=20, title=\"\"\n",
        "):\n",
        "    tr, val = trainer.get_dataloader(data)\n",
        "    preds, targets, avg_loss, avg_acc = trainer.predict(model, val)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
        "    fig.suptitle(\n",
        "        f\"{title} - Val Acc: {avg_acc:.4f}\",\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "    # 1. Confusion matrix\n",
        "    cm = confusion_matrix(\n",
        "        targets.flatten(), preds.flatten(), labels=np.arange(1, vocab_size)\n",
        "    )\n",
        "    im = axs[0].imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n",
        "    axs[0].set_xlabel(\"Predicted Token\", fontsize=11)\n",
        "    axs[0].set_ylabel(\"Target Token\", fontsize=11)\n",
        "    axs[0].set_title(\"Confusion Matrix\", fontsize=12)\n",
        "    plt.colorbar(im, ax=axs[0], fraction=0.046)\n",
        "\n",
        "    # 2. Training and validation loss\n",
        "    axs[1].plot(trainer.train_loss, label=\"Train Loss\", linewidth=2)\n",
        "    axs[1].plot(trainer.valid_loss, label=\"Val Loss\", linewidth=2)\n",
        "    axs[1].set_ylabel(\"Loss\", fontsize=11)\n",
        "    axs[1].set_xlabel(\"Epoch\", fontsize=11)\n",
        "    axs[1].set_title(\"Training & Validation Loss\", fontsize=12)\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(alpha=0.3)\n",
        "\n",
        "    # 3. Training and validation accuracy\n",
        "    axs[2].plot(trainer.train_acc, label=\"Train Acc\", linewidth=2)\n",
        "    axs[2].plot(trainer.val_acc, label=\"Val Acc\", linewidth=2)\n",
        "    axs[2].set_ylabel(\"Accuracy\", fontsize=11)\n",
        "    axs[2].set_xlabel(\"Epoch\", fontsize=11)\n",
        "    axs[2].set_title(\"Training & Validation Accuracy\", fontsize=12)\n",
        "    axs[2].set_ylim([0, 1.05])\n",
        "    axs[2].legend()\n",
        "    axs[2].grid(alpha=0.3)\n",
        "\n",
        "    # 4. Positional encoding visualization (if applicable)\n",
        "    if positional_encoding_type == \"t5\" and model is not None:\n",
        "        # Visualize learned T5 biases\n",
        "        seq_len = 64\n",
        "        t5_bias = t5_relative_position_bias_with_values(\n",
        "            seq_len, seq_len, model.t5_bias.detach().cpu()\n",
        "        )\n",
        "        im = axs[3].imshow(\n",
        "            t5_bias[0].numpy(), cmap=\"RdBu\", aspect=\"auto\", vmin=-3, vmax=3\n",
        "        )\n",
        "        axs[3].set_xlabel(\"Key Position\", fontsize=11)\n",
        "        axs[3].set_ylabel(\"Query Position\", fontsize=11)\n",
        "        axs[3].set_title(\"Learned T5 Bias (Head 0)\", fontsize=12)\n",
        "        axs[3].plot([0, seq_len - 1], [0, seq_len - 1], \"k--\", linewidth=1, alpha=0.3)\n",
        "        plt.colorbar(im, ax=axs[3], fraction=0.046)\n",
        "\n",
        "    elif positional_encoding_type == \"alibi\":\n",
        "        # Visualize ALiBi biases\n",
        "        seq_len = 64\n",
        "        alibi = alibi_bias(seq_len, seq_len, model.layers[0].nhead if model else 8)\n",
        "        im = axs[3].imshow(alibi[0].numpy(), cmap=\"RdBu\", aspect=\"auto\")\n",
        "        axs[3].set_xlabel(\"Key Position\", fontsize=11)\n",
        "        axs[3].set_ylabel(\"Query Position\", fontsize=11)\n",
        "        axs[3].set_title(\"ALiBi Bias (Head 0)\", fontsize=12)\n",
        "        axs[3].plot([0, seq_len - 1], [0, seq_len - 1], \"k--\", linewidth=1, alpha=0.3)\n",
        "        plt.colorbar(im, ax=axs[3], fraction=0.046)\n",
        "\n",
        "    elif positional_encoding_type == \"rope\" and model is not None:\n",
        "        # Visualize RoPE attention pattern\n",
        "        seq_len = 64\n",
        "        d_model = model.d_model\n",
        "        q = torch.randn(1, seq_len, d_model)\n",
        "        k = torch.randn(1, seq_len, d_model)\n",
        "        positions = torch.arange(seq_len)\n",
        "        q_rope = apply_rotary_position_embedding(q, positions)\n",
        "        k_rope = apply_rotary_position_embedding(k, positions)\n",
        "        scores = torch.matmul(q_rope, k_rope.transpose(-2, -1)) / np.sqrt(d_model)\n",
        "        im = axs[3].imshow(scores[0].numpy(), cmap=\"RdBu\", aspect=\"auto\")\n",
        "        axs[3].set_xlabel(\"Key Position\", fontsize=11)\n",
        "        axs[3].set_ylabel(\"Query Position\", fontsize=11)\n",
        "        axs[3].set_title(\"RoPE Attention Pattern\", fontsize=12)\n",
        "        axs[3].plot([0, seq_len - 1], [0, seq_len - 1], \"k--\", linewidth=1, alpha=0.3)\n",
        "        plt.colorbar(im, ax=axs[3], fraction=0.046)\n",
        "\n",
        "    elif positional_encoding_type == \"sinusoidal\":\n",
        "        # Visualize sinusoidal encoding\n",
        "        seq_len = 64\n",
        "        sin_enc = sinusoidal_position_encoding(seq_len, model.d_model if model else 64)\n",
        "        im = axs[3].imshow(sin_enc.T.numpy(), cmap=\"RdBu\", aspect=\"auto\")\n",
        "        axs[3].set_xlabel(\"Position\", fontsize=11)\n",
        "        axs[3].set_ylabel(\"Embedding Dimension\", fontsize=11)\n",
        "        axs[3].set_title(\"Sinusoidal Encoding\", fontsize=12)\n",
        "        plt.colorbar(im, ax=axs[3], fraction=0.046)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3481c62d",
      "metadata": {
        "id": "3481c62d"
      },
      "source": [
        "### Train the models and examine the output\n",
        "Unlike previous homeworks, we won't ask you to optimize the hyper-parameters. If your implementation is correct, with the given hyper-parameters, you should achieve >0.8 accuracy with sinussoidal, T5, and RoPE. ~0.3 with no encodings and ALiBi.\n",
        "\n",
        "(ungraded)\n",
        "\n",
        "After running the training code and examining the visualizations, answer the following question:\n",
        "\n",
        "1. Can the model learn the shift task without any position encoding (pos_encoding=\"none\")?\n",
        "   Why or why not? What does this tell you about the importance of position information?\n",
        "1. Why does the ALiBi model perform poorly? What tweaks could have improved the performance?\n",
        "1. Are certain positional embeddings better suited for certain datasets? (You can also experiment with three other datasets included in the next cells).\n",
        "1. Examine the encoding heatmaps (4th column). Describe the patterns, are they similar to how you would expect them to be?\n",
        "\n",
        "**Task-specific**\n",
        "1. The shift task requires outputting token 0 for the first `distance` positions.\n",
        "   Can you identify if the model learned this from the confusion matrix?\n",
        "1. The shift task is a \"relative position\" task (copy from distance=4 back).\n",
        "   Explain why relative position encodings (RoPE, T5, ALiBi) should theoretically\n",
        "   perform better than absolute encodings (Sinusoidal) on this task.\n",
        "1. If you trained on sequences of length 16, which encoding would you expect to\n",
        "   perform best on sequences of length 32? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98daa59f",
      "metadata": {
        "id": "98daa59f"
      },
      "outputs": [],
      "source": [
        "# don't change,\n",
        "args = {\n",
        "    \"lr\": 3e-4,\n",
        "    \"d_model\": 64,\n",
        "    \"nhead\": 8,\n",
        "    \"batch_size\": 32,\n",
        "    \"num_epochs\": 25,\n",
        "    \"gradient_clip_val\": 1.0,\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configuration\n",
        "    set_seed()\n",
        "    data = ShiftDataset(\n",
        "        data_args[\"num_samples\"],\n",
        "        data_args[\"seq_len\"],\n",
        "        data_args[\"vocab_size\"],\n",
        "        data_args[\"distance\"],\n",
        "    )\n",
        "    for positional_encoding_type in [\"none\", \"sinusoidal\", \"rope\", \"t5\", \"alibi\"]:\n",
        "        print(\"=\" * 30 + positional_encoding_type + \"=\" * 30)\n",
        "        set_seed()\n",
        "        model = TinyTransformer(\n",
        "            data.vocab_size,\n",
        "            d_model=args[\"d_model\"],\n",
        "            nhead=args[\"nhead\"],\n",
        "            num_layers=2,\n",
        "            max_len=data_args[\"seq_len\"],\n",
        "            pos_encoding=positional_encoding_type,\n",
        "            norm_type=\"rmsnorm\",\n",
        "            pre_norm=True,\n",
        "        )\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
        "\n",
        "        trainer = Trainer(\n",
        "            batch_size=args[\"batch_size\"],\n",
        "            max_epochs=args[\"num_epochs\"],\n",
        "            gradient_clip_val=args[\"gradient_clip_val\"],\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        trainer.fit(model, data, optimizer)\n",
        "\n",
        "        visualize_training(\n",
        "            trainer, data, positional_encoding_type, model, data.vocab_size\n",
        "        )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7f3e38",
      "metadata": {
        "id": "6d7f3e38"
      },
      "source": [
        "(Ungraded)\n",
        "\n",
        "If you would like, you can experiment with different datasets below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e919c0b3",
      "metadata": {
        "id": "e919c0b3"
      },
      "outputs": [],
      "source": [
        "# don't change\n",
        "class ReversalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Reverse the input sequence.\n",
        "\n",
        "    Input:  [a, b, c, d, e, f, g, h]\n",
        "    Output: [h, g, f, e, d, c, b, a]\n",
        "\n",
        "    Requires knowing absolute positions to map position i  position (n-1-i)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, seq_len=16, vocab_size=20):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = torch.randint(1, self.vocab_size, (self.seq_len,))\n",
        "        tgt = torch.flip(src, dims=[0])  # Reverse the sequence\n",
        "\n",
        "        return {\"inputs\": src, \"targets\": tgt}\n",
        "\n",
        "\n",
        "class PositionDependentDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each position in the sequence should output a specific token based on position modulo vocab_size.\n",
        "\n",
        "    Example (vocab_size=10):\n",
        "    Position:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...]\n",
        "    Target:    [1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2,  3,  ...]\n",
        "\n",
        "    Input is random, output depends only on absolute position!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, seq_len=32, vocab_size=10):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input is random noise\n",
        "        src = torch.randint(1, self.vocab_size, (self.seq_len,))\n",
        "\n",
        "        # Target depends ONLY on absolute position\n",
        "        positions = torch.arange(self.seq_len)\n",
        "        tgt = (positions % (self.vocab_size - 1)) + 1  # Cycle through 1 to vocab_size-1\n",
        "\n",
        "        return {\"inputs\": src, \"targets\": tgt}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d1940d",
      "metadata": {
        "id": "f3d1940d"
      },
      "source": [
        "# Part 2 - Architectural Changes - Train\n",
        "\n",
        "For this part of the assignment we will use a different dataset, where we sort the numbers. There is actually nothing to change here (you have completed this part when implementing the architecture and norm layers) but make sure that your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d5558e",
      "metadata": {
        "id": "c1d5558e"
      },
      "outputs": [],
      "source": [
        "class SortingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Sort sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples=1000, seq_len=32, vocab_size=20):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = torch.randint(1, self.vocab_size, (self.seq_len,))\n",
        "        tgt = torch.sort(src)[0]\n",
        "\n",
        "        return {\"inputs\": src, \"targets\": tgt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d441b54f",
      "metadata": {
        "id": "d441b54f"
      },
      "outputs": [],
      "source": [
        "# don't change\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Configuration\n",
        "    set_seed()\n",
        "    data = SortingDataset(\n",
        "        data_args[\"num_samples\"], data_args[\"seq_len\"], data_args[\"vocab_size\"]\n",
        "    )\n",
        "    for norm_type in [\"rmsnorm\", \"layernorm\"]:\n",
        "        for pre_norm in [True, False]:\n",
        "            positional_encoding_type = \"rope\"\n",
        "            # for positional_encoding_type in [\"sinusoidal\", \"rope\", \"t5\", \"alibi\"]:\n",
        "            title = \"Pre-LN\" if pre_norm else \"Post-LN\"\n",
        "            title = f\"{norm_type} - {title} - {positional_encoding_type}\"\n",
        "            print(\"=\" * 30 + title + \"=\" * 30)\n",
        "            set_seed()\n",
        "            model = TinyTransformer(\n",
        "                data.vocab_size,\n",
        "                d_model=args[\"d_model\"],\n",
        "                nhead=args[\"nhead\"],\n",
        "                num_layers=2,\n",
        "                max_len=data_args[\"seq_len\"],\n",
        "                pos_encoding=positional_encoding_type,\n",
        "                norm_type=norm_type,\n",
        "                pre_norm=pre_norm,\n",
        "            )\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
        "\n",
        "            trainer = Trainer(\n",
        "                batch_size=args[\"batch_size\"],\n",
        "                max_epochs=args[\"num_epochs\"],\n",
        "                gradient_clip_val=args[\"gradient_clip_val\"],\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            trainer.fit(model, data, optimizer)\n",
        "\n",
        "            visualize_training(trainer, data, positional_encoding_type, model, data.vocab_size, title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da3e3b2",
      "metadata": {
        "id": "3da3e3b2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "795f1d8b",
      "metadata": {
        "id": "795f1d8b"
      },
      "source": [
        "# Part 3 - Architecture Detective (2 Points)\n",
        "\n",
        "In this part of the homework you will play the architecture detective for five mystery models. These models are/were popular models of NLP. We standardized their state dicts. Your task is to determine the following information about each model looking at their state_dicts.\n",
        "\n",
        "It might look like:\n",
        "```python\n",
        "{\n",
        "  \"vocab_size\": 20, # int\n",
        "  \"d_model\": 12, # int\n",
        "  \"ffn_size\": 32, # int\n",
        "  \"num_hidden_layers\": 2, # int\n",
        "  \"norm_type\": \"rmsnorm\", # \"layernorm\" or \"rmsnorm\"\n",
        "  \"attention_type\": \"mha\", # \"mha\" (for multi-head attention) or \"gqa\" (for grouped-query attention)\n",
        "  \"is_gated\": False,\n",
        "  \"is_tied_embeddings\": False,\n",
        "  \"decoder_only\": False,\n",
        "}\n",
        "```\n",
        "\n",
        "If you are curious and want to go the extra mile you can also try to match the model architectures to their source.\n",
        "\n",
        "## General Hints:\n",
        "At this point in the class you should be familiar with most of these terms and be able to identify them by looking at the state_dicts.\n",
        "\n",
        "- `is_tied_embeddings`: for small models, we might tie (share) the input and output embeddings. [True or False]\n",
        "- `is_gated`: indicates that the feed-forward network includes an additional linear layer\n",
        "  to gate activations (e.g., GEGLU, SwiGLU). [True or False]\n",
        "- `decoder_only`: True if the model follows a decoder-only architecture, False if it is an encoder-decoder model.\n",
        "\n",
        "\n",
        "  The models are listed below:\n",
        "  - 0: https://huggingface.co/r-three/mystery-model-0/blob/main/model.safetensors\n",
        "  - 1: https://huggingface.co/r-three/mystery-model-1/blob/main/model.safetensors\n",
        "  - 2: https://huggingface.co/r-three/mystery-model-2/blob/main/model.safetensors\n",
        "  - 3: https://huggingface.co/r-three/mystery-model-3/blob/main/model.safetensors\n",
        "  - 4: https://huggingface.co/r-three/mystery-model-4/blob/main/model.safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b268c47",
      "metadata": {
        "id": "8b268c47"
      },
      "outputs": [],
      "source": [
        "\n",
        "default_model_dct = {\n",
        "    \"vocab_size\": 0,\n",
        "    \"d_model\": 0,\n",
        "    \"ffn_size\": 0,\n",
        "    \"num_hidden_layers\": 0,\n",
        "    # \"layernorm or rmsnorm\"\n",
        "    \"norm_type\": \"\",\n",
        "    # attention_type should be \"mha (multi-head) or gqa (grouped-query)\",\n",
        "    \"attention_type\":  \"\",\n",
        "    # is_gated, is_tied_embeddings, and decoder_only should be boolean\n",
        "    \"is_gated\": None,\n",
        "    \"is_tied_embeddings\": None,\n",
        "    \"decoder_only\": None,\n",
        "}\n",
        "\n",
        "def type_check(model_dct):\n",
        "    assert all([k in default_model_dct.keys() for k in model_dct.keys()]), \"Don't add additional keys\"\n",
        "    assert all([k in model_dct.keys() for k in default_model_dct.keys()]), \"Don't miss any keys\"\n",
        "    assert isinstance(model_dct[\"vocab_size\"], int), \"vocab_size should be an integer\"\n",
        "    assert isinstance(model_dct[\"d_model\"], int), \"d_model should be an integer\"\n",
        "    assert isinstance(model_dct[\"ffn_size\"], int), \"ffn_size should be an integer\"\n",
        "    assert isinstance(model_dct[\"num_hidden_layers\"], int), \"num_hidden_layers should be an integer\"\n",
        "    assert model_dct[\"norm_type\"] in [\"layernorm\", \"rmsnorm\"]\n",
        "    assert model_dct[\"attention_type\"] in [\"mhq\", \"gqa\"]\n",
        "    assert isinstance(model_dct[\"is_gated\"], bool), \"is_gated should be a boolean\"\n",
        "    assert isinstance(model_dct[\"is_tied_embeddings\"], bool), \"is_tied_embeddings should be a boolean\"\n",
        "    assert isinstance(model_dct[\"decoder_only\"], bool), \"decoder_only should be a boolean\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mystery_model_0():\n",
        "    res = default_model_dct\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO:\n",
        "    #########################################################################\n",
        "    return res\n",
        "\n",
        "def mystery_model_1():\n",
        "    res = default_model_dct\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO:\n",
        "    #########################################################################\n",
        "    return res\n",
        "\n",
        "\n",
        "def mystery_model_2():\n",
        "    res = default_model_dct\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO:\n",
        "    #########################################################################\n",
        "    return res\n",
        "\n",
        "\n",
        "def mystery_model_3():\n",
        "    res = default_model_dct\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO:\n",
        "    #########################################################################\n",
        "    return res\n",
        "\n",
        "\n",
        "def mystery_model_4():\n",
        "    res = default_model_dct\n",
        "    ########################### YOUR CODE ###################################\n",
        "    ## TODO:\n",
        "    #########################################################################\n",
        "    return res\n",
        "\n"
      ],
      "metadata": {
        "id": "LxZTtti6WCuL"
      },
      "id": "LxZTtti6WCuL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6c4bd2",
      "metadata": {
        "id": "dd6c4bd2"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    type_check(myster_model_0())\n",
        "    type_check(myster_model_1())\n",
        "    type_check(myster_model_2())\n",
        "    type_check(myster_model_3())\n",
        "    type_check(myster_model_4())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0cb91a",
      "metadata": {
        "id": "1d0cb91a"
      },
      "source": [
        "# Collaboration / External Help\n",
        "Disclose any help you used (LLM usage, blogs, search, Github links, etc) and collaborations with your classmates. If you  completed the homework on your own, you can leave this part empty.\n",
        "\n",
        "> TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prep-ta-fall-2025 (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}