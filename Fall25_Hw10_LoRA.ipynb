{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfraAd/CSC413-Homeworks/blob/main/Fall25_Hw10_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc910e76",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bc910e76"
      },
      "source": [
        "# Homework 10\n",
        "\n",
        "Pre-trained large language models (LLMs) compress knowledge from vast amounts of text, but in practice they often need additional fine-tuning to perform well on specific downstream tasks. Fine-tuning can be done by updating all of a model’s parameters (full-model fine-tuning) or by modifying only a small subset, a strategy known as parameter-efficient fine-tuning (PEFT), which is typically preferred for its computational and memory savings.\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is a popular option for PEFT, because it introduces small trainable matrices (A and B) that adjust a model’s generation without altering most existing parameters. In this homework, we will fine-tune a pretrained LLM, HuggingFace’s SmolLM2, on a downstream dataset using both full-model training and LoRA. After evaluating the full-model version in Part 1, we will implement our own LoRA-based fine-tuning in Part 2. For more background on LoRA, you may find it helpful to review the original paper: https://arxiv.org/pdf/2106.09685\n",
        "\n",
        "\n",
        "**!NOTE!**\n",
        "* Please run any training related code under the `__name__ == '__main__'` block. This prevents any time-out when running your solution on Markus. Basically, any code that is not a python function should go under the `__name__ == '__main__'` block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "33809d55",
      "metadata": {
        "id": "33809d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09dccc7c-d987-4adb-d18c-9c8de5587006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    !pip install datasets # comment out when submitting\n",
        "    !pip install evaluate # comment out when submitting\n",
        "\n",
        "    import os\n",
        "    import torch\n",
        "    import random\n",
        "    import evaluate\n",
        "    import requests\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import torch.nn as nn\n",
        "    from io import BytesIO\n",
        "    from typing import List\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.cm as cm\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "    from transformers import get_scheduler, get_linear_schedule_with_warmup\n",
        "    from datasets import load_dataset\n",
        "    from torch.optim import AdamW\n",
        "    from torch.utils.data import DataLoader\n",
        "    from huggingface_hub import HfApi, hf_hub_download, hf_hub_url, login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    def in_colab():\n",
        "        try:\n",
        "            import google.colab\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "\n",
        "    if in_colab():\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        MODEL_SAVE_PATH = '/content/drive/MyDrive/CSC413'\n",
        "    else:\n",
        "        MODEL_SAVE_PATH = '.'"
      ],
      "metadata": {
        "id": "4kOVE8aJNnGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f167062c-26f5-4783-a710-655281babac2"
      },
      "id": "4kOVE8aJNnGq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up\n",
        "\n",
        "HuggingFace’s **SmolLM2-135M** is a pretrained language model with 135 million parameters, trained using a next-token prediction objective in which the model learns to generate the next tokens in a sequence given some input text.  However, as you will soon observe, the base model is not yet capable of producing clean, well-structured answers in response to user prompts.\n",
        "\n",
        "To adapt the model to engage in multi-turn dialogue and better follow instructions, we will fine-tune it on a subset of **SmolTalk**, a synthetic dataset created specifically to improve the model's instruction following ability (fine-tuning an LLM on such dataset is often referred to as \"instruction-tuning\" in practice). SmolTalk is the same dataset used to produce SmolLM2’s Instruct variants, and our fine-tuning will demonstrate how instruction tuning improves conversational and response generation ability.\n",
        "\n",
        "Finally, you will **load your fine-tuned models to HuggingFace**. To do this, you should [sign up](https://huggingface.co/join) if you don't have an account already, and [set up an access token](https://huggingface.co/docs/hub/en/security-tokens). While this is not the only approach, one way you can easily push your trained model on Colab:\n",
        "\n",
        "```\n",
        "if __name__ == '__main__':\n",
        "    !huggingface-cli login # paste your token when prompted\n",
        "    your_finetuned_model.push_to_hub('your_username/model_repo_name_you_choose')\n",
        "    tokenizer.push_to_hub('your_username/model_repo_name_you_choose')\n",
        "```"
      ],
      "metadata": {
        "id": "eURzIfjIMSQf"
      },
      "id": "eURzIfjIMSQf"
    },
    {
      "cell_type": "code",
      "source": [
        "######################## DO NOT MODIFY ########################\n",
        "# Download the model and tokenizer\n",
        "if __name__ == '__main__':\n",
        "    checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "    tokenizer.pad_token = '<|pad_token|>'\n",
        "    print(tokenizer.eos_token)\n",
        "    print(tokenizer.bos_token)\n",
        "    print(tokenizer.pad_token)\n",
        "\n",
        "    model_full = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
        "    print(model_full)\n",
        "\n",
        "    inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
        "    outputs = model_full.generate(inputs)\n",
        "    print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "IKpjGWBPMKNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aec5591-49f2-4b17-a3d6-67a835876bbb"
      },
      "id": "IKpjGWBPMKNs",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n",
            "<|endoftext|>\n",
            "<|pad_token|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n",
            "Gravity is the force that holds the Earth and the Moon together.\n",
            "\n",
            "The Moon is a satellite of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## DO NOT MODIFY ########################\n",
        "# Download the dataset\n",
        "if __name__ == '__main__':\n",
        "    ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")\n",
        "    print(ds['train'][0])"
      ],
      "metadata": {
        "id": "wNB3tSIIMKaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bcbca5-a80c-4304-8fcf-bdd8bfa0a94f"
      },
      "id": "wNB3tSIIMKaf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'full_topic': 'Travel/Vacation destinations/Beach resorts', 'messages': [{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\", 'role': 'user'}, {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\", 'role': 'assistant'}, {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?', 'role': 'user'}, {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.', 'role': 'assistant'}, {'content': \"Okay, I'll look into those. Thanks for the recommendations!\", 'role': 'user'}, {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\", 'role': 'assistant'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ paste your HuggingFace access token here! ############\n",
        "if __name__ == '__main__':\n",
        "    login()"
      ],
      "metadata": {
        "id": "iT9TkJxwoRFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "79da272be4b24b53aa4684e68914ec75",
            "4cae1edc7e794d93ba8d305d7ec435b7",
            "b2b1ebc3e13c4669b5f30b4f6c070f49",
            "092b5a8ab5c84c17b6decdc73af383a8",
            "b3120560dc0447878f4a05ceb3598447",
            "cc43408087cd4844a23e4df9db981fdf",
            "00fb96b5e1374f538bd3883ff808f49a",
            "71df4f876d114b07a81529c42cbe5993",
            "44e5c63edd9e493d9eb01f8e2e400e10",
            "14b7cf88daad4a1ab076d9dbec4b090f",
            "fe597c68af6246108571bdc2184cc05f",
            "5088b004c57547bab5b27a932d7d7c84",
            "c1f301bfae0947d0af231f6e8829a6d4",
            "89514b343b3f46f89a59e7c65a291e3c",
            "53ec289204d340818009d262b0d3674e",
            "a9668eb39b974a28b72d6e550023a4fa",
            "f5eabf8eccc9458382d3e8456bd7f04d",
            "8cc7cecf78d944aea36da6737f3d8bf8",
            "29bd096e5c8044c2a1d67d979b4eefac",
            "d57ef60d32534f16a47a1f4ba1aa9a2b"
          ]
        },
        "outputId": "a00b00ec-66df-4c62-9f2d-64b69f3ec89b"
      },
      "id": "iT9TkJxwoRFg",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79da272be4b24b53aa4684e68914ec75"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Full-model fine-tuning on instruction dataset (2 point)\n",
        "\n",
        "Before we implement the LoRA fine-tuning workflow, we will first establish a baseline using **full-model fine-tuning**. During instruction fine-tuning, it is common to apply a template to the dataset to format inputs and outputs into a consistent conversational structure that the model can learn from. This templated dataset is then used to fine-tune the LLM.\n",
        "\n",
        "The base-model is trained to predict the next token and may not inherently understand what a “user message” or an “assistant response” is. Templates impose a conversation structure to the text, from whcih the model is trained to predict when the assistant's turn is and what style of output to produce. For instance:\n",
        "\n",
        "```\n",
        "<|user|>: What is 1+1?\n",
        "<|assistant|>: It is 2.\n",
        "```\n",
        "\n",
        "In this section, you will write your own functions required to fine-tune the model (e.g. processing the original dataset, creating the training loop or using a Trainer), similar to what you have implemented in previous assignments.\n",
        "\n",
        "We will fine-tune the model on the *everydata-conversation* split of the SmolTalk dataset, which contains multi-turn conversations between a user and an assistant across a wide range of topics (sports, fashion, health, entertainment, and more). After fine-tuning, you will see how even a simple instruction-tuning approach can significantly improve generation quality."
      ],
      "metadata": {
        "id": "TFRpu7p6D9pg"
      },
      "id": "TFRpu7p6D9pg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.a Convert the dataset to chat template (0.5 point)\n",
        "\n",
        "**TODO**\n",
        "* Complete the apply_my_chat_template() function to convert a raw text data into a chat-templated format. You can define your own chat template, or look up any common chat templates used in practice.\n",
        "\n",
        "Given a list of user-assistant messages, your function should convert it into a single formatted string. For instance:\n",
        "```\n",
        "user_input = [{'content': 'how are you?', 'role': 'user'},\n",
        "              {'content': 'I'm good, thanks! And you?', 'role': 'assistant'}\n",
        "]\n",
        "templated_output = apply_my_chat_template(user_input)\n",
        "print(templated_output)\n",
        "-----------------------------------------\n",
        "<|user|> How are you?\n",
        "<|assistant|> I'm good, thanks! And you?\n",
        "-----------------------------------------\n",
        "```"
      ],
      "metadata": {
        "id": "jcEIdyNqJxKv"
      },
      "id": "jcEIdyNqJxKv"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_my_chat_template(messages: List) -> str:\n",
        "\n",
        "    formatted = \"\"\n",
        "    ################ YOUR CODE ################\n",
        "    # TODO: given a list user-assistant interactions, create a formatted string\n",
        "    #       with \"user\" and \"assistant\" headers\n",
        "    for message in messages:\n",
        "        role = message['role']\n",
        "        content = message['content']\n",
        "        formatted += f\"<|{role}|> {content}\\n\"\n",
        "\n",
        "    return formatted.strip()\n",
        "    ###########################################\n"
      ],
      "metadata": {
        "id": "590nsG1UuS_1"
      },
      "id": "590nsG1UuS_1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ############ TESTING THE TEMPLATE CODE ############\n",
        "    formatted_input = apply_my_chat_template([{'content': 'hi there!', 'role': 'user'},\n",
        "                                              {'content': 'hi, how is it going?', 'role':'assistant'}])\n",
        "    print(formatted_input)"
      ],
      "metadata": {
        "id": "ez-r6ysBd3m7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bbec1f-6748-4300-f328-45aa2bc7fe0a"
      },
      "id": "ez-r6ysBd3m7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|> hi there!\n",
            "<|assistant|> hi, how is it going?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.b Fine-tuning the model (1.5 point)\n",
        "\n",
        "Unlike LLMs with a classification head, which output one of a fixed set of labels (such as the model used in Homework 8), SmolLM2 generates open-ended text responses. There are multiple ways to evaluate the quality of such generative models. In this assignment we will use the validation loss after fine-tuning as the quantitative measure of model quality. In addition, you will design a set of custom prompts to qualitatively compare the model’s responses before and after fine-tuning.\n",
        "\n",
        "Your fine-tuned model will be evaluated using the check_validation_loss() function. The function takes your fine-tuned model, apply_my_template() function, and the original dataset.\n",
        "\n",
        "**TODO**\n",
        "* Define your **custom prompts (10 total)** used evaluate the model generation quality.\n",
        "* Complete the **count_trainable_parameter()** function to count the number of tunable parameters.\n",
        "* Set up your own **training function** to run training. Achieve validation loss < 0.5"
      ],
      "metadata": {
        "id": "dH-WsZz_Zqnl"
      },
      "id": "dH-WsZz_Zqnl"
    },
    {
      "cell_type": "code",
      "source": [
        "##################### DO NOT MODIFY THE CODE! #####################\n",
        "def check_validation_loss(model, tokenizer, my_template_function, dataset):\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenized_dataset = dataset.map(lambda x: tokenizer(my_template_function(x['messages'])))\n",
        "    tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x['input_ids']})\n",
        "\n",
        "    tokenized_dataset = tokenized_dataset.select_columns(['input_ids','labels','attention_mask'])\n",
        "    tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    valid_dataloader = DataLoader(tokenized_dataset['test'],\n",
        "                                  batch_size=1)\n",
        "\n",
        "    model.to(device)\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            out = model(**{k:v.to(device) for k, v in batch.items()})\n",
        "            batch_loss.append(out.loss)\n",
        "\n",
        "    avg_loss = sum(batch_loss) / len(batch_loss)\n",
        "\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "suh8qaSrewNs"
      },
      "id": "suh8qaSrewNs",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    ########################### YOUR CODE ###########################\n",
        "    # TODO: count the total number of tunable parameters\n",
        "    # HINT: for the default SmolLM2-135M, the function returns around ~135M.\n",
        "    #       for LoRA training, this value is much smaller.\n",
        "    total_params = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            total_params += param.numel()\n",
        "    return total_params\n",
        "    #############################################################"
      ],
      "metadata": {
        "id": "rosqISLFL9bc"
      },
      "id": "rosqISLFL9bc",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################### YOUR PROMPTS ###########################\n",
        "# TODO: define 10 user prompts to assess the model quality\n",
        "YOUR_PROMPTS = [\n",
        "                [{'role':'user', 'content': 'Why is the sky blue?'}],\n",
        "                [{'role':'user', 'content': 'What is the capital of France?'}],\n",
        "                [{'role':'user', 'content': 'Explain photosynthesis in simple terms.'}],\n",
        "                [{'role':'user', 'content': 'How do airplanes fly?'}],\n",
        "                [{'role':'user', 'content': 'What causes seasons on Earth?'}],\n",
        "                [{'role':'user', 'content': 'Tell me about the water cycle.'}],\n",
        "                [{'role':'user', 'content': 'What is artificial intelligence?'}],\n",
        "                [{'role':'user', 'content': 'How does a microwave oven work?'}],\n",
        "                [{'role':'user', 'content': 'What are the main causes of climate change?'}],\n",
        "                [{'role':'user', 'content': 'Describe the process of evolution.'}],\n",
        "               ]"
      ],
      "metadata": {
        "id": "GuMRGRsFjob2"
      },
      "id": "GuMRGRsFjob2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, write any training code necessary to run full model fine-tuning of SmolLM2 on the instruction dataset. Make sure to put your code block under `if __name__ == '__main__': ` to avoid having the code running during the autograding."
      ],
      "metadata": {
        "id": "G9pjMKt7kVXh"
      },
      "id": "G9pjMKt7kVXh"
    },
    {
      "cell_type": "code",
      "source": [
        "################### CHECK THE VALIDATION LOSS ###################\n",
        "if __name__ == '__main__':\n",
        "    # Fine tune the model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenized_train = ds.map(lambda x: tokenizer(apply_my_chat_template(x['messages'])))\n",
        "    tokenized_train = tokenized_train.map(lambda x: {\"labels\": x['input_ids']})\n",
        "\n",
        "    tokenized_train = tokenized_train.select_columns(['input_ids','labels','attention_mask'])\n",
        "    tokenized_train.set_format(\"torch\")\n",
        "\n",
        "    train_dataloader = DataLoader(tokenized_train['train'], batch_size=32)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        [p for p in model_full.parameters() if p.requires_grad],\n",
        "        lr=5e-5\n",
        "    )\n",
        "\n",
        "    num_epochs = 10\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * num_training_steps),\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model_full.train()\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model_full(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      # After each epoch, check validation loss\n",
        "      val_loss = check_validation_loss(model_full, tokenizer, apply_my_chat_template, ds)\n",
        "      print(f\"Epoch {epoch+1}: validation loss = {val_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    full_valid_loss = check_validation_loss(model_full, tokenizer, apply_my_chat_template, ds)\n",
        "    print(full_valid_loss)"
      ],
      "metadata": {
        "id": "aKOch3G6kSY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6f6a4e27-6d87-434a-ce2c-7d0e0c3b803c"
      },
      "id": "aKOch3G6kSY9",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 192 at dim 1 (got 229)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-764810047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mmodel_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"np\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m   1034\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3455\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3459\u001b[0m     def create_token_type_ids_from_sequences(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    809\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     ) from e\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    812\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################### CHECK MODEL GENERATION QUALITY BEFORE/AFTER FINE-TUNING ####################\n",
        "if __name__ == '__main__':\n",
        "    for idx, p in enumerate(YOUR_PROMPTS):\n",
        "        templated_ipnut = apply_my_chat_template(p) + '\\n<|im_start|><|assistant|>: '\n",
        "        tokenized_input = tokenizer(templated_ipnut, return_tensors='pt')\n",
        "        out = model_full.generate(**{k:v.to(device) for k, v in tokenized_input.items()}, max_new_tokens=50)\n",
        "\n",
        "        print(f\"{idx}\")\n",
        "        print(\">>> Raw input text: \", p)\n",
        "        print(\">>> Templated output: \", templated_ipnut)\n",
        "        print(\">>> Model full output: \\n\" , tokenizer.decode(out[0]))\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "bzqFIh7VkOaM"
      },
      "id": "bzqFIh7VkOaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############### TO SUBMIT ###############\n",
        "YOUR_HF_USER_NAME = \"afra7778\"\n",
        "YOUR_FULL_MODEL_REPO_NAME = \"smollm2-finetuned-chat-instruct-full\"\n",
        "#########################################"
      ],
      "metadata": {
        "id": "H7kCjE6LEhCY"
      },
      "id": "H7kCjE6LEhCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### SAVE THE MODEL WEIGHTS #########\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # first, save locally\n",
        "    model_full.save_pretrained(f\"{MODEL_SAVE_PATH}/{YOUR_FULL_MODEL_REPO_NAME}\")\n",
        "    tokenizer.save_pretrained(f\"{MODEL_SAVE_PATH}/{YOUR_FULL_MODEL_REPO_NAME}\")\n",
        "\n",
        "    # then upload to huggingface\n",
        "    api = HfApi()\n",
        "    api.create_repo(YOUR_FULL_MODEL_REPO_NAME, exist_ok=True, repo_type=\"model\")\n",
        "    api.upload_folder(\n",
        "        folder_path=f\"{MODEL_SAVE_PATH}/{YOUR_FULL_MODEL_REPO_NAME}\",\n",
        "        repo_id=f\"{YOUR_HF_USER_NAME}/{YOUR_FULL_MODEL_REPO_NAME}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "JcZYBly0y_H9"
      },
      "id": "JcZYBly0y_H9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Motivating low-rank adaptation (LoRA) training (0.5 point)\n",
        "\n",
        "When we fine-tune a pretrained model, we are effectively learning an update to the model’s weight matrices:\n",
        "$$W' = W + \\Delta W$$\n",
        "where $W$ is the pretrained weights (frozen) and $\\Delta W $ is the learned task-specific update.\n",
        "\n",
        "Instead of learning a full matrix $\\Delta W \\in \\mathbb{R}^{d \\times k}$,\n",
        "LoRA assumes that this update has low intrinsic rank.\n",
        "That is, we can approximate:\n",
        "\n",
        "$$\\Delta W = B A$$\n",
        "\n",
        "where $A \\in \\mathbb{R}^{r \\times k}$, $B \\in \\mathbb{R}^{d \\times r}$, and $r \\ll \\min(d, k)$ (the *rank* of the update).\n",
        "The model’s forward pass becomes:\n",
        "   $\n",
        "   y = (W + s \\cdot B A) x\n",
        "   $,\n",
        "where $s$ is a scaling factor ($\\alpha$ / r) that balances the LoRA update magnitude.\n",
        "\n",
        "In this section, you will check if this assumption indeed holds by analyzing the $\\Delta W$ between the base model and its fine-tuned version.\n",
        "\n",
        "**TODO**\n",
        "* Implement the reconstruction_err() function, which computes the reconstruction error of a matrix when it is approximated using its top-rank components. This helps us analyze whether the LoRA's low-rank assumption always holds. If it does not, what does this implies for the performance of LoRA fine-tuning?"
      ],
      "metadata": {
        "id": "SMorb9506Q56"
      },
      "id": "SMorb9506Q56"
    },
    {
      "cell_type": "code",
      "source": [
        "################## DO NOT CHANGE ##################\n",
        "def plot_reconstruction_error(df):\n",
        "    ncols=3\n",
        "    nrows=2\n",
        "    f, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(10,6))\n",
        "\n",
        "    num_lines = 30\n",
        "    cmap = cm.get_cmap(\"viridis\")\n",
        "\n",
        "    for idx, target_module in enumerate(['k_proj','q_proj','o_proj','v_proj','up_proj','gate_proj']):\n",
        "        for i, col in enumerate([c for c in df.columns if target_module in c]):\n",
        "            color = cmap(i / (num_lines - 1))\n",
        "            axes[idx//3][idx%3].plot(df['rank'], df[col], color=color, label=col, marker='.')\n",
        "        axes[idx//3][idx%3].set_title(target_module)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "37b6qCFf6PUY"
      },
      "id": "37b6qCFf6PUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruction_err(W, r):\n",
        "\n",
        "    reconstruction_error = None\n",
        "\n",
        "    ############### YOUR CODE ###############\n",
        "    # TODO:\n",
        "    # 1. Perform SVD on the matrix W and compute its best rank-r approximation\n",
        "    #    by reconstructing W' using the top-r singular values and corresponding vectors.\n",
        "    # 2. Return the reconstruction error as the Frobenius norm between W and W', ||W - W'||_F\n",
        "    # NOTE: 1. Use torch instead of numpy to perform svd\n",
        "    #       2. When given r > rank of the matrix, use r=matrix rank to approximate W' !\n",
        "\n",
        "    # Perform SVD\n",
        "    U, S, Vt = torch.linalg.svd(W, full_matrices=False)\n",
        "\n",
        "    # Ensure r doesn't exceed the actual rank\n",
        "    actual_rank = min(r, S.shape[0])\n",
        "\n",
        "    # Reconstruct using top-r singular values\n",
        "    W_approx = U[:, :actual_rank] @ torch.diag(S[:actual_rank]) @ Vt[:actual_rank, :]\n",
        "\n",
        "    # Compute Frobenius norm of the difference\n",
        "    reconstruction_error = torch.linalg.norm(W - W_approx, ord='fro')\n",
        "    #########################################\n",
        "\n",
        "    return reconstruction_error"
      ],
      "metadata": {
        "id": "_9hFJpOW6TAa"
      },
      "id": "_9hFJpOW6TAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to load the base model and the model you have just fine-tuned to compute the $\\Delta W$ between the two models. Then, you will analyze whether $\\Delta W$ indeed has low intrinsic dimensionality by\n",
        "\n",
        "1. computing the reconstruction error of $\\Delta W$ when it is approximated using its top-r components, and\n",
        "2. plotting how the error converges towards 0 as r increases.\n",
        "\n",
        "Does the plot imply that some modules / layers have higher rank than others?"
      ],
      "metadata": {
        "id": "TOF9JhVG6Xju"
      },
      "id": "TOF9JhVG6Xju"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # load the base model and the model we trained!\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model1 = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "    model2 = AutoModelForCausalLM.from_pretrained(f\"{YOUR_HF_USER_NAME}/{YOUR_FULL_MODEL_REPO_NAME}\")"
      ],
      "metadata": {
        "id": "foRNC9mp6W-5"
      },
      "id": "foRNC9mp6W-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ VISUALIZATION CODE (could take a few minutes to run) ################\n",
        "if __name__ == '__main__':\n",
        "    # To visualize the reconstruction error\n",
        "    d = {}\n",
        "    for name, p in model1.named_parameters():\n",
        "        if 'self_attn' in name or 'mlp' in name:\n",
        "            w1 = model1.state_dict()[name]\n",
        "            w2 = model2.state_dict()[name]\n",
        "            d[name] = {}\n",
        "            for r in [1, 8, 32, 64, 128, 256, 576]:\n",
        "                w_delta = w1 - w2\n",
        "                d[name][r] = reconstruction_err(w_delta, r)\n",
        "\n",
        "    # create a dataframe\n",
        "    df = pd.DataFrame(d).reset_index().rename(columns={'index':'rank'})\n",
        "\n",
        "    # plot; earlier layers (close to 0) have darker colors\n",
        "    plot_reconstruction_error(df)"
      ],
      "metadata": {
        "id": "wuwomUxV6XEL"
      },
      "id": "wuwomUxV6XEL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: LoRA fine-tuning (4 points)\n",
        "\n",
        "\n",
        "In this section, you will implement parameter-efficient training using LoRA. You will define a custom LoRA layer and attach it to each target module in the model. During training, only the LoRA parameters will be updated while the original model weights remain frozen. The choice of rank (*r*) plays an important role in both performance and efficiency.\n",
        "\n",
        "How does the number of trainable parameters change as we vary the rank? Is the model performance affected? To explore this trade-off, you can perform a hyperparameter sweep over different values of *r* to determine which setting offers the best balance between performance and efficiency."
      ],
      "metadata": {
        "id": "ji55Zan46dTI"
      },
      "id": "ji55Zan46dTI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.a Create a LoRA model by injecting a custom LoRA layer (1 point)\n",
        "\n",
        "\n",
        "When we fine-tune a pretrained model, we are effectively learning an update to the model’s weight matrices:\n",
        "$$W' = W + \\Delta W$$\n",
        "where $W$ is the pretrained weights (frozen) and $\\Delta W $ is the learned task-specific update.\n",
        "\n",
        "Instead of learning a full matrix $\\Delta W \\in \\mathbb{R}^{d \\times k}$,\n",
        "LoRA assumes that this update has low intrinsic rank.\n",
        "That is, we can approximate:\n",
        "\n",
        "$$\\Delta W = B A$$\n",
        "\n",
        "where $A \\in \\mathbb{R}^{r \\times k}$, $B \\in \\mathbb{R}^{d \\times r}$, and $r \\ll \\min(d, k)$ (the *rank* of the update).\n",
        "The model’s forward pass becomes:\n",
        "   $\n",
        "   y = (W + s \\cdot B A) x\n",
        "   $,\n",
        "where $s$ is a scaling factor ($\\alpha$ / r) that balances the LoRA update magnitude.\n",
        "\n",
        "\n",
        "In this section, define a custom linear layer for LoRA. It should 1. initialize the A and B matrices, and 2. implement a forward pass that applies the LoRA update to the original layer output.\n",
        "\n",
        "\n",
        "**TODO**\n",
        "* Complete the LoRALinear class, which creates low-rank A and B matrices and  added the LoRA weights applied output to the original linear layer’s output."
      ],
      "metadata": {
        "id": "CwvZ4xgmHOn8"
      },
      "id": "CwvZ4xgmHOn8"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "    model_lora  = AutoModelForCausalLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "BHOuBuKUHNIv"
      },
      "id": "BHOuBuKUHNIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 base_layer: nn.Linear,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 r: int, alpha: int):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scaling = self.alpha / self.r\n",
        "\n",
        "        ################# YOUR CODE #################\n",
        "        # TODO: LoRA parameters\n",
        "        #       randomly initialize A matrix\n",
        "        #       then initialize B matrix as 0 matrix\n",
        "        self.A = nn.Parameter(torch.randn(r, in_features))\n",
        "        self.B = nn.Parameter(torch.zeros(out_features, r))\n",
        "        #############################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ################# YOUR CODE #################\n",
        "        # NOTE: LoRALinear(x) shape should be the same as\n",
        "        #       base_layer(x) shape\n",
        "        base_output = self.base_layer(x)\n",
        "\n",
        "        # Compute LoRA adaptation: x @ A^T @ B^T scaled by alpha/r\n",
        "        lora_output = (x @ self.A.T @ self.B.T) * self.scaling\n",
        "\n",
        "        return base_output + lora_output\n",
        "        #############################################"
      ],
      "metadata": {
        "id": "AIKx-9cLHnpr"
      },
      "id": "AIKx-9cLHnpr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.b Inject the custom layer into the base model (1 point)\n",
        "\n",
        "Inject the custom LoRA layer into the base model by iterating over its modules and replacing each target linear layer with a LoRALinear module.\n",
        "\n",
        "**TODO**\n",
        "* Complete **replace_target_with_lora_layer()** to  locate each target module and replace it with a LoRALinear instance containing the LoRA A and B matrices."
      ],
      "metadata": {
        "id": "-KvVJ415gRkz"
      },
      "id": "-KvVJ415gRkz"
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_target_with_lora_layer(model,\n",
        "                                   r: int,\n",
        "                                   alpha: int,\n",
        "                                   target_modules=['q_proj','k_proj','v_proj','o_proj']) -> None:\n",
        "\n",
        "    for name, m in model.named_modules():\n",
        "        layer_name = name.split('.')[-1]\n",
        "        if layer_name in target_modules:\n",
        "            ####################### YOUR CODE #######################\n",
        "            # TODO: iterate through the model modules to find the target modules\n",
        "            #       then replace it with the LoRALinear\n",
        "            # HINT: Once you find the target module and its parent module,\n",
        "            #       you can replace target module using setattr(parent_module, target_module_name, your_LoRALinear)\n",
        "            # Get parent module\n",
        "            parent_name = '.'.join(name.split('.')[:-1])\n",
        "            if parent_name:\n",
        "                parent = dict(model.named_modules())[parent_name]\n",
        "            else:\n",
        "                parent = model\n",
        "\n",
        "            # Get the original layer\n",
        "            original_layer = m\n",
        "\n",
        "            # Create LoRA layer\n",
        "            lora_layer = LoRALinear(\n",
        "                base_layer=original_layer,\n",
        "                in_features=original_layer.in_features,\n",
        "                out_features=original_layer.out_features,\n",
        "                r=r,\n",
        "                alpha=alpha\n",
        "            )\n",
        "\n",
        "            # Replace the layer\n",
        "            setattr(parent, layer_name, lora_layer)\n",
        "            ##########################################################"
      ],
      "metadata": {
        "id": "weVz4hHxHoEK"
      },
      "id": "weVz4hHxHoEK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After injecting the LoRALinear layer to the target modules, the following for loop should run without an error:\n",
        "\n",
        "```\n",
        "replace_target_with_lora_layer(model_lora,\n",
        "                                r=RANK,\n",
        "                                alpha=ALPHA,\n",
        "                                target_modules=TARGET_MODULES)\n",
        "\n",
        "\n",
        "for name, m in model_lora.named_modules():\n",
        "    layer_name = name.split('.')[-1]\n",
        "    if layer_name in TARGET_MODULES:\n",
        "        if not isinstance(dict(model_lora.named_modules())[name], LoRALinear):\n",
        "          print(\"Incorrect!\")\n",
        "          break\n",
        "```"
      ],
      "metadata": {
        "id": "_6PeH1DGvvCS"
      },
      "id": "_6PeH1DGvvCS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.c Freeze the base model weights for parameter-efficient fine-tuning (0.5 point)\n",
        "\n",
        "Finally, ensure that only the LoRA A and B matrices remain trainable by freezing all other model parameters.\n",
        "\n",
        "**TODO**\n",
        "* Complete **set_requires_grad_for_lora()** so that only the LoRA parameters require gradients, while all base model weights are frozen.\n"
      ],
      "metadata": {
        "id": "Hs6Wk5jxgYLe"
      },
      "id": "Hs6Wk5jxgYLe"
    },
    {
      "cell_type": "code",
      "source": [
        "def set_requires_grad_for_lora(model):\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        ####################### YOUR CODE #######################\n",
        "        # TODO: Freeze the base model weight, and only set requires_grad=True\n",
        "        #       for A and B matrices\n",
        "        if '.A' in name or '.B' in name:\n",
        "            p.requires_grad = True\n",
        "        else:\n",
        "            p.requires_grad = False\n",
        "        ##########################################################"
      ],
      "metadata": {
        "id": "fDICOagSIEmw"
      },
      "id": "fDICOagSIEmw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.d Train LoRA model (1.5 point)\n",
        "\n",
        "Fine-tune the LoRA model on the dataset. Do the same learning rate and number of epochs used for full-model fine-tuning still work here? If not, consider varying the learning rates and LoRA ranks to find the best configuration.\n",
        "\n",
        "**TODO**\n",
        "* **Fine-tune the LoRA model** on the everyday-chat dataset. Achieve validation loss < 0.5.\n",
        "* Compare the LoRA-fine-tuned model with the full fine-tuned model. Which one achieves lower validation loss? Which produces better generations?\n",
        "* After fine-tuning, upload the adapters to HuggingFace."
      ],
      "metadata": {
        "id": "tnkhQKLAIcgJ"
      },
      "id": "tnkhQKLAIcgJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Put your final hyperparameter values here (TO SUBMIT)\n",
        "LORA_HYPERPARM = {\n",
        "    'rank': 8,\n",
        "    'alpha': 16,\n",
        "    'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        "}"
      ],
      "metadata": {
        "id": "AuXiD4ps7-9Q"
      },
      "id": "AuXiD4ps7-9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ TO PREPARE FOR LORA TRAINING ############\n",
        "if __name__ == '__main__':\n",
        "    print(\"Before setting the LoRA: \", count_trainable_parameters(model_lora))\n",
        "    replace_target_with_lora_layer(model_lora,\n",
        "                                   r=LORA_HYPERPARM['rank'],\n",
        "                                   alpha=LORA_HYPERPARM['alpha'],\n",
        "                                   target_modules=LORA_HYPERPARM['target_modules'])\n",
        "    set_requires_grad_for_lora(model_lora)\n",
        "\n",
        "    print(\"After setting the LoRA: \", count_trainable_parameters(model_lora))\n",
        "    model_lora.to(device)\n",
        "\n",
        "    tokenized_train = ds.map(lambda x: tokenizer(apply_my_chat_template(x['messages'])))\n",
        "    tokenized_train = tokenized_train.map(lambda x: {\"labels\": x['input_ids']})\n",
        "\n",
        "    tokenized_train = tokenized_train.select_columns(['input_ids','labels','attention_mask'])\n",
        "    tokenized_train.set_format(\"torch\")\n",
        "\n",
        "    train_dataloader = DataLoader(tokenized_train['train'], batch_size=32)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "      [p for p in model_lora.parameters() if p.requires_grad],\n",
        "      lr=5e-5\n",
        "    )\n",
        "\n",
        "    num_epochs = 10\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=int(0.1 * num_training_steps),\n",
        "      num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model_lora.train()\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model_lora(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      # After each epoch, check validation loss\n",
        "      val_loss = check_validation_loss(model_lora, tokenizer, apply_my_chat_template, ds)\n",
        "      print(f\"Epoch {epoch+1}: validation loss = {val_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    full_valid_loss = check_validation_loss(model_full, tokenizer, apply_my_chat_template, ds)\n",
        "    print(full_valid_loss)"
      ],
      "metadata": {
        "id": "i3mLlE3A64IP"
      },
      "id": "i3mLlE3A64IP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################### CHECK THE MODEL GENERATION BEFOER/AFTER FINE-TUNING ####################\n",
        "if __name__ == '__main__':\n",
        "    final_val_loss = check_validation_loss(model_lora, tokenizer, apply_my_chat_template, ds)\n",
        "    print(\"Validation loss: \", final_val_loss)\n",
        "\n",
        "    for idx, p in enumerate(YOUR_PROMPTS):\n",
        "        templated_ipnut = apply_my_chat_template(p) + '\\n<|im_start|><|assistant|>: '\n",
        "        tokenized_input = tokenizer(templated_ipnut, return_tensors='pt')\n",
        "        out = model_lora.generate(**{k:v.to(device) for k, v in tokenized_input.items()},\n",
        "                                  max_new_tokens=50,\n",
        "                                  eos_token_id=tokenizer.eos_token_id)\n",
        "        print(f\"{idx}\")\n",
        "        print(\">>> Raw input text: \", p)\n",
        "        print(\">>> Templated output: \", templated_ipnut)\n",
        "        print(\">>> Model full output: \\n\" , tokenizer.decode(out[0]))\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "J8MVA8qcYtW3"
      },
      "id": "J8MVA8qcYtW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############### TO SUBMIT ###############\n",
        "YOUR_LORA_ADAPTER_REPO_NAME = \"smollm2-finetuned-chat-instruct-lora-adapters\"\n",
        "#########################################"
      ],
      "metadata": {
        "id": "NAFc4lLWEpuI"
      },
      "id": "NAFc4lLWEpuI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, write any training code necessary to run the model fine-tuning. Make sure to put your code block under `if __name__ == '__main__': ` to avoid having the code running during the autograding."
      ],
      "metadata": {
        "id": "TBMGUCv87AY6"
      },
      "id": "TBMGUCv87AY6"
    },
    {
      "cell_type": "code",
      "source": [
        "######### SAVE THE LORA ADAPTER WEIGHTS ONLY AND UPLOAD TO HUB #########\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # first, save the adapter (trained A,B matrices) and the tokenizer locally\n",
        "    adapters = {k: v.cpu() for k, v in model_lora.state_dict().items() if \".A\" in k or \".B\" in k}\n",
        "    os.makedirs(f\"{MODEL_SAVE_PATH}/{YOUR_LORA_ADAPTER_REPO_NAME}\", exist_ok=True)\n",
        "    torch.save(adapters, f\"{MODEL_SAVE_PATH}/{YOUR_LORA_ADAPTER_REPO_NAME}/lora_adapters.pt\")\n",
        "    tokenizer.save_pretrained(f\"{MODEL_SAVE_PATH}/{YOUR_LORA_ADAPTER_REPO_NAME}\")\n",
        "\n",
        "    # then push the folder to hub\n",
        "    api = HfApi()\n",
        "    api.create_repo(YOUR_LORA_ADAPTER_REPO_NAME, exist_ok=True, repo_type='model')\n",
        "    api.upload_folder(\n",
        "        folder_path = f'{MODEL_SAVE_PATH}/{YOUR_LORA_ADAPTER_REPO_NAME}',\n",
        "        repo_id=f'{YOUR_HF_USER_NAME}/{YOUR_LORA_ADAPTER_REPO_NAME}'\n",
        "    )"
      ],
      "metadata": {
        "id": "JyHnE53dZ1fm"
      },
      "id": "JyHnE53dZ1fm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load your trained adapter to the base model, we can reconstruct the model by:"
      ],
      "metadata": {
        "id": "fJFNP4UFFGAE"
      },
      "id": "fJFNP4UFFGAE"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "    model_loaded  = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "\n",
        "    # attach default lora adapters\n",
        "    replace_target_with_lora_layer(model_loaded,\n",
        "                                   r=LORA_HYPERPARM['rank'],\n",
        "                                   alpha=LORA_HYPERPARM['alpha'],\n",
        "                                   target_modules=LORA_HYPERPARM['target_modules'],\n",
        "                                   )\n",
        "\n",
        "    # load the trained adapter from Hub\n",
        "    url = hf_hub_url(repo_id=f\"{YOUR_HF_USER_NAME}/{YOUR_LORA_ADAPTER_REPO_NAME}\",\n",
        "                    filename=\"lora_adapters.pt\")\n",
        "    tokenizer_loaded = AutoTokenizer.from_pretrained(f\"{YOUR_HF_USER_NAME}/{YOUR_LORA_ADAPTER_REPO_NAME}\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    adapter_bytes = BytesIO(response.content)\n",
        "    adapter_state_dict = torch.load(adapter_bytes)\n",
        "\n",
        "    # load the trained adapter to the model, replacing the default weights\n",
        "    model_loaded.load_state_dict(adapter_state_dict, strict=False)\n",
        "\n",
        "    # check the validation loss\n",
        "    valid_loss_loaded = check_validation_loss(model_loaded, tokenizer_loaded, apply_my_chat_template, ds)\n",
        "    print(valid_loss_loaded)"
      ],
      "metadata": {
        "id": "dRfOdE-QDfrG"
      },
      "id": "dRfOdE-QDfrG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79da272be4b24b53aa4684e68914ec75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_00fb96b5e1374f538bd3883ff808f49a"
          }
        },
        "4cae1edc7e794d93ba8d305d7ec435b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71df4f876d114b07a81529c42cbe5993",
            "placeholder": "​",
            "style": "IPY_MODEL_44e5c63edd9e493d9eb01f8e2e400e10",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b2b1ebc3e13c4669b5f30b4f6c070f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_14b7cf88daad4a1ab076d9dbec4b090f",
            "placeholder": "​",
            "style": "IPY_MODEL_fe597c68af6246108571bdc2184cc05f",
            "value": ""
          }
        },
        "092b5a8ab5c84c17b6decdc73af383a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_5088b004c57547bab5b27a932d7d7c84",
            "style": "IPY_MODEL_c1f301bfae0947d0af231f6e8829a6d4",
            "value": true
          }
        },
        "b3120560dc0447878f4a05ceb3598447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_89514b343b3f46f89a59e7c65a291e3c",
            "style": "IPY_MODEL_53ec289204d340818009d262b0d3674e",
            "tooltip": ""
          }
        },
        "cc43408087cd4844a23e4df9db981fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9668eb39b974a28b72d6e550023a4fa",
            "placeholder": "​",
            "style": "IPY_MODEL_f5eabf8eccc9458382d3e8456bd7f04d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "00fb96b5e1374f538bd3883ff808f49a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "71df4f876d114b07a81529c42cbe5993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e5c63edd9e493d9eb01f8e2e400e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14b7cf88daad4a1ab076d9dbec4b090f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe597c68af6246108571bdc2184cc05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5088b004c57547bab5b27a932d7d7c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f301bfae0947d0af231f6e8829a6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89514b343b3f46f89a59e7c65a291e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ec289204d340818009d262b0d3674e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a9668eb39b974a28b72d6e550023a4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5eabf8eccc9458382d3e8456bd7f04d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cc7cecf78d944aea36da6737f3d8bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29bd096e5c8044c2a1d67d979b4eefac",
            "placeholder": "​",
            "style": "IPY_MODEL_d57ef60d32534f16a47a1f4ba1aa9a2b",
            "value": "Connecting..."
          }
        },
        "29bd096e5c8044c2a1d67d979b4eefac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57ef60d32534f16a47a1f4ba1aa9a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}