{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV4g7YkC25iC"
      },
      "source": [
        "# Homework 8\n",
        "\n",
        "In this homework, you will train a multi-label text classifier on a subset of [AG News](https://huggingface.co/datasets/r-three/ag_news_subset) dataset using a pre-trained BERT model. The AG News dataset consists of news articles categorized into one of four topics (0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech).\n",
        "\n",
        "**In part 1**, you will fine-tune a BERT-style model on the AG News dataset and evaluate its performance. You can find a tutorial for loading BERT and fine-tuning [here](https://huggingface.co/docs/transformers/training). For simplicity, I recommend using the [Hugging Face Transformers library](https://huggingface.co/docs/transformers/index).You're welcome to use a different framework if you prefer.\n",
        "\n",
        "**In part 2**, instead of fine-tuning a BERT-style model directly, you will use the representations from the BERT-style model as input to a linear classifier. Does this approach perform better or worse?\n",
        "\n",
        "For both part 1 and part 2, your goal is to achieve a test accuracy above the specified thresholds. You won’t have access to the test labels—just like in real-world applications!\n",
        "\n",
        "**Tips about fine-tuning**\n",
        "\n",
        "* Data preprocessing: raw text data should be tokenized before being fed to the model as batches during trainig.\n",
        "* Hyperparameter choices: Experiment with settings such as learning rate, warmup ratio, optimizer, number of training steps, and batch size.\n",
        "* Avoid overfitting: remember that your fine-tuned model will be evaluated on the test set!\n",
        "\n",
        "\n",
        "**!! IMPORTANT NOTE !!**\n",
        "\n",
        "You are free to explore and implement the training code however you want to maximize the model performance. However, please put the code you're running under `if __name__ == '__main__':` so that the particular training step is not run when we later evaluate your final script! Otherwise you may fail the Markus tests due to timeout.\n",
        "\n",
        "```\n",
        "if __name__ == '__main__':\n",
        "    # your training code to fine-tune the model\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYmukJqi_qQl"
      },
      "source": [
        "# Part 1 (4 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "edPVKxZtQ0u2",
        "collapsed": true,
        "outputId": "f425bca5-a07c-4574-d9f2-322ba8beeda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3761893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    !pip install datasets\n",
        "    !pip install evaluate\n",
        "    !pip install -U sentence-transformers\n",
        "\n",
        "    from datasets import load_dataset, DatasetDict\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch\n",
        "    import evaluate\n",
        "\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import joblib\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    ################# Import additional packages you need #################\n",
        "    #####################################################################################\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwHmIaTUQZYy"
      },
      "outputs": [],
      "source": [
        "################## HELPER CODE FOR SAVING RELEVANT FILES ##################\n",
        "if __name__ == '__main__':\n",
        "    def in_colab():\n",
        "        try:\n",
        "            import google.colab\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "\n",
        "    if in_colab():\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_PATH = '/content/drive/MyDrive/CSC413'\n",
        "    else:\n",
        "        SAVE_PATH = '.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MenK8lkBOq3g"
      },
      "source": [
        "## Part 1.a\n",
        "\n",
        "Fine-tune [TinyBERT](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D) on AG News and evaluate the results. You can find a tutorial for loading BERT and fine-tuning [here](https://huggingface.co/docs/transformers/training). In that tutorial, you will need to change the dataset from `\"yelp_review_full\"` to the correct dataset path and the model from `\"bert-base-uncased\"` to `\"huawei-noah/TinyBERT_General_4L_312D\"`. You'll also need to modify the code since AG New is a four-class classification dataset (unlike the Yelp Reviews dataset, which is a five-class classification dataset).\n",
        "\n",
        "**TODO**\n",
        "* After fine-tuning the model, save model predictions on the test set to *part1_tiny_bert_model_test_prediction.csv*. The csv file should contain \"index\" columns, corresponding to the unique sample index, and \"pred\" column, the model prediction on that sample. Your model should achieve >= 80% on the test accuracy to receive a full mark.\n",
        "\n",
        "```\n",
        "index, pred\n",
        "0,model_pred_value_0\n",
        "1,model_pred_value_1\n",
        "2,model_pred_value_2\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3Zr_lHDWVTU"
      },
      "outputs": [],
      "source": [
        "######################## DO NOT MODIFY THE CODE ########################\n",
        "if __name__ == '__main__':\n",
        "    dataset = load_dataset('r-three/ag_news_subset')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=4)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
        "    print(dataset[\"train\"][100])\n",
        "#########################################################################\n",
        "    # Tokenization function\n",
        "    def tokenize_function(examples):\n",
        "        # Combine title and description for richer context\n",
        "        texts = [f\"{title} {desc}\" for title, desc in zip(examples['title'], examples['description'])]\n",
        "        return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    print(\"Tokenizing datasets...\")\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Metric\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"{SAVE_PATH}/tinybert_results\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"Training TinyBERT...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"TinyBERT Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Predict on test set\n",
        "    print(\"Generating TinyBERT predictions on test set...\")\n",
        "    test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "    test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    part1_tiny_bert_pred = pd.DataFrame({\n",
        "        'index': range(len(test_preds)),\n",
        "        'pred': test_preds\n",
        "    })\n",
        "    part1_tiny_bert_pred.to_csv(f\"{SAVE_PATH}/part1_tiny_bert_model_test_prediction.csv\", index=False)\n",
        "    print(f\"Saved TinyBERT predictions to {SAVE_PATH}/part1_tiny_bert_model_test_prediction.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your trianing code here...**"
      ],
      "metadata": {
        "id": "aWZ8R0IEbz44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your prediction is saved in pandas dataframe, you can do something like:\n",
        "```\n",
        "if __name__ == '__main__':\n",
        "   part1_tiny_bert_pred.to_csv(f\"{SAVE_PATH}/part1_tiny_bert_model_test_prediction.csv\", index=False)\n",
        "```"
      ],
      "metadata": {
        "id": "SHieq6Luch21"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JTLJgvUIT46"
      },
      "source": [
        "## Part 1.b\n",
        "\n",
        "For this section, choose a different pre-trained BERT-style model from the [Hugging Face Model Hub](https://huggingface.co/models) and fine-tune it. There are tons of options - part of the homework is navigating the hub to find different models! I recommend picking a model that is smaller than BERT-Base (as TinyBERT is) just to make things computationally cheaper. Is the final validation accuracy higher or lower with this other model?\n",
        "\n",
        "**TODO**\n",
        "* As in part 1.a, save model predictions on the test set to *part1_hf_bert_model_test_prediction.csv*. The csv file should contain \"index\" columns, corresponding to the unique sample index, and \"pred\" column, the model prediction on that sample. Your model should achieve >=80% on the test accuracy to receive a full mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAw02tJzJDme"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    ############### YOUR CODE ###############\n",
        "    # TODO: find a new HF BERT based model from HuggingFace and load it.\n",
        "    HF_BERT_BASED_MODEL = \"prajjwal1/bert-mini\"\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    print(f\"Loading model: {HF_BERT_BASED_MODEL}\")\n",
        "    model_hf = AutoModelForSequenceClassification.from_pretrained(HF_BERT_BASED_MODEL, num_labels=4)\n",
        "    tokenizer_hf = AutoTokenizer.from_pretrained(HF_BERT_BASED_MODEL)\n",
        "\n",
        "    # Tokenization function for the new model\n",
        "    def tokenize_function_hf(examples):\n",
        "        texts = [f\"{title} {desc}\" for title, desc in zip(examples['title'], examples['description'])]\n",
        "        return tokenizer_hf(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    # Metric function\n",
        "    def compute_metrics_hf(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    print(\"Tokenizing datasets with new tokenizer...\")\n",
        "    tokenized_datasets_hf = dataset.map(tokenize_function_hf, batched=True)\n",
        "\n",
        "    # Data collator\n",
        "    data_collator_hf = DataCollatorWithPadding(tokenizer=tokenizer_hf)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args_hf = TrainingArguments(\n",
        "        output_dir=f\"{SAVE_PATH}/hf_bert_results\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer_hf = Trainer(\n",
        "        model=model_hf,\n",
        "        args=training_args_hf,\n",
        "        train_dataset=tokenized_datasets_hf[\"train\"],\n",
        "        eval_dataset=tokenized_datasets_hf[\"validation\"],\n",
        "        tokenizer=tokenizer_hf,\n",
        "        data_collator=data_collator_hf,\n",
        "        compute_metrics=compute_metrics_hf,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(f\"Training {HF_BERT_BASED_MODEL}...\")\n",
        "    trainer_hf.train()\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results_hf = trainer_hf.evaluate()\n",
        "    print(f\"Validation Accuracy: {eval_results_hf['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Predict on test set\n",
        "    print(\"Generating predictions on test set...\")\n",
        "    test_predictions_hf = trainer_hf.predict(tokenized_datasets_hf[\"test\"])\n",
        "    test_preds_hf = np.argmax(test_predictions_hf.predictions, axis=1)\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    part1_hf_bert_pred = pd.DataFrame({\n",
        "        'index': range(len(test_preds_hf)),\n",
        "        'pred': test_preds_hf\n",
        "    })\n",
        "    part1_hf_bert_pred.to_csv(f\"{SAVE_PATH}/part1_hf_bert_model_test_prediction.csv\", index=False)\n",
        "    print(f\"Saved predictions to {SAVE_PATH}/part1_hf_bert_model_test_prediction.csv\")\n",
        "    #########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your training code here...**"
      ],
      "metadata": {
        "id": "QP_4e2M4cSz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, you can consider something like:\n",
        "\n",
        "```\n",
        "if __name__ == '__main__':\n",
        "   part1_hf_bert_pred.to_csv(f\"{SAVE_PATH}/part1_hf_bert_model_test_prediction.csv\", index=False)\n",
        "```"
      ],
      "metadata": {
        "id": "jOyGA96Dc65h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yt5QSetH7Vu"
      },
      "source": [
        "# Part 2 (2.5 points)\n",
        "\n",
        "Instead of fine-tuning the full model on a target dataset, it's also possible to use the output representations from a BERT-style model as input to a linear classifier and *only* train the classifier (leaving the rest of the pre-trained parameters fixed). You can do this easily using the [`sentence-transformers`](https://www.sbert.net/) library. Using `sentence-tranformers` gives you back a fixed-length representation of a given text sequence. To achieve this, you need to\n",
        "1. Pick a pre-trained sentence Transformer.\n",
        "2. Load the AG News dataset and feed the text from each example into the model.\n",
        "3. Train a linear classifier on the representations.\n",
        "4. Evaluate performance on the validation set.\n",
        "\n",
        "For the second step, you can learn more about how to use Hugging Face datasets [here](https://huggingface.co/docs/datasets/index). For the third and fourth step, it's possible to either do this directly in PyTorch, or collect the learned representations and use them as feature vectors to train a linear classifier in any other library (e.g. [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html)). For this homework, you will implement the second approach.\n",
        "\n",
        "After you complete the above steps, is the accuracy on the validation set higher or lower using a fixed sentence Transformer?\n",
        "\n",
        "**TODO**:\n",
        "* Complete the `encode_data` function: the function embeds each text sample into an output representation using the provided sentence encoder. The function is called to map a text data sample to the model representation, as shown below:\n",
        "```\n",
        "dataset.map(lambda x: encode_data(sen_model, x), batched=True)\n",
        "```\n",
        "* Train a Logistic Regression classifier: use sklearn.linear_model.LogisticRegression to fit the model on the encoded text data.\n",
        "* Save your trained model: After training, saved teh fitted logistic regression model as `sentence_encoder_classification.pkl`. Your model should achieve >=85% on the test accuracy to receive a full mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cym2hkG1WbVE"
      },
      "outputs": [],
      "source": [
        "def encode_data(model, x):\n",
        "    \"\"\"Takes the model and the dataset object\n",
        "        Returns a dictionary consisting of \"encoded_input\" and \"label\" as keys.\n",
        "        - \"encoded_input\" contains the tokenized text features produced by the sentence transformer.\n",
        "        - \"label\" is the target class label for each example.\n",
        "        encoded_input is the encoded text input, and label is the target label.\n",
        "        NOTE: Please assume the dataset object is the original one loaded via\n",
        "              load_dataset('r-three/') for reproducibility.\n",
        "              Which means if you want to create additional features to create the encoded_input,\n",
        "              do so within this function.\n",
        "    \"\"\"\n",
        "    ####################### YOUR CODE ##########################\n",
        "    # Combine title and description for richer text representation\n",
        "    texts = [f\"{title} {desc}\" for title, desc in zip(x['title'], x['description'])]\n",
        "\n",
        "    # Encode the texts using the sentence transformer\n",
        "    embeddings = model.encode(texts, show_progress_bar=False)\n",
        "\n",
        "    # Return dictionary with encoded input and labels\n",
        "    d = {\n",
        "        'encoded_input': embeddings,\n",
        "        'label': x['label']\n",
        "    }\n",
        "    return d\n",
        "    ############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB836CIZqulu"
      },
      "outputs": [],
      "source": [
        "########### PUT YOUR MODEL HERE ###########\n",
        "SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n",
        "###########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQvqBakJjJ7b"
      },
      "outputs": [],
      "source": [
        "########### DO NOT CHANGE THIS CODE ###########\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    sen_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
        "    # Prepare the dataset\n",
        "    tokenized_dataset = dataset.map(lambda x: encode_data(sen_model, x), batched=True)\n",
        "    print(tokenized_dataset['train'][100])\n",
        "    X_train = np.stack([np.array(x['encoded_input']) for x in tokenized_dataset['train']])\n",
        "    X_val = np.stack([np.array(x['encoded_input']) for x in tokenized_dataset['validation']])\n",
        "    y_train = np.stack([np.array(x['label']) for x in tokenized_dataset['train']])\n",
        "    y_val = np.stack([np.array(x['label']) for x in tokenized_dataset['validation']])\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(X_val.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk2cKDK_bk_y"
      },
      "outputs": [],
      "source": [
        "########### COMPLETE THE FOLLOWING LOGISTIC REGRESSION CODE ###########\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the logistic regression classifier on encoded training data\n",
        "    print(\"Training Logistic Regression classifier...\")\n",
        "    classifier = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        solver='lbfgs',\n",
        "        multi_class='multinomial',\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit the classifier\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_accuracy = classifier.score(X_val, y_val)\n",
        "    print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Generate predictions on test set\n",
        "    test_preds = classifier.predict(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_j3mx8IMvNG"
      },
      "outputs": [],
      "source": [
        "######################## TO SUBMIT ########################\n",
        "if __name__ == \"__main__\":\n",
        "    # Save the trained model\n",
        "    model_path = f\"{SAVE_PATH}/sentence_encoder_classification.pkl\"\n",
        "\n",
        "    joblib.dump(classifier, model_path)\n",
        "\n",
        "    print(f\"Saved model to {model_path}\")\n",
        "    print(f\"Part 1.a predictions saved to {SAVE_PATH}/part1_tiny_bert_model_test_prediction.csv\")\n",
        "    print(f\"Part 1.b predictions saved to {SAVE_PATH}/part1_hf_bert_model_test_prediction.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}